{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYXqCV5BVYoWrBBhCwyHUy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leosammallahti/colabstuff/blob/main/Mozi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLaR4UMk5BKp"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 0) Setup & Installs\n",
        "# =========================\n",
        "!pip -q install gspread google-auth google-auth-oauthlib google-auth-httplib2\n",
        "!pip -q install pandas numpy tqdm\n",
        "!pip -q install sentence-transformers faiss-cpu\n",
        "\n",
        "# (Optional) For topic modeling later\n",
        "# !pip -q install bertopic umap-learn hdbscan\n",
        "\n",
        "# =========================\n",
        "# 1) Auth & Load Google Sheet\n",
        "# =========================\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "SHEET_TITLE = \"Mozi_corrected\"  # exact sheet name\n",
        "WORKSHEET_NAME = None           # None = first worksheet; set to name if you prefer\n",
        "RANGE_A1 = \"A1:F591\"            # as provided\n",
        "\n",
        "def load_sheet_to_df(sheet_title=SHEET_TITLE, worksheet_name=WORKSHEET_NAME, a1_range=RANGE_A1):\n",
        "    \"\"\"\n",
        "    Opens a Google Sheet and loads the specified A1 range into a pandas DataFrame.\n",
        "    Robust to ragged rows: pads/truncates rows to header length.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        sh = gc.open(sheet_title)\n",
        "    except Exception as e:\n",
        "        print(\"Couldn't open by title. If this fails, paste the full Google Sheets URL when prompted next.\")\n",
        "        print(\"Error:\", e)\n",
        "        url = input(\"Paste Google Sheets URL (the doc that contains 'Mozi_corrected'): \").strip()\n",
        "        sh = gc.open_by_url(url)\n",
        "\n",
        "    ws = sh.worksheet(worksheet_name) if worksheet_name else sh.sheet1\n",
        "    values = ws.get(a1_range)  # returns list of lists; rows can be ragged\n",
        "    if not values or len(values) < 2:\n",
        "        raise ValueError(f\"No data (or only header) returned for range {a1_range}. Check the range.\")\n",
        "\n",
        "    header = [h.strip() for h in values[0]]\n",
        "    H = len(header)\n",
        "\n",
        "    # Diagnose ragged rows\n",
        "    bad_short, bad_long = [], []\n",
        "    for i, row in enumerate(values[1:], start=2):  # 1-based + header -> start=2\n",
        "        if len(row) < H:\n",
        "            bad_short.append((i, len(row)))\n",
        "        elif len(row) > H:\n",
        "            bad_long.append((i, len(row)))\n",
        "\n",
        "    if bad_short or bad_long:\n",
        "        print(f\"[INFO] Header has {H} columns: {header}\")\n",
        "        if bad_short:\n",
        "            print(f\"[WARN] {len(bad_short)} rows shorter than header (showing up to 5): {bad_short[:5]}\")\n",
        "        if bad_long:\n",
        "            print(f\"[WARN] {len(bad_long)} rows longer than header (showing up to 5): {bad_long[:5]}\")\n",
        "\n",
        "    # Normalize rows: pad with empty strings or truncate to match header length\n",
        "    norm_rows = []\n",
        "    for row in values[1:]:\n",
        "        if len(row) < H:\n",
        "            row = row + [\"\"] * (H - len(row))\n",
        "        elif len(row) > H:\n",
        "            row = row[:H]\n",
        "        norm_rows.append(row)\n",
        "\n",
        "    df = pd.DataFrame(norm_rows, columns=header)\n",
        "\n",
        "    # (Optional) ensure expected columns exist, even if empty\n",
        "    for col in [\"Chapter\", \"Verse\", \"Text\", \"Translator_Note\", \"Text_Clean\", \"Translator_Note_Clean\"]:\n",
        "      if col not in df.columns:\n",
        "          df[col] = \"\"\n",
        "    return df\n",
        "\n",
        "df = load_sheet_to_df()\n",
        "print(\"Loaded DataFrame shape:\", df.shape)\n",
        "print(\"Columns:\", list(df.columns))\n",
        "\n",
        "# =========================\n",
        "# 2) Quick Sanity Checks / EDA\n",
        "# =========================\n",
        "# Standardize expected columns (just in case of whitespace)\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "expected_cols = [\n",
        "    \"Chapter\", \"Verse\", \"Text\", \"Translator_Note\", \"Text_Clean\", \"Translator_Note_Clean\"\n",
        "]\n",
        "missing = [c for c in expected_cols if c not in df.columns]\n",
        "if missing:\n",
        "    print(\"WARNING: Missing expected columns:\", missing)\n",
        "\n",
        "# Clean up basic types\n",
        "for col in [\"Chapter\", \"Verse\"]:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "# Basic summary\n",
        "print(\"\\n=== Basic Summary ===\")\n",
        "if \"Chapter\" in df.columns:\n",
        "    print(\"Chapters count:\", df[\"Chapter\"].nunique())\n",
        "if \"Verse\" in df.columns:\n",
        "    print(\"Verses count (non-null):\", df[\"Verse\"].notna().sum())\n",
        "if \"Translator_Note\" in df.columns:\n",
        "    blank_notes = (df[\"Translator_Note\"].astype(str).str.strip() == \"\").sum()\n",
        "    print(\"Blank translator notes:\", blank_notes)\n",
        "\n",
        "# Length diagnostics\n",
        "text_col = \"Text_Clean\" if \"Text_Clean\" in df.columns else \"Text\"\n",
        "if text_col in df.columns:\n",
        "    lens = df[text_col].fillna(\"\").astype(str).str.len()\n",
        "    print(\"Avg verse length:\", lens.mean().round(1))\n",
        "    print(\"Median verse length:\", lens.median())\n",
        "    print(\"Max verse length:\", lens.max())\n",
        "\n",
        "# Drop fully empty rows in the text column (rare but safe)\n",
        "df = df[df[text_col].fillna(\"\").astype(str).str.strip() != \"\"].reset_index(drop=True)\n",
        "\n",
        "# =========================\n",
        "# 3) Build Embeddings + FAISS (Semantic Search)\n",
        "# =========================\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from tqdm import tqdm\n",
        "\n",
        "# You can upgrade to a philosophy-specific model later; start fast & small here:\n",
        "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedder = SentenceTransformer(EMBED_MODEL)\n",
        "\n",
        "# Prepare corpus (we’ll search over verses)\n",
        "corpus_texts = df[text_col].astype(str).tolist()\n",
        "\n",
        "# Compute embeddings\n",
        "batch_size = 128\n",
        "emb_list = []\n",
        "for i in tqdm(range(0, len(corpus_texts), batch_size), desc=\"Embedding\"):\n",
        "    batch = corpus_texts[i:i+batch_size]\n",
        "    emb = embedder.encode(batch, show_progress_bar=False, normalize_embeddings=True)\n",
        "    emb_list.append(emb)\n",
        "embeddings = np.vstack(emb_list).astype(\"float32\")\n",
        "\n",
        "# Build FAISS index (cosine similarity via inner product on normalized vectors)\n",
        "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "print(\"FAISS index built. Vectors:\", index.ntotal)\n",
        "\n",
        "# =========================\n",
        "# 4) Simple Search Helper\n",
        "# =========================\n",
        "def search_verses(query, top_k=5):\n",
        "    \"\"\"\n",
        "    Returns top_k most similar verses to the query using the FAISS index.\n",
        "    \"\"\"\n",
        "    q_emb = embedder.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
        "    scores, idxs = index.search(q_emb, top_k)\n",
        "    idxs = idxs[0].tolist()\n",
        "    scores = scores[0].tolist()\n",
        "\n",
        "    results = []\n",
        "    for rank, (i, s) in enumerate(zip(idxs, scores), start=1):\n",
        "        row = df.iloc[i].to_dict()\n",
        "        row[\"_rank\"] = rank\n",
        "        row[\"_score\"] = float(s)\n",
        "        row[\"_text_col\"] = row.get(text_col, \"\")\n",
        "        results.append(row)\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Quick smoke test queries (you can edit these)\n",
        "test_queries = [\n",
        "    \"filial piety\",\n",
        "    \"benefit the people\",\n",
        "    \"Heaven's will\",\n",
        "]\n",
        "\n",
        "for q in test_queries:\n",
        "    print(f\"\\n=== Query: {q} ===\")\n",
        "    display(search_verses(q, top_k=5)[[\"_rank\",\"_score\",\"Chapter\",\"Verse\",\"_text_col\"]])\n",
        "\n",
        "# =========================\n",
        "# 5) Optional: Topic Modeling with BERTopic (Run later if you like)\n",
        "# =========================\n",
        "# If you enabled the installs above, uncomment this section to try themes.\n",
        "# from bertopic import BERTopic\n",
        "# topic_model = BERTopic(verbose=True)\n",
        "# topics, probs = topic_model.fit_transform(corpus_texts, embeddings=embeddings)\n",
        "# topic_info = topic_model.get_topic_info()\n",
        "# display(topic_info.head(15))\n",
        "# # To inspect a specific topic:\n",
        "# # display(pd.DataFrame(topic_model.get_topic(0), columns=[\"term\", \"weight\"]).head(20))\n",
        "\n",
        "# =========================\n",
        "# 6) (Optional) Save outputs to Drive\n",
        "# =========================\n",
        "# Uncomment if you want a CSV of embeddings' metadata + text for later use.\n",
        "# out = df.copy()\n",
        "# out[\"embed_dim\"] = embeddings.shape[1]\n",
        "# out.to_csv(\"/content/mozi_metadata.csv\", index=False)\n",
        "# print(\"Saved /content/mozi_metadata.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "base_folder = \"/content/drive/MyDrive/Chinese Philosophy/Mozi\"\n",
        "os.makedirs(base_folder, exist_ok=True)\n",
        "print(\"Created/verified:\", base_folder)\n"
      ],
      "metadata": {
        "id": "4pVCMZNn6_Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assumes `df` already exists from your loader\n",
        "KEEP = [\"Chapter\",\"Verse\",\"Text\",\"Translator_Note\",\"Text_Clean\",\"Translator_Note_Clean\"]\n",
        "clean_path = os.path.join(BASE_FOLDER, \"mozi_clean.csv\")\n",
        "df[KEEP].copy().to_csv(clean_path, index=False)\n",
        "print(\"Saved:\", clean_path)\n",
        "\n",
        "counts_path = os.path.join(BASE_FOLDER, \"mozi_rows_by_chapter.csv\")\n",
        "df.groupby(\"Chapter\").size().rename(\"rows\").to_frame().to_csv(counts_path)\n",
        "print(\"Saved:\", counts_path)\n"
      ],
      "metadata": {
        "id": "OJC4_ZJf7KnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-shot fix + save\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Mount Drive (idempotent)\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# 2) Define/create the target folder\n",
        "BASE_FOLDER = \"/content/drive/MyDrive/Chinese Philosophy/Mozi\"\n",
        "os.makedirs(BASE_FOLDER, exist_ok=True)\n",
        "print(\"Created/verified:\", BASE_FOLDER)\n",
        "\n",
        "# 3) Make sure df exists\n",
        "if \"df\" not in globals():\n",
        "    raise RuntimeError(\"DataFrame `df` not found. Run the sheet loader cell first to create `df`.\")\n",
        "\n",
        "# 4) Save files\n",
        "KEEP = [\"Chapter\",\"Verse\",\"Text\",\"Translator_Note\",\"Text_Clean\",\"Translator_Note_Clean\"]\n",
        "clean_path = os.path.join(BASE_FOLDER, \"mozi_clean.csv\")\n",
        "df[KEEP].copy().to_csv(clean_path, index=False)\n",
        "print(\"Saved:\", clean_path)\n",
        "\n",
        "counts_path = os.path.join(BASE_FOLDER, \"mozi_rows_by_chapter.csv\")\n",
        "df.groupby(\"Chapter\").size().rename(\"rows\").to_frame().to_csv(counts_path)\n",
        "print(\"Saved:\", counts_path)\n"
      ],
      "metadata": {
        "id": "Jjlg8LOn8Dw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assumes `df` already exists from your loader\n",
        "KEEP = [\"Chapter\",\"Verse\",\"Text\",\"Translator_Note\",\"Text_Clean\",\"Translator_Note_Clean\"]\n",
        "clean_path = os.path.join(BASE_FOLDER, \"mozi_clean.csv\")\n",
        "df[KEEP].copy().to_csv(clean_path, index=False)\n",
        "print(\"Saved:\", clean_path)\n",
        "\n",
        "counts_path = os.path.join(BASE_FOLDER, \"mozi_rows_by_chapter.csv\")\n",
        "df.groupby(\"Chapter\").size().rename(\"rows\").to_frame().to_csv(counts_path)\n",
        "print(\"Saved:\", counts_path)\n"
      ],
      "metadata": {
        "id": "aIlQc0Qx8Snm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def kwic(keyword: str, window: int = 40, use_clean=True, max_hits=200):\n",
        "    col = \"Text_Clean\" if use_clean and \"Text_Clean\" in df.columns else \"Text\"\n",
        "    patt = re.compile(re.escape(keyword), flags=re.IGNORECASE)\n",
        "    rows = []\n",
        "    texts = df[col].fillna(\"\").astype(str).tolist()\n",
        "    for i, s in enumerate(texts):\n",
        "        for m in patt.finditer(s):\n",
        "            start = max(0, m.start()-window); end = min(len(s), m.end()+window)\n",
        "            rows.append({\n",
        "                \"Chapter\": int(df.loc[i, \"Chapter\"]) if pd.notna(df.loc[i,\"Chapter\"]) else None,\n",
        "                \"Verse\": int(df.loc[i, \"Verse\"]) if pd.notna(df.loc[i,\"Verse\"]) else None,\n",
        "                \"Left\": s[start:m.start()],\n",
        "                \"Hit\": s[m.start():m.end()],\n",
        "                \"Right\": s[m.end():end],\n",
        "                \"Full\": s\n",
        "            })\n",
        "            if len(rows) >= max_hits:\n",
        "                break\n",
        "        if len(rows) >= max_hits:\n",
        "            break\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# Example run + save\n",
        "kw = \"Heaven\"\n",
        "kwic_df = kwic(kw, window=50, max_hits=200)\n",
        "kwic_path = os.path.join(BASE_FOLDER, f\"kwic_{kw.replace(' ','_').lower()}.csv\")\n",
        "kwic_df.to_csv(kwic_path, index=False)\n",
        "print(f\"KWIC saved: {kwic_path} ({len(kwic_df)} hits)\")\n",
        "kwic_df.head(10)\n"
      ],
      "metadata": {
        "id": "PwDkjhFO8Wu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def batch_search(queries, top_k=7, out_name=\"mozi_batch_search.csv\"):\n",
        "    all_rows = []\n",
        "    for q in queries:\n",
        "        res = search_verses(q, top_k=top_k).copy()\n",
        "        res.insert(0, \"query\", q)\n",
        "        all_rows.append(res)\n",
        "    out = pd.concat(all_rows, ignore_index=True)\n",
        "    keep = [\"query\",\"_rank\",\"_score\",\"Chapter\",\"Verse\"]\n",
        "    txt = \"Text_Clean\" if \"Text_Clean\" in out.columns else \"Text\"\n",
        "    for c in [txt, \"Text\", \"Translator_Note\"]:\n",
        "        if c in out.columns: keep.append(c)\n",
        "    out_path = os.path.join(BASE_FOLDER, out_name)\n",
        "    out[keep].to_csv(out_path, index=False)\n",
        "    print(f\"Saved: {out_path}  (rows={len(out)})\")\n",
        "    return out[keep]\n",
        "\n",
        "queries = [\n",
        "    \"benefit the people\",\n",
        "    \"filial piety\",\n",
        "    \"Heaven's intention\",\n",
        "    \"against fatalism\",\n",
        "    \"universal love\",\n",
        "]\n",
        "batch_search(queries, top_k=7, out_name=\"mozi_batch_search.csv\").head(10)\n"
      ],
      "metadata": {
        "id": "GUf2GJlJ8wLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install once:\n",
        "!pip -q install bertopic umap-learn hdbscan\n",
        "\n",
        "from bertopic import BERTopic\n",
        "import pandas as pd\n",
        "\n",
        "# Use precomputed `embeddings` from your current session for better/faster topics\n",
        "texts = df[text_col].astype(str).tolist()\n",
        "topic_model = BERTopic(verbose=False)\n",
        "topics, probs = topic_model.fit_transform(texts, embeddings=embeddings)\n",
        "\n",
        "topic_info = topic_model.get_topic_info()\n",
        "topic_info_path = os.path.join(BASE_FOLDER, \"mozi_topics_overview.csv\")\n",
        "topic_info.to_csv(topic_info_path, index=False)\n",
        "print(\"Saved:\", topic_info_path)\n",
        "\n",
        "assignments = df[[\"Chapter\",\"Verse\"]].copy()\n",
        "assignments[\"topic_id\"] = topics\n",
        "assignments_path = os.path.join(BASE_FOLDER, \"mozi_topics_assignments.csv\")\n",
        "assignments.to_csv(assignments_path, index=False)\n",
        "print(\"Saved:\", assignments_path)\n",
        "\n",
        "# Optional: dump top terms for first N topics\n",
        "N = min(15, len(topic_info))\n",
        "rows = []\n",
        "for i in range(N):\n",
        "    t_id = topic_info.iloc[i][\"Topic\"]\n",
        "    terms = topic_model.get_topic(t_id) or []\n",
        "    for term, weight in terms[:15]:\n",
        "        rows.append({\"topic_id\": t_id, \"term\": term, \"weight\": weight})\n",
        "top_terms = pd.DataFrame(rows)\n",
        "terms_path = os.path.join(BASE_FOLDER, \"mozi_topics_top_terms.csv\")\n",
        "top_terms.to_csv(terms_path, index=False)\n",
        "print(\"Saved:\", terms_path)\n",
        "\n",
        "topic_info.head(10)\n"
      ],
      "metadata": {
        "id": "oY0l_I_J8yPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Silence those SyntaxWarnings (optional)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n",
        "\n",
        "# Better vectorizer for philosophical English\n",
        "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
        "from bertopic import BERTopic\n",
        "import os, pandas as pd\n",
        "\n",
        "# 1) Build a stopword list that removes glue words + Mozi-specific filler\n",
        "custom_sw = set(ENGLISH_STOP_WORDS) | {\n",
        "    \"the\",\"and\",\"of\",\"to\",\"in\",\"is\",\"it\",\"this\",\"that\",\"be\",\"as\",\"are\",\"was\",\"were\",\n",
        "    \"not\",\"so\",\"one\",\"all\",\"there\",\"their\",\"them\",\"what\",\"when\",\"then\",\n",
        "    # domain fillers you may want to hide\n",
        "    \"mozi\",\"master\",\"said\",\"therefore\",\"thus\",\"hence\",\"people\",\"men\"\n",
        "}\n",
        "\n",
        "# 2) Vectorizer: allow unigrams & bigrams, filter very common & very rare terms\n",
        "vectorizer_model = CountVectorizer(\n",
        "    lowercase=True,\n",
        "    stop_words=list(custom_sw),\n",
        "    ngram_range=(1,2),\n",
        "    min_df=3,        # ignore terms that appear in <3 verses\n",
        "    max_df=0.90      # ignore terms that appear in >90% of verses\n",
        ")\n",
        "\n",
        "# 3) Fit BERTopic using your precomputed embeddings for speed/quality\n",
        "texts = df[text_col].astype(str).tolist()\n",
        "\n",
        "topic_model = BERTopic(\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    top_n_words=12,     # clearer topic labels\n",
        "    min_topic_size=15,  # avoid micro-topics; tweak (10–30) as you like\n",
        "    nr_topics=\"auto\",   # let it merge similar topics\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "topics, probs = topic_model.fit_transform(texts, embeddings=embeddings)\n",
        "\n",
        "# 4) Save improved outputs to your Drive folder\n",
        "topic_info = topic_model.get_topic_info()\n",
        "topic_info_path = os.path.join(BASE_FOLDER, \"mozi_topics_overview_refined.csv\")\n",
        "topic_info.to_csv(topic_info_path, index=False)\n",
        "\n",
        "assignments = df[[\"Chapter\",\"Verse\"]].copy()\n",
        "assignments[\"topic_id\"] = topics\n",
        "assignments_path = os.path.join(BASE_FOLDER, \"mozi_topics_assignments_refined.csv\")\n",
        "assignments.to_csv(assignments_path, index=False)\n",
        "\n",
        "# Export top terms per topic\n",
        "rows = []\n",
        "for t_id in topic_info[\"Topic\"].tolist():\n",
        "    terms = topic_model.get_topic(t_id) or []\n",
        "    for term, weight in terms[:15]:\n",
        "        rows.append({\"topic_id\": t_id, \"term\": term, \"weight\": weight})\n",
        "top_terms = pd.DataFrame(rows)\n",
        "terms_path = os.path.join(BASE_FOLDER, \"mozi_topics_top_terms_refined.csv\")\n",
        "top_terms.to_csv(terms_path, index=False)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\" \", topic_info_path)\n",
        "print(\" \", assignments_path)\n",
        "print(\" \", terms_path)\n",
        "\n",
        "# Peek the first real topic (skip -1 which is 'outliers')\n",
        "topic_info.head(10)\n"
      ],
      "metadata": {
        "id": "4s6oDQMK9eeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Adaptive BERTopic refinement (handles small corpora) ---\n",
        "import warnings, os, pandas as pd\n",
        "warnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# 0) Build/clean corpus\n",
        "texts = (\n",
        "    df[text_col]\n",
        "    .astype(str)\n",
        "    .map(lambda s: s.strip())\n",
        "    .replace({\"nan\": \"\"})\n",
        "    .tolist()\n",
        ")\n",
        "# Remove empties and (optionally) duplicates\n",
        "texts = [t for t in texts if t]\n",
        "# (Optional) de-dup: uncomment if many exact repeats\n",
        "# texts = list(dict.fromkeys(texts))\n",
        "\n",
        "N = len(texts)\n",
        "print(f\"Docs for BERTopic: {N}\")\n",
        "if N < 5:\n",
        "    raise ValueError(\"Too few documents for topic modeling. Use more verses or skip BERTopic.\")\n",
        "\n",
        "# 1) Adaptive df thresholds\n",
        "# Keep proportional thresholds to avoid min_df > max_df\n",
        "min_df_prop = max(2 / N, 0.002)   # at least ~2 docs or 0.2%\n",
        "max_df_prop = 0.9                 # drop super-common terms\n",
        "if max_df_prop <= min_df_prop:    # safety: widen if tiny corpus\n",
        "    max_df_prop = min(0.99, min_df_prop + 0.05)\n",
        "\n",
        "# 2) Stopwords (tune as needed)\n",
        "custom_sw = set(ENGLISH_STOP_WORDS) | {\n",
        "    \"the\",\"and\",\"of\",\"to\",\"in\",\"is\",\"it\",\"this\",\"that\",\"be\",\"as\",\"are\",\"was\",\"were\",\n",
        "    \"not\",\"so\",\"one\",\"all\",\"there\",\"their\",\"them\",\"what\",\"when\",\"then\",\n",
        "    \"mozi\",\"master\",\"said\",\"therefore\",\"thus\",\"hence\",\"people\",\"men\"\n",
        "}\n",
        "\n",
        "vectorizer_model = CountVectorizer(\n",
        "    lowercase=True,\n",
        "    stop_words=list(custom_sw),\n",
        "    ngram_range=(1,2),\n",
        "    min_df=min_df_prop,\n",
        "    max_df=max_df_prop\n",
        ")\n",
        "\n",
        "topic_model = BERTopic(\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    top_n_words=12,\n",
        "    min_topic_size=max(10, int(0.02 * N)),  # ~2% of corpus, at least 10\n",
        "    nr_topics=\"auto\",\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Use your precomputed `embeddings` for quality/speed\n",
        "topics, probs = topic_model.fit_transform(texts, embeddings=embeddings)\n",
        "\n",
        "# Save outputs\n",
        "topic_info = topic_model.get_topic_info()\n",
        "topic_info_path = os.path.join(BASE_FOLDER, \"mozi_topics_overview_refined.csv\")\n",
        "topic_info.to_csv(topic_info_path, index=False)\n",
        "\n",
        "assignments = pd.DataFrame({\"topic_id\": topics})\n",
        "assignments_path = os.path.join(BASE_FOLDER, \"mozi_topics_assignments_refined.csv\")\n",
        "assignments.to_csv(assignments_path, index=False)\n",
        "\n",
        "rows = []\n",
        "for t_id in topic_info[\"Topic\"].tolist():\n",
        "    terms = topic_model.get_topic(t_id) or []\n",
        "    for term, weight in terms[:15]:\n",
        "        rows.append({\"topic_id\": t_id, \"term\": term, \"weight\": weight})\n",
        "top_terms = pd.DataFrame(rows)\n",
        "terms_path = os.path.join(BASE_FOLDER, \"mozi_topics_top_terms_refined.csv\")\n",
        "top_terms.to_csv(terms_path, index=False)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\" \", topic_info_path)\n",
        "print(\" \", assignments_path)\n",
        "print(\" \", terms_path)\n",
        "topic_info.head(10)\n"
      ],
      "metadata": {
        "id": "APV_S2TQ9xY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step E: Philosophy-tuned embeddings swap ---\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load a philosophy/classics tuned embedding model\n",
        "# Options:\n",
        "# - \"shibing624/text2vec-base-chinese-paraphrase\" (classical CN)\n",
        "# - \"sentence-transformers/all-mpnet-base-v2\" (general strong English)\n",
        "# - \"llmrails/finetuned-philosophy-bert\" (if accessible)\n",
        "\n",
        "embed_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "# Recompute embeddings for the cleaned text corpus\n",
        "texts = (\n",
        "    df[text_col]\n",
        "    .astype(str)\n",
        "    .map(lambda s: s.strip())\n",
        "    .replace({\"nan\": \"\"})\n",
        "    .tolist()\n",
        ")\n",
        "texts = [t for t in texts if t]\n",
        "\n",
        "embeddings = embed_model.encode(\n",
        "    texts,\n",
        "    batch_size=32,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True\n",
        ")\n",
        "\n",
        "print(\"Embeddings shape:\", embeddings.shape)\n",
        "print(\"Preview:\", embeddings[0][:10])\n"
      ],
      "metadata": {
        "id": "eCsCWESu90HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gradio\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "\n",
        "def search_filtered(query, top_k=10, chapter=None, chapter_range=None):\n",
        "    hits = search_verses(query, top_k=200)\n",
        "    if chapter not in (\"\", None):\n",
        "        try: chapter = int(chapter); hits = hits[hits[\"Chapter\"] == chapter]\n",
        "        except: pass\n",
        "    if chapter_range and len(chapter_range) == 2:\n",
        "        lo, hi = chapter_range\n",
        "        hits = hits[(hits[\"Chapter\"] >= lo) & (hi >= hits[\"Chapter\"])]\n",
        "    return hits.head(top_k).reset_index(drop=True)\n",
        "\n",
        "def ui_search(query, k, chapter, lo, hi):\n",
        "    cr = (int(lo), int(hi)) if lo and hi else None\n",
        "    res = search_filtered(query, top_k=int(k), chapter=chapter, chapter_range=cr)\n",
        "    cols = [\"_rank\",\"_score\",\"Chapter\",\"Verse\",text_col]\n",
        "    return res[cols]\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Mozi Semantic Search\")\n",
        "    with gr.Row():\n",
        "        q = gr.Textbox(label=\"Query\", value=\"universal love\")\n",
        "        k = gr.Number(label=\"Top K\", value=5, precision=0)\n",
        "    with gr.Row():\n",
        "        chapter = gr.Textbox(label=\"Exact Chapter (optional)\", value=\"\")\n",
        "        lo = gr.Textbox(label=\"Chapter range start (optional)\", value=\"\")\n",
        "        hi = gr.Textbox(label=\"Chapter range end (optional)\", value=\"\")\n",
        "    btn = gr.Button(\"Search\")\n",
        "    out = gr.Dataframe(wrap=True)\n",
        "    btn.click(ui_search, inputs=[q,k,chapter,lo,hi], outputs=[out])\n",
        "\n",
        "demo.launch(share=False)\n"
      ],
      "metadata": {
        "id": "liLjVIfh9BsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig_h = topic_model.visualize_hierarchy()\n",
        "fig_h.write_html(os.path.join(BASE_FOLDER, \"mozi_topics_hierarchy.html\"))\n",
        "\n",
        "fig_b = topic_model.visualize_barchart(top_n_topics=15)\n",
        "fig_b.write_html(os.path.join(BASE_FOLDER, \"mozi_topics_barchart.html\"))\n",
        "\n",
        "fig_s = topic_model.visualize_topics()\n",
        "fig_s.write_html(os.path.join(BASE_FOLDER, \"mozi_topics_similarity_map.html\"))\n",
        "\n",
        "print(\"Saved interactive HTMLs to:\", BASE_FOLDER)\n"
      ],
      "metadata": {
        "id": "J9-mGtmCBZBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Colab setup ---\n",
        "!pip -q install spacy==3.7.4 textblob==0.18.0 wordfreq==3.1.1 nltk==3.9.1 pandas==2.2.2 matplotlib==3.9.0 gradio==4.41.0\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Drive + folders\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, pandas as pd\n",
        "BASE_DIR = \"/content/drive/MyDrive/Chinese Philosophy/Mozi\"\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "# Input corpus: expected CSV with at least columns: ['chapter','verse','text']\n",
        "# If your file name differs, change here:\n",
        "MOZI_CSV = os.path.join(BASE_DIR, \"Mozi_corrected.csv\")\n",
        "assert os.path.exists(MOZI_CSV), f\"Missing: {MOZI_CSV}. Please place it in {BASE_DIR}\"\n",
        "\n",
        "df = pd.read_csv(MOZI_CSV)\n",
        "df = df.rename(columns={c:c.lower() for c in df.columns})\n",
        "assert 'text' in df.columns, \"CSV must include a 'text' column (English translation).\"\n",
        "len(df), df.columns.tolist()[:8]\n"
      ],
      "metadata": {
        "id": "zQ_nj6B5G8-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 0 (fixed): installs + robust data loader ---\n",
        "\n",
        "# 0a) Install compatible packages (spaCy 3.8 supports pydantic v2; works with modern gradio)\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install \"spacy==3.8.2\" \"gradio>=4.41,<5\" \"textblob==0.18.0\" \"wordfreq==3.1.1\" \"nltk==3.9.1\" \"pandas==2.2.2\" \"matplotlib==3.9.0\"\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# 0b) Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# 0c) Find your Mozi CSV automatically (tries common places and falls back to a Drive-wide search)\n",
        "import os, pandas as pd, re\n",
        "from pathlib import Path\n",
        "\n",
        "SEARCH_BASES = [\n",
        "    \"/content/drive/MyDrive/Chinese Philosophy/Mozi\",\n",
        "    \"/content/drive/MyDrive/Chinese Philosophy\",\n",
        "    \"/content/drive/MyDrive/Mozi\",\n",
        "    \"/content/drive/MyDrive\",\n",
        "]\n",
        "PATTERNS = [\"Mozi_corrected.csv\", \"Mozi*.csv\"]\n",
        "\n",
        "cands = []\n",
        "for base in SEARCH_BASES:\n",
        "    p = Path(base)\n",
        "    if p.exists():\n",
        "        for pat in PATTERNS:\n",
        "            cands += list(p.rglob(pat))\n",
        "\n",
        "# If nothing found in common places, do a broader (but slower) search of MyDrive\n",
        "if not cands:\n",
        "    cands = list(Path(\"/content/drive/MyDrive\").rglob(\"Mozi*.csv\"))\n",
        "\n",
        "if not cands:\n",
        "    raise FileNotFoundError(\n",
        "        \"Couldn't find a Mozi CSV. Place one (e.g., 'Mozi_corrected.csv') in \"\n",
        "        \"/content/drive/MyDrive/Chinese Philosophy/Mozi and re-run this cell.\"\n",
        "    )\n",
        "\n",
        "# Prefer the most recently modified match\n",
        "cands = sorted(cands, key=lambda x: x.stat().st_mtime, reverse=True)\n",
        "MOZI_CSV = str(cands[0])\n",
        "print(\"Using CSV:\", MOZI_CSV)\n",
        "\n",
        "# 0d) Load + normalize columns\n",
        "df = pd.read_csv(MOZI_CSV)\n",
        "df.columns = [c.strip().lower() for c in df.columns]\n",
        "\n",
        "# Try to normalize the text column name to 'text'\n",
        "TEXT_CANDIDATES = [\"text\", \"translation\", \"english\", \"content\", \"verse_text\", \"line_text\"]\n",
        "for cand in TEXT_CANDIDATES:\n",
        "    if cand in df.columns:\n",
        "        if cand != \"text\":\n",
        "            df.rename(columns={cand: \"text\"}, inplace=True)\n",
        "        break\n",
        "else:\n",
        "    raise AssertionError(f\"Your CSV needs a text column. Tried {TEXT_CANDIDATES}.\")\n",
        "\n",
        "print(f\"Loaded {len(df):,} rows and {len(df.columns)} columns.\")\n",
        "df.head(2)\n"
      ],
      "metadata": {
        "id": "I-QQLP-aIPeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy, pandas as pd, math\n",
        "from wordfreq import zipf_frequency\n",
        "import nltk; nltk.download('wordnet'); nltk.download('omw-1.4')\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def is_concrete_lemma(lemma):\n",
        "    # Heuristic: “more concrete” if it has many physical-object synsets or very high frequency\n",
        "    syns = wn.synsets(lemma, pos=wn.NOUN) + wn.synsets(lemma, pos=wn.VERB) + wn.synsets(lemma, pos=wn.ADJ)\n",
        "    physical = [s for s in syns if any('physical' in str(h) or 'object' in str(h) for h in s.hypernyms())]\n",
        "    return (len(physical) >= 1) or (zipf_frequency(lemma, 'en') >= 4.5)\n",
        "\n",
        "def candidate_metaphor(doc):\n",
        "    rows = []\n",
        "    for sent in doc.sents:\n",
        "        for token in sent:\n",
        "            # Flag verbs/adjectives modifying abstract nouns, etc. (very conservative)\n",
        "            reason = None\n",
        "            if token.pos_ in {\"VERB\",\"ADJ\"}:\n",
        "                # look for head/dependent noun\n",
        "                nouns = [c for c in token.children if c.pos_==\"NOUN\"] + ([token.head] if token.head.pos_==\"NOUN\" else [])\n",
        "                for n in nouns:\n",
        "                    if is_concrete_lemma(token.lemma_) and not is_concrete_lemma(n.lemma_):\n",
        "                        reason = f\"CCO: {token.lemma_} (concrete) → {n.lemma_} (abstract)\"\n",
        "            if token.dep_==\"attr\" and token.head.pos_==\"NOUN\" and token.pos_==\"NOUN\":\n",
        "                # \"X is a Y\" pattern; if Y concrete and X abstract, mark Y\n",
        "                head = token.head\n",
        "                if is_concrete_lemma(token.lemma_) and not is_concrete_lemma(head.lemma_):\n",
        "                    reason = f\"Copula-CCO: {token.lemma_} (concrete) ≈ {head.lemma_} (abstract)\"\n",
        "            rows.append({\n",
        "                \"sent_text\": sent.text,\n",
        "                \"token\": token.text, \"lemma\": token.lemma_, \"pos\": token.pos_, \"dep\": token.dep_,\n",
        "                \"is_metaphor_candidate\": 1 if reason else 0, \"reason\": reason\n",
        "            })\n",
        "    return rows\n",
        "\n",
        "out = []\n",
        "for i, row in df.iterrows():\n",
        "    doc = nlp(str(row['text']))\n",
        "    rows = candidate_metaphor(doc)\n",
        "    for r in rows:\n",
        "        r.update({\"row_id\": i, \"chapter\": row.get(\"chapter\"), \"verse\": row.get(\"verse\")})\n",
        "    out.extend(rows)\n",
        "\n",
        "meta_df = pd.DataFrame(out)\n",
        "# Keep only flagged tokens for the working list (but save full annotated for audit)\n",
        "meta_all = os.path.join(BASE_DIR, \"metaphor_enriched_full.csv\")\n",
        "meta_flagged = os.path.join(BASE_DIR, \"metaphor_enriched.csv\")\n",
        "meta_df.to_csv(meta_all, index=False)\n",
        "meta_df[meta_df.is_metaphor_candidate==1].to_csv(meta_flagged, index=False)\n",
        "meta_flagged, meta_all\n"
      ],
      "metadata": {
        "id": "44PQ3g-iImaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 0 (fixed): installs + robust data loader ---\n",
        "\n",
        "# 0a) Install compatible packages (spaCy 3.8 supports pydantic v2; works with modern gradio)\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install \"spacy==3.8.2\" \"gradio>=4.41,<5\" \"textblob==0.18.0\" \"wordfreq==3.1.1\" \"nltk==3.9.1\" \"pandas==2.2.2\" \"matplotlib==3.9.0\"\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# 0b) Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# 0c) Find your Mozi CSV automatically (tries common places and falls back to a Drive-wide search)\n",
        "import os, pandas as pd, re\n",
        "from pathlib import Path\n",
        "\n",
        "SEARCH_BASES = [\n",
        "    \"/content/drive/MyDrive/Chinese Philosophy/Mozi\",\n",
        "    \"/content/drive/MyDrive/Chinese Philosophy\",\n",
        "    \"/content/drive/MyDrive/Mozi\",\n",
        "    \"/content/drive/MyDrive\",\n",
        "]\n",
        "PATTERNS = [\"Mozi_corrected.csv\", \"Mozi*.csv\"]\n",
        "\n",
        "cands = []\n",
        "for base in SEARCH_BASES:\n",
        "    p = Path(base)\n",
        "    if p.exists():\n",
        "        for pat in PATTERNS:\n",
        "            cands += list(p.rglob(pat))\n",
        "\n",
        "# If nothing found in common places, do a broader (but slower) search of MyDrive\n",
        "if not cands:\n",
        "    cands = list(Path(\"/content/drive/MyDrive\").rglob(\"Mozi*.csv\"))\n",
        "\n",
        "if not cands:\n",
        "    raise FileNotFoundError(\n",
        "        \"Couldn't find a Mozi CSV. Place one (e.g., 'Mozi_corrected.csv') in \"\n",
        "        \"/content/drive/MyDrive/Chinese Philosophy/Mozi and re-run this cell.\"\n",
        "    )\n",
        "\n",
        "# Prefer the most recently modified match\n",
        "cands = sorted(cands, key=lambda x: x.stat().st_mtime, reverse=True)\n",
        "MOZI_CSV = str(cands[0])\n",
        "print(\"Using CSV:\", MOZI_CSV)\n",
        "\n",
        "# 0d) Load + normalize columns\n",
        "df = pd.read_csv(MOZI_CSV)\n",
        "df.columns = [c.strip().lower() for c in df.columns]\n",
        "\n",
        "# Try to normalize the text column name to 'text'\n",
        "TEXT_CANDIDATES = [\"text\", \"translation\", \"english\", \"content\", \"verse_text\", \"line_text\"]\n",
        "for cand in TEXT_CANDIDATES:\n",
        "    if cand in df.columns:\n",
        "        if cand != \"text\":\n",
        "            df.rename(columns={cand: \"text\"}, inplace=True)\n",
        "        break\n",
        "else:\n",
        "    raise AssertionError(f\"Your CSV needs a text column. Tried {TEXT_CANDIDATES}.\")\n",
        "\n",
        "print(f\"Loaded {len(df):,} rows and {len(df.columns)} columns.\")\n",
        "df.head(2)\n"
      ],
      "metadata": {
        "id": "KEGJs2FkKt3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy, pandas as pd, math\n",
        "from wordfreq import zipf_frequency\n",
        "import nltk; nltk.download('wordnet'); nltk.download('omw-1.4')\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def is_concrete_lemma(lemma):\n",
        "    # Heuristic: “more concrete” if it has many physical-object synsets or very high frequency\n",
        "    syns = wn.synsets(lemma, pos=wn.NOUN) + wn.synsets(lemma, pos=wn.VERB) + wn.synsets(lemma, pos=wn.ADJ)\n",
        "    physical = [s for s in syns if any('physical' in str(h) or 'object' in str(h) for h in s.hypernyms())]\n",
        "    return (len(physical) >= 1) or (zipf_frequency(lemma, 'en') >= 4.5)\n",
        "\n",
        "def candidate_metaphor(doc):\n",
        "    rows = []\n",
        "    for sent in doc.sents:\n",
        "        for token in sent:\n",
        "            # Flag verbs/adjectives modifying abstract nouns, etc. (very conservative)\n",
        "            reason = None\n",
        "            if token.pos_ in {\"VERB\",\"ADJ\"}:\n",
        "                # look for head/dependent noun\n",
        "                nouns = [c for c in token.children if c.pos_==\"NOUN\"] + ([token.head] if token.head.pos_==\"NOUN\" else [])\n",
        "                for n in nouns:\n",
        "                    if is_concrete_lemma(token.lemma_) and not is_concrete_lemma(n.lemma_):\n",
        "                        reason = f\"CCO: {token.lemma_} (concrete) → {n.lemma_} (abstract)\"\n",
        "            if token.dep_==\"attr\" and token.head.pos_==\"NOUN\" and token.pos_==\"NOUN\":\n",
        "                # \"X is a Y\" pattern; if Y concrete and X abstract, mark Y\n",
        "                head = token.head\n",
        "                if is_concrete_lemma(token.lemma_) and not is_concrete_lemma(head.lemma_):\n",
        "                    reason = f\"Copula-CCO: {token.lemma_} (concrete) ≈ {head.lemma_} (abstract)\"\n",
        "            rows.append({\n",
        "                \"sent_text\": sent.text,\n",
        "                \"token\": token.text, \"lemma\": token.lemma_, \"pos\": token.pos_, \"dep\": token.dep_,\n",
        "                \"is_metaphor_candidate\": 1 if reason else 0, \"reason\": reason\n",
        "            })\n",
        "    return rows\n",
        "\n",
        "out = []\n",
        "for i, row in df.iterrows():\n",
        "    doc = nlp(str(row['text']))\n",
        "    rows = candidate_metaphor(doc)\n",
        "    for r in rows:\n",
        "        r.update({\"row_id\": i, \"chapter\": row.get(\"chapter\"), \"verse\": row.get(\"verse\")})\n",
        "    out.extend(rows)\n",
        "\n",
        "meta_df = pd.DataFrame(out)\n",
        "# Keep only flagged tokens for the working list (but save full annotated for audit)\n",
        "meta_all = os.path.join(BASE_DIR, \"metaphor_enriched_full.csv\")\n",
        "meta_flagged = os.path.join(BASE_DIR, \"metaphor_enriched.csv\")\n",
        "meta_df.to_csv(meta_all, index=False)\n",
        "meta_df[meta_df.is_metaphor_candidate==1].to_csv(meta_flagged, index=False)\n",
        "meta_flagged, meta_all\n"
      ],
      "metadata": {
        "id": "YquD9GBzK9zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "print(\"Restarting runtime so spaCy 3.8 + model are cleanly loaded…\")\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "id": "K9qCWj8iLVbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1 (safe) ---\n",
        "\n",
        "# 1) Imports + light checks\n",
        "import importlib, sys, os, math, pandas as pd\n",
        "import nltk; nltk.download('wordnet'); nltk.download('omw-1.4')\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "import spacy\n",
        "print(\"spaCy version:\", spacy.__version__)\n",
        "\n",
        "# If Step 0 variables don't exist (e.g., after restart), reload Drive + CSV quickly\n",
        "if \"df\" not in globals() or \"BASE_DIR\" not in globals():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    from pathlib import Path\n",
        "    SEARCH_BASES = [\n",
        "        \"/content/drive/MyDrive/Chinese Philosophy/Mozi\",\n",
        "        \"/content/drive/MyDrive/Chinese Philosophy\",\n",
        "        \"/content/drive/MyDrive\",\n",
        "    ]\n",
        "    cands = []\n",
        "    for base in SEARCH_BASES:\n",
        "        p = Path(base)\n",
        "        if p.exists():\n",
        "            cands += list(p.rglob(\"Mozi*.csv\"))\n",
        "    assert cands, \"Place a Mozi CSV (e.g., Mozi_corrected.csv) under MyDrive and re-run.\"\n",
        "    cands = sorted(cands, key=lambda x: x.stat().st_mtime, reverse=True)\n",
        "    MOZI_CSV = str(cands[0])\n",
        "    print(\"Using CSV:\", MOZI_CSV)\n",
        "    df = pd.read_csv(MOZI_CSV)\n",
        "    df.columns = [c.strip().lower() for c in df.columns]\n",
        "    for cand in [\"text\",\"translation\",\"english\",\"content\",\"verse_text\",\"line_text\"]:\n",
        "        if cand in df.columns:\n",
        "            if cand != \"text\": df.rename(columns={cand:\"text\"}, inplace=True)\n",
        "            break\n",
        "    assert \"text\" in df.columns, \"CSV must contain a text column.\"\n",
        "    BASE_DIR = \"/content/drive/MyDrive/Chinese Philosophy/Mozi\"\n",
        "    os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "# 2) Load spaCy model with a robust fallback\n",
        "def load_nlp():\n",
        "    try:\n",
        "        return spacy.load(\"en_core_web_sm\")\n",
        "    except Exception as e1:\n",
        "        try:\n",
        "            en_model = importlib.import_module(\"en_core_web_sm\")\n",
        "            return en_model.load()\n",
        "        except Exception as e2:\n",
        "            raise RuntimeError(\n",
        "                \"Failed to load en_core_web_sm. If you just installed it, \"\n",
        "                \"run the restart cell, then Step 0 again, then re-run this.\"\n",
        "            )\n",
        "\n",
        "nlp = load_nlp()\n",
        "print(\"Loaded pipeline:\", nlp.pipe_names)\n",
        "\n",
        "# 3) Concreteness heuristic (WordNet lexnames + frequency fallback)\n",
        "from wordfreq import zipf_frequency\n",
        "\n",
        "CONCRETE_LEX = {\"noun.artifact\",\"noun.object\",\"noun.substance\",\"noun.body\",\"noun.phenomenon\"}\n",
        "ABSTRACT_LEX  = {\"noun.attribute\",\"noun.cognition\",\"noun.communication\",\"noun.group\",\n",
        "                 \"noun.motive\",\"noun.act\",\"noun.state\",\"noun.time\"}\n",
        "\n",
        "def is_concrete_lemma(lemma:str):\n",
        "    syns = wn.synsets(lemma)\n",
        "    if any(s.lexname() in CONCRETE_LEX for s in syns):\n",
        "        return True\n",
        "    if any(s.lexname() in ABSTRACT_LEX for s in syns):\n",
        "        return False\n",
        "    # frequency fallback: very common content words tend to be literal in baseline usage\n",
        "    return zipf_frequency(lemma, 'en') >= 4.5\n",
        "\n",
        "# 4) Candidate detector (Category-Concord/Conflict cues)\n",
        "def candidates_from_doc(doc):\n",
        "    rows = []\n",
        "    for sent in doc.sents:\n",
        "        for tok in sent:\n",
        "            reason = None\n",
        "            # verb/adj applied to an abstract noun → metaphor candidate\n",
        "            if tok.pos_ in {\"VERB\",\"ADJ\"}:\n",
        "                nouns = [c for c in tok.children if c.pos_==\"NOUN\"]\n",
        "                if tok.head.pos_==\"NOUN\": nouns.append(tok.head)\n",
        "                for n in nouns:\n",
        "                    if is_concrete_lemma(tok.lemma_.lower()) and not is_concrete_lemma(n.lemma_.lower()):\n",
        "                        reason = f\"CCO: {tok.lemma_} (concrete) → {n.lemma_} (abstract)\"\n",
        "                        break\n",
        "            # copula “X is Y”: if Y is concrete and X abstract, flag Y\n",
        "            if not reason and tok.dep_==\"attr\" and tok.head.pos_==\"NOUN\" and tok.pos_==\"NOUN\":\n",
        "                head = tok.head\n",
        "                if is_concrete_lemma(tok.lemma_.lower()) and not is_concrete_lemma(head.lemma_.lower()):\n",
        "                    reason = f\"Copula-CCO: {tok.lemma_} (concrete) ≈ {head.lemma_} (abstract)\"\n",
        "            rows.append({\n",
        "                \"sent_text\": sent.text,\n",
        "                \"token\": tok.text, \"lemma\": tok.lemma_, \"pos\": tok.pos_, \"dep\": tok.dep_,\n",
        "                \"is_metaphor_candidate\": 1 if reason else 0, \"reason\": reason\n",
        "            })\n",
        "    return rows\n",
        "\n",
        "# 5) Efficient pass with nlp.pipe\n",
        "texts = df[\"text\"].astype(str).tolist()\n",
        "out_rows = []\n",
        "for i, doc in enumerate(nlp.pipe(texts, batch_size=256, n_process=2)):\n",
        "    for r in candidates_from_doc(doc):\n",
        "        r.update({\n",
        "            \"row_id\": i,\n",
        "            \"chapter\": df.get(\"chapter\", pd.Series([None]*len(df))).iloc[i] if \"chapter\" in df.columns else None,\n",
        "            \"verse\": df.get(\"verse\", pd.Series([None]*len(df))).iloc[i] if \"verse\" in df.columns else None\n",
        "        })\n",
        "        out_rows.append(r)\n",
        "\n",
        "meta_df = pd.DataFrame(out_rows)\n",
        "\n",
        "# 6) Save results\n",
        "meta_all = os.path.join(BASE_DIR, \"metaphor_enriched_full.csv\")\n",
        "meta_flagged = os.path.join(BASE_DIR, \"metaphor_enriched.csv\")\n",
        "meta_df.to_csv(meta_all, index=False)\n",
        "meta_df[meta_df.is_metaphor_candidate==1].to_csv(meta_flagged, index=False)\n",
        "print(\"Wrote:\\n \", meta_flagged, \"\\n \", meta_all)\n",
        "\n",
        "# quick peek\n",
        "meta_df[meta_df.is_metaphor_candidate==1].head(10)\n"
      ],
      "metadata": {
        "id": "d3LJyh8gLaIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Builds a lightweight UI to label metaphoricity (0–3) + source/target domains.\n",
        "\n",
        "import os, datetime, pandas as pd, gradio as gr\n",
        "\n",
        "# Reuse BASE_DIR from Step 0 if present; otherwise fall back to the standard path\n",
        "BASE_DIR = globals().get(\"BASE_DIR\", \"/content/drive/MyDrive/Chinese Philosophy/Mozi\")\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "flagged_path = os.path.join(BASE_DIR, \"metaphor_enriched.csv\")\n",
        "assert os.path.exists(flagged_path), f\"Missing {flagged_path}. Run Step 1 first.\"\n",
        "\n",
        "labels_csv = os.path.join(BASE_DIR, \"metaphor_labels.csv\")\n",
        "if not os.path.exists(labels_csv):\n",
        "    pd.DataFrame(columns=[\n",
        "        \"row_id\",\"chapter\",\"verse\",\"sent_text\",\"token\",\"lemma\",\n",
        "        \"metaphoricity_0to3\",\"source_domain\",\"target_domain\",\"notes\",\"timestamp\"\n",
        "    ]).to_csv(labels_csv, index=False)\n",
        "\n",
        "work = pd.read_csv(flagged_path).reset_index(drop=True)\n",
        "\n",
        "domain_choices = [\n",
        "    \"MECHANICS\",\"MEASUREMENT\",\"CRAFT_CONSTRUCTION\",\"WAR\",\n",
        "    \"ORGANIC_FAMILY\",\"MORAL_PURIFICATION\",\"JUDGMENT_BALANCE\",\n",
        "    \"OTHER\",\"UNSURE\"\n",
        "]\n",
        "met_levels = [0,1,2,3]\n",
        "\n",
        "def load_item(idx):\n",
        "    i = max(0, min(int(idx), len(work)-1))\n",
        "    r = work.iloc[i]\n",
        "    header = f\"Ch {r.get('chapter')} • Verse {r.get('verse')}\"\n",
        "    return i, f\"{header}\\n\\n{r['sent_text']}\", r['token'], r['lemma']\n",
        "\n",
        "def save_item(idx, level, src, tgt, notes):\n",
        "    r = work.iloc[int(idx)]\n",
        "    row = {\n",
        "        \"row_id\": int(r[\"row_id\"]), \"chapter\": r.get(\"chapter\"), \"verse\": r.get(\"verse\"),\n",
        "        \"sent_text\": r[\"sent_text\"], \"token\": r[\"token\"], \"lemma\": r[\"lemma\"],\n",
        "        \"metaphoricity_0to3\": int(level), \"source_domain\": src, \"target_domain\": tgt,\n",
        "        \"notes\": notes, \"timestamp\": datetime.datetime.utcnow().isoformat()\n",
        "    }\n",
        "    df_lab = pd.read_csv(labels_csv)\n",
        "    df_lab = pd.concat([df_lab, pd.DataFrame([row])], ignore_index=True)\n",
        "    df_lab.to_csv(labels_csv, index=False)\n",
        "    return f\"Saved ✓ row_id={row['row_id']} ({row['token']})\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Mozi metaphor labeler (token-level)\")\n",
        "    idx = gr.Number(value=0, precision=0, label=\"Row index\")\n",
        "    sent = gr.Textbox(label=\"Sentence\", lines=5)\n",
        "    tok  = gr.Textbox(label=\"Token\")\n",
        "    lem  = gr.Textbox(label=\"Lemma\")\n",
        "    level = gr.Radio(met_levels, value=2, label=\"Metaphoricity (0 literal → 3 strongly metaphorical)\")\n",
        "    src = gr.Dropdown(domain_choices, value=\"MECHANICS\", label=\"Source domain\")\n",
        "    tgt = gr.Dropdown(domain_choices, value=\"MEASUREMENT\", label=\"Target domain\")\n",
        "    notes = gr.Textbox(label=\"Notes\")\n",
        "    status = gr.Textbox(label=\"Status\")\n",
        "\n",
        "    btn_load = gr.Button(\"Load\")\n",
        "    btn_save = gr.Button(\"Save label\")\n",
        "    btn_next = gr.Button(\"Next →\")\n",
        "\n",
        "    btn_load.click(load_item, inputs=[idx], outputs=[idx, sent, tok, lem])\n",
        "    btn_save.click(save_item, inputs=[idx, level, src, tgt, notes], outputs=[status])\n",
        "    btn_next.click(lambda i: int(i)+1, inputs=[idx], outputs=[idx]).then(load_item, [idx],[idx, sent, tok, lem])\n",
        "\n",
        "demo.launch(share=False)\n"
      ],
      "metadata": {
        "id": "vowDeX-UMe47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Produces metaphor_enriched_labeled.csv where human labels (if present) override the heuristic flag.\n",
        "\n",
        "import pandas as pd, os\n",
        "\n",
        "BASE_DIR = globals().get(\"BASE_DIR\", \"/content/drive/MyDrive/Chinese Philosophy/Mozi\")\n",
        "meta_all_path = os.path.join(BASE_DIR, \"metaphor_enriched_full.csv\")\n",
        "labels_csv = os.path.join(BASE_DIR, \"metaphor_labels.csv\")\n",
        "out_path = os.path.join(BASE_DIR, \"metaphor_enriched_labeled.csv\")\n",
        "\n",
        "meta_all = pd.read_csv(meta_all_path)\n",
        "labels = pd.read_csv(labels_csv) if os.path.exists(labels_csv) else pd.DataFrame()\n",
        "\n",
        "if len(labels):\n",
        "    lbl_cols = [\"row_id\",\"token\",\"lemma\",\"metaphoricity_0to3\",\"source_domain\",\"target_domain\",\"notes\"]\n",
        "    merged = meta_all.merge(labels[lbl_cols], on=[\"row_id\",\"token\",\"lemma\"], how=\"left\")\n",
        "    # If human label exists, set candidate=1 when label>=1; else keep heuristic\n",
        "    merged[\"is_metaphor_human\"] = merged[\"metaphoricity_0to3\"].fillna(-1).astype(int)\n",
        "    merged[\"is_metaphor_final\"] = merged.apply(\n",
        "        lambda r: 1 if r[\"is_metaphor_human\"]>=1 else (r[\"is_metaphor_candidate\"] if r[\"is_metaphor_human\"]==-1 else 0),\n",
        "        axis=1\n",
        "    )\n",
        "else:\n",
        "    merged = meta_all.copy()\n",
        "    merged[\"is_metaphor_human\"] = -1\n",
        "    merged[\"is_metaphor_final\"] = merged[\"is_metaphor_candidate\"]\n",
        "\n",
        "merged.to_csv(out_path, index=False)\n",
        "print(\"Wrote:\", out_path, \"rows:\", len(merged))\n",
        "merged.head(3)\n"
      ],
      "metadata": {
        "id": "cX40C3o2MjeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 4: CMI-style discovery (systemic mappings) ===\n",
        "# Compares verb–noun selectional preferences in a source corpus vs. Mozi,\n",
        "# then surfaces verbs that are characteristic of the source domain and also\n",
        "# used with key Mozi nouns (evidence for conceptual metaphors).\n",
        "# Output: cmi_candidates.csv under BASE_DIR.\n",
        "\n",
        "import os, glob, math, pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# 1) Load spaCy (with fallback download of the model if needed)\n",
        "try:\n",
        "    import spacy\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"spacy==3.8.2\"], check=True)\n",
        "    import spacy\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# 2) Ensure Drive + locate Mozi CSV if df not already in memory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "BASE_DIR = globals().get(\"BASE_DIR\", \"/content/drive/MyDrive/Chinese Philosophy/Mozi\")\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "if \"df\" not in globals():\n",
        "    from pathlib import Path\n",
        "    cands = []\n",
        "    for base in [BASE_DIR, \"/content/drive/MyDrive/Chinese Philosophy\", \"/content/drive/MyDrive\"]:\n",
        "        p = Path(base)\n",
        "        if p.exists():\n",
        "            cands += list(p.rglob(\"Mozi*.csv\"))\n",
        "    assert cands, \"No Mozi CSV found. Place e.g. 'Mozi_corrected.csv' under MyDrive.\"\n",
        "    cands = sorted(cands, key=lambda x: x.stat().st_mtime, reverse=True)\n",
        "    df = pd.read_csv(cands[0])\n",
        "    df.columns = [c.strip().lower() for c in df.columns]\n",
        "    for cand in [\"text\",\"translation\",\"english\",\"content\",\"verse_text\",\"line_text\"]:\n",
        "        if cand in df.columns:\n",
        "            if cand != \"text\": df.rename(columns={cand:\"text\"}, inplace=True)\n",
        "            break\n",
        "    assert \"text\" in df.columns, \"CSV must include a 'text' column.\"\n",
        "\n",
        "# 3) Load source corpus (.txt files you curated)\n",
        "SRC_DIR = os.path.join(BASE_DIR, \"source_corpus\")\n",
        "os.makedirs(SRC_DIR, exist_ok=True)\n",
        "src_files = glob.glob(os.path.join(SRC_DIR, \"*.txt\"))\n",
        "assert src_files, f\"No source texts in {SRC_DIR}. Add one or more .txt files (e.g., mechanics chapters) and re-run.\"\n",
        "\n",
        "# 4) Extract verb–noun pairs from texts\n",
        "def vn_pairs_from_texts(texts):\n",
        "    cnt = Counter()\n"
      ],
      "metadata": {
        "id": "V_STVBq2MolG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 4 (auto-bootstrap): build source_corpus and run CMI discovery ===\n",
        "import os, re, glob, math, pandas as pd\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "# ---- Ensure Drive + find Mozi CSV (reuses earlier vars if present) ----\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "BASE_DIR = globals().get(\"BASE_DIR\", \"/content/drive/MyDrive/Chinese Philosophy/Mozi\")\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "if \"df\" not in globals():\n",
        "    # locate Mozi CSV automatically\n",
        "    cands = []\n",
        "    for base in [BASE_DIR, \"/content/drive/MyDrive/Chinese Philosophy\", \"/content/drive/MyDrive\"]:\n",
        "        p = Path(base)\n",
        "        if p.exists():\n",
        "            cands += list(p.rglob(\"Mozi*.csv\"))\n",
        "    assert cands, \"No Mozi CSV found. Put e.g. 'Mozi_corrected.csv' under MyDrive and rerun.\"\n",
        "    cands = sorted(cands, key=lambda x: x.stat().st_mtime, reverse=True)\n",
        "    MOZI_CSV = str(cands[0])\n",
        "    print(\"Using CSV:\", MOZI_CSV)\n",
        "    df = pd.read_csv(MOZI_CSV)\n",
        "    df.columns = [c.strip().lower() for c in df.columns]\n",
        "    for cand in [\"text\",\"translation\",\"english\",\"content\",\"verse_text\",\"line_text\"]:\n",
        "        if cand in df.columns:\n",
        "            if cand != \"text\": df.rename(columns={cand:\"text\"}, inplace=True)\n",
        "            break\n",
        "    assert \"text\" in df.columns, \"CSV must include a 'text' column.\"\n",
        "\n",
        "# ---- Create/seed source_corpus if empty ----\n",
        "SRC_DIR = os.path.join(BASE_DIR, \"source_corpus\")\n",
        "os.makedirs(SRC_DIR, exist_ok=True)\n",
        "\n",
        "src_files = glob.glob(os.path.join(SRC_DIR, \"*.txt\"))\n",
        "if not src_files:\n",
        "    print(\"No .txt in source_corpus — bootstrapping from Mozi text…\")\n",
        "    mechanics_kw = r\"\\b(lever|levers|pulley|axle|hinge|gear|wheel|carpenter|crafts?man|craft|carpentry|build|construct|construction|chisel|saw|plane|hammer|drill|carve|cut|join|measure|weigh|balance|scale|standard|plumb|square|level)\\b\"\n",
        "    measurement_kw = r\"\\b(measure|measured|measuring|weight|weigh|weighed|balance|balanced|scale|standard|rule|gauge|calibrate|calibration|yardstick|plumb|square|level)\\b\"\n",
        "\n",
        "    texts = df[\"text\"].astype(str).tolist()\n",
        "    mech_lines = [t for t in texts if re.search(mechanics_kw, t, flags=re.I)]\n",
        "    meas_lines = [t for t in texts if re.search(measurement_kw, t, flags=re.I)]\n",
        "\n",
        "    # write bootstrapped files if any lines found\n",
        "    if mech_lines:\n",
        "        with open(os.path.join(SRC_DIR, \"mechanics_bootstrap.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"\\n\".join(mech_lines))\n",
        "    if meas_lines:\n",
        "        with open(os.path.join(SRC_DIR, \"measurement_bootstrap.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"\\n\".join(meas_lines))\n",
        "\n",
        "    src_files = glob.glob(os.path.join(SRC_DIR, \"*.txt\"))\n",
        "    if not src_files:\n",
        "        raise AssertionError(\n",
        "            f\"Bootstrap found no mechanics/measurement lines. \"\n",
        "            f\"Add at least one .txt to {SRC_DIR} and rerun.\"\n",
        "        )\n",
        "\n",
        "print(f\"Source files: {len(src_files)} → {[Path(p).name for p in src_files][:5]}\")\n",
        "\n",
        "# ---- Load spaCy (and model if needed) ----\n",
        "try:\n",
        "    import spacy\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"spacy==3.8.2\"], check=True)\n",
        "    import spacy\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# ---- Helper: extract verb–noun pairs ----\n",
        "def vn_pairs_from_texts(texts):\n",
        "    cnt = Counter()\n",
        "    for doc in nlp.pipe(texts, batch_size=128, n_process=2):\n",
        "        for tok in doc:\n",
        "            if tok.pos_ == \"VERB\":\n",
        "                for c in tok.children:\n",
        "                    if c.pos_ == \"NOUN\" and c.dep_ in (\"dobj\",\"obj\",\"nsubj\",\"nsubjpass\"):\n",
        "                        cnt[(tok.lemma_.lower(), c.lemma_.lower())] += 1\n",
        "    return cnt\n",
        "\n",
        "# ---- Build source (bootstrapped) and target (Mozi) pair counts ----\n",
        "src_texts = [open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\").read() for fp in src_files]\n",
        "tgt_texts = df[\"text\"].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "src_pairs = vn_pairs_from_texts(src_texts)\n",
        "tgt_pairs = vn_pairs_from_texts(tgt_texts)\n",
        "\n",
        "print(\"Counts → src_pairs:\", sum(src_pairs.values()), \"tgt_pairs:\", sum(tgt_pairs.values()))\n",
        "\n",
        "# ---- PMI-like selectional preference on source ----\n",
        "verb_tot_src, noun_tot_src = Counter(), Counter()\n",
        "N_src = sum(src_pairs.values())\n",
        "for (v, n), c in src_pairs.items():\n",
        "    verb_tot_src[v] += c\n",
        "    noun_tot_src[n] += c\n",
        "\n",
        "def pmi_src(v, n, c):\n",
        "    if c < 3 or N_src == 0:\n",
        "        return None\n",
        "    pv = verb_tot_src[v] / N_src\n",
        "    pn = noun_tot_src[n] / N_src\n",
        "    pvn = c / N_src\n",
        "    return math.log2((pvn / (pv * pn + 1e-12)) + 1e-12)\n",
        "\n",
        "# ---- Look for same verbs with key Mozi nouns ----\n",
        "mozi_target_nouns = {\n",
        "    \"state\",\"ruler\",\"government\",\"law\",\"people\",\"heaven\",\n",
        "    \"virtue\",\"order\",\"benefit\",\"punishment\"\n",
        "}\n",
        "\n",
        "rows = []\n",
        "for (v, n), c in src_pairs.items():\n",
        "    score = pmi_src(v, n, c)\n",
        "    if score is None or score < 2.0:\n",
        "        continue\n",
        "    for (vv, nn), cc in tgt_pairs.items():\n",
        "        if vv == v and nn in mozi_target_nouns and cc >= 2:\n",
        "            rows.append({\n",
        "                \"verb\": v,\n",
        "                \"src_noun\": n,\n",
        "                \"tgt_noun\": nn,\n",
        "                \"src_pair_count\": c,\n",
        "                \"tgt_pair_count\": cc,\n",
        "                \"src_pmi\": round(score, 3)\n",
        "            })\n",
        "\n",
        "cmi_df = (pd.DataFrame(rows)\n",
        "          .sort_values([\"src_pmi\",\"tgt_pair_count\"], ascending=[False, False])\n",
        "          .drop_duplicates())\n",
        "\n",
        "out_path = os.path.join(BASE_DIR, \"cmi_candidates.csv\")\n",
        "cmi_df.to_csv(out_path, index=False)\n",
        "print(\"Wrote:\", out_path, \"rows:\", len(cmi_df))\n",
        "cmi_df.head(20)\n"
      ],
      "metadata": {
        "id": "eDku_Rr3NfPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect and summarize cmi_candidates.csv\n",
        "import os, pandas as pd\n",
        "\n",
        "BASE_DIR = globals().get(\"BASE_DIR\", \"/content/drive/MyDrive/Chinese Philosophy/Mozi\")\n",
        "cmi_path = os.path.join(BASE_DIR, \"cmi_candidates.csv\")\n",
        "assert os.path.exists(cmi_path), f\"Missing {cmi_path}. Re-run Step 4.\"\n",
        "\n",
        "cmi = pd.read_csv(cmi_path)\n",
        "print(\"Rows:\", len(cmi))\n",
        "display(cmi.head(10))\n",
        "\n",
        "# Summaries\n",
        "by_verb = (cmi.groupby(\"verb\")\n",
        "             .agg(target_nouns=(\"tgt_noun\",\"nunique\"),\n",
        "                  total_pairs=(\"tgt_pair_count\",\"sum\"),\n",
        "                  avg_src_pmi=(\"src_pmi\",\"mean\"))\n",
        "             .sort_values([\"target_nouns\",\"total_pairs\",\"avg_src_pmi\"], ascending=False))\n",
        "\n",
        "by_mapping = (cmi.assign(score=cmi[\"src_pmi\"]*cmi[\"tgt_pair_count\"])\n",
        "                .sort_values([\"score\",\"src_pmi\",\"tgt_pair_count\"], ascending=False))\n",
        "\n",
        "summary_path = os.path.join(BASE_DIR, \"cmi_summary.csv\")\n",
        "with pd.ExcelWriter(os.path.join(BASE_DIR, \"cmi_summary.xlsx\")) as xw:\n",
        "    by_verb.to_excel(xw, sheet_name=\"by_verb\")\n",
        "    by_mapping.to_excel(xw, sheet_name=\"by_mapping\", index=False)\n",
        "by_verb.head(15), by_mapping.head(15), summary_path\n"
      ],
      "metadata": {
        "id": "EBIiWiCXOJ9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull example sentences from Mozi where (verb, tgt_noun) co-occur; save a small sample.\n",
        "import os, pandas as pd, spacy\n",
        "from collections import defaultdict\n",
        "\n",
        "BASE_DIR = globals().get(\"BASE_DIR\", \"/content/drive/MyDrive/Chinese Philosophy/Mozi\")\n",
        "cmi = pd.read_csv(os.path.join(BASE_DIR, \"cmi_candidates.csv\"))\n",
        "\n",
        "# Load text if not in memory\n",
        "if \"df\" not in globals():\n",
        "    import glob\n",
        "    mozi_csv = sorted(glob.glob(os.path.join(BASE_DIR, \"Mozi*.csv\")), key=os.path.getmtime, reverse=True)[0]\n",
        "    df = pd.read_csv(mozi_csv)\n",
        "    df.columns = [c.strip().lower() for c in df.columns]\n",
        "    for cand in [\"text\",\"translation\",\"english\",\"content\",\"verse_text\",\"line_text\"]:\n",
        "        if cand in df.columns:\n",
        "            if cand != \"text\": df.rename(columns={cand:\"text\"}, inplace=True)\n",
        "            break\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Top 10 verb→tgt_noun pairs by score\n",
        "pairs = (cmi.assign(score=cmi[\"src_pmi\"]*cmi[\"tgt_pair_count\"])\n",
        "            .sort_values([\"score\",\"src_pmi\",\"tgt_pair_count\"], ascending=False)\n",
        "            .head(10))[[\"verb\",\"tgt_noun\"]].drop_duplicates().values.tolist()\n",
        "\n",
        "evidence = []\n",
        "for i, doc in enumerate(nlp.pipe(df[\"text\"].astype(str).tolist(), batch_size=128, n_process=2, disable=[\"ner\"])):\n",
        "    lemmas = {t.lemma_.lower() for t in doc}\n",
        "    for v, n in pairs:\n",
        "        if v in lemmas and n in lemmas:\n",
        "            evidence.append({\n",
        "                \"row_id\": i,\n",
        "                \"chapter\": df.get(\"chapter\", None)[i] if \"chapter\" in df.columns else None,\n",
        "                \"verse\": df.get(\"verse\", None)[i] if \"verse\" in df.columns else None,\n",
        "                \"verb\": v, \"tgt_noun\": n,\n",
        "                \"sentence\": doc.text\n",
        "            })\n",
        "\n",
        "ev_df = pd.DataFrame(evidence).drop_duplicates().groupby([\"verb\",\"tgt_noun\"]).head(3)\n",
        "ev_path = os.path.join(BASE_DIR, \"cmi_evidence_samples.csv\")\n",
        "ev_df.to_csv(ev_path, index=False)\n",
        "print(\"Wrote:\", ev_path, \"rows:\", len(ev_df))\n",
        "ev_df.head(10)\n"
      ],
      "metadata": {
        "id": "n9RbAZQfOL2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Joins metaphor_enriched.csv with BERTopic assignments (if found) to plot a heatmap.\n",
        "# Fallback: if no topics file found, groups by 'chapter'.\n",
        "\n",
        "import os, glob, pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
        "\n",
        "BASE_DIR = globals().get(\"BASE_DIR\", \"/content/drive/MyDrive/Chinese Philosophy/Mozi\")\n",
        "meta_path = os.path.join(BASE_DIR, \"metaphor_enriched.csv\")\n",
        "assert os.path.exists(meta_path), f\"Missing {meta_path}. Run Step 1 first.\"\n",
        "meta = pd.read_csv(meta_path)\n",
        "\n",
        "# Simple family classifier from 'reason' string\n",
        "def family_from_reason(r):\n",
        "    r = str(r).lower()\n",
        "    if any(k in r for k in [\"weigh\",\"balance\",\"measure\",\"scale\",\"standard\",\"plumb\",\"square\",\"level\"]):\n",
        "        return \"MEASUREMENT\"\n",
        "    if any(k in r for k in [\"build\",\"construct\",\"craft\",\"carpenter\",\"join\",\"chisel\",\"plane\",\"saw\",\"hammer\",\"drill\"]):\n",
        "        return \"CRAFT_CONSTRUCTION\"\n",
        "    return \"OTHER\"\n",
        "\n",
        "meta = meta.loc[meta[\"is_metaphor_candidate\"]==1].copy()\n",
        "meta[\"family\"] = meta[\"reason\"].apply(family_from_reason)\n",
        "\n",
        "# Load Mozi text to get row_id→chapter mapping (and to align with topics)\n",
        "if \"df\" not in globals():\n",
        "    mozi_csv = sorted(glob.glob(os.path.join(BASE_DIR, \"Mozi*.csv\")), key=os.path.getmtime, reverse=True)[0]\n",
        "    df = pd.read_csv(mozi_csv)\n",
        "    df.columns = [c.strip().lower() for c in df.columns]\n",
        "    for cand in [\"text\",\"translation\",\"english\",\"content\",\"verse_text\",\"line_text\"]:\n",
        "        if cand in df.columns:\n",
        "            if cand != \"text\": df.rename(columns={cand:\"text\"}, inplace=True)\n",
        "            break\n",
        "df = df.reset_index().rename(columns={\"index\":\"row_id\"})\n",
        "\n",
        "# Try to find a BERTopic assignments CSV\n",
        "topic_file = None\n",
        "for path in [BASE_DIR, \"/content/drive/MyDrive/Chinese Philosophy\", \"/content/drive/MyDrive\"]:\n",
        "    hits = glob.glob(os.path.join(path, \"*bertopic*assign*.csv\")) + glob.glob(os.path.join(path, \"bertopic_assignments.csv\"))\n",
        "    if hits:\n",
        "        topic_file = sorted(hits, key=os.path.getmtime, reverse=True)[0]\n",
        "        break\n",
        "\n",
        "if topic_file:\n",
        "    topics = pd.read_csv(topic_file)\n",
        "    # Heuristics for column names\n",
        "    cand_cols = {\"row_id\",\"doc_id\",\"index\",\"i\"}\n",
        "    id_col = next((c for c in topics.columns if c in cand_cols), None)\n",
        "    topic_col = next((c for c in topics.columns if \"topic\" in c.lower()), None)\n",
        "    assert id_col and topic_col, f\"Could not infer id/topic columns from {topic_file}.\"\n",
        "    topics = topics.rename(columns={id_col:\"row_id\", topic_col:\"topic\"})\n",
        "    grouping = \"topic\"\n",
        "    base = df.merge(topics[[\"row_id\",\"topic\"]], on=\"row_id\", how=\"left\")\n",
        "    base[\"topic\"] = base[\"topic\"].fillna(-1).astype(int)\n",
        "else:\n",
        "    # Fallback: group by chapter\n",
        "    grouping = \"chapter\"\n",
        "    base = df.copy()\n",
        "    if \"chapter\" not in base.columns:\n",
        "        base[\"chapter\"] = pd.cut(base[\"row_id\"], bins=10, labels=[f\"seg{i}\" for i in range(10)])\n",
        "\n",
        "joined = (base[[\"row_id\", grouping]]\n",
        "          .merge(meta[[\"row_id\",\"family\"]], on=\"row_id\", how=\"left\")\n",
        "          .dropna(subset=[\"family\"]))\n",
        "\n",
        "pivot = (joined.groupby([grouping,\"family\"]).size()\n",
        "         .unstack(fill_value=0)\n",
        "         .reindex(columns=sorted(joined[\"family\"].unique())))\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.imshow(pivot.values, aspect='auto')\n",
        "plt.xticks(range(pivot.shape[1]), pivot.columns, rotation=45, ha='right')\n",
        "plt.yticks(range(pivot.shape[0]), pivot.index)\n",
        "plt.title(f\"Metaphor family density by {grouping}\")\n",
        "plt.colorbar(label=\"count\")\n",
        "plt.tight_layout()\n",
        "\n",
        "out_png = os.path.join(BASE_DIR, f\"metaphor_heatmap_by_{grouping}.png\")\n",
        "plt.savefig(out_png, dpi=160)\n",
        "print(\"Wrote:\", out_png)\n",
        "pivot.head(10)\n"
      ],
      "metadata": {
        "id": "G8YTD2PFPA87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 5 (no labels): auto-derive metaphor families from CMI + lemmas and plot ---\n",
        "\n",
        "import os, glob, math, pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
        "\n",
        "BASE_DIR = globals().get(\"BASE_DIR\", \"/content/drive/MyDrive/Chinese Philosophy/Mozi\")\n",
        "\n",
        "# 1) Load metaphor candidates\n",
        "meta_path_labeled = os.path.join(BASE_DIR, \"metaphor_enriched_labeled.csv\")\n",
        "meta_path_flagged = os.path.join(BASE_DIR, \"metaphor_enriched.csv\")\n",
        "meta = pd.read_csv(meta_path_labeled if os.path.exists(meta_path_labeled) else meta_path_flagged)\n",
        "\n",
        "# keep only flagged candidates (no human labels yet)\n",
        "if \"is_metaphor_final\" in meta.columns:\n",
        "    meta = meta[meta[\"is_metaphor_final\"]==1].copy()\n",
        "else:\n",
        "    meta = meta[meta[\"is_metaphor_candidate\"]==1].copy()\n",
        "\n",
        "# 2) Load Mozi text for row_id → chapter mapping\n",
        "if \"df\" not in globals():\n",
        "    mozi_csv = sorted(glob.glob(os.path.join(BASE_DIR, \"Mozi*.csv\")), key=os.path.getmtime, reverse=True)[0]\n",
        "    df = pd.read_csv(mozi_csv)\n",
        "    df.columns = [c.strip().lower() for c in df.columns]\n",
        "    for cand in [\"text\",\"translation\",\"english\",\"content\",\"verse_text\",\"line_text\"]:\n",
        "        if cand in df.columns:\n",
        "            if cand != \"text\": df.rename(columns={cand:\"text\"}, inplace=True)\n",
        "            break\n",
        "df = df.reset_index().rename(columns={\"index\":\"row_id\"})\n",
        "if \"chapter\" not in df.columns:\n",
        "    # fallback grouping if chapter missing\n",
        "    bins = 10\n",
        "    df[\"chapter\"] = pd.cut(df[\"row_id\"], bins=bins, labels=[f\"seg{i:02d}\" for i in range(bins)])\n",
        "\n",
        "# 3) Seed lexicons + auto-expand from CMI\n",
        "cmi_path = os.path.join(BASE_DIR, \"cmi_candidates.csv\")\n",
        "MEAS_NOUNS = {\n",
        "    \"measure\",\"weight\",\"balance\",\"scale\",\"standard\",\"plumb\",\"square\",\"level\",\"rule\",\"gauge\",\"yardstick\"\n",
        "}\n",
        "CRAFT_MECH_NOUNS = {\n",
        "    \"lever\",\"pulley\",\"axle\",\"wheel\",\"hinge\",\"gear\",\"tool\",\"chisel\",\"saw\",\"plane\",\"hammer\",\"drill\",\n",
        "    \"timber\",\"wood\",\"carpenter\",\"craftsman\",\"craft\",\"wedge\",\"rope\",\"plank\",\"beam\",\"joinery\",\"mortise\",\"tenon\"\n",
        "}\n",
        "\n",
        "MEAS_LEM = {\"measure\",\"weigh\",\"balance\",\"scale\",\"standard\",\"gauge\",\"calibrate\",\"level\",\"square\",\"plumb\",\"rule\"}\n",
        "CRAFT_LEM = {\"build\",\"construct\",\"craft\",\"carpenter\",\"join\",\"chisel\",\"plane\",\"saw\",\"hammer\",\"drill\",\n",
        "             \"carve\",\"cut\",\"assemble\",\"erect\",\"repair\",\"polish\",\"shape\",\"file\",\"hew\",\"mortise\",\"tenon\"}\n",
        "\n",
        "if os.path.exists(cmi_path):\n",
        "    cmi = pd.read_csv(cmi_path)\n",
        "    meas_auto = set(cmi.loc[cmi[\"src_noun\"].str.lower().isin(MEAS_NOUNS), \"verb\"].str.lower())\n",
        "    craft_auto = set(cmi.loc[cmi[\"src_noun\"].str.lower().isin(CRAFT_MECH_NOUNS), \"verb\"].str.lower())\n",
        "    MEAS_LEM |= meas_auto\n",
        "    CRAFT_LEM |= craft_auto\n",
        "    print(\"Auto-added measurement verbs from CMI:\", sorted(list(meas_auto))[:30])\n",
        "    print(\"Auto-added craft/mechanics verbs from CMI:\", sorted(list(craft_auto))[:30])\n",
        "else:\n",
        "    print(\"No cmi_candidates.csv found; using seed lexicons only.\")\n",
        "\n",
        "# 4) Family classifier (lemma-first, then token string fallback)\n",
        "def family_from_row(r):\n",
        "    lem = str(r.get(\"lemma\",\"\")).lower()\n",
        "    tok = str(r.get(\"token\",\"\")).lower()\n",
        "    if lem in MEAS_LEM or any(k in tok for k in MEAS_LEM):\n",
        "        return \"MEASUREMENT\"\n",
        "    if lem in CRAFT_LEM or any(k in tok for k in CRAFT_LEM):\n",
        "        return \"CRAFT_CONSTRUCTION\"\n",
        "    return \"OTHER\"\n",
        "\n",
        "meta[\"family\"] = meta.apply(family_from_row, axis=1)\n",
        "\n",
        "# 5) Aggregate to sentence-level to avoid multi-token inflation\n",
        "sent_level = (meta.groupby([\"row_id\",\"family\"])\n",
        "                  .size()\n",
        "                  .reset_index(name=\"hits\"))\n",
        "\n",
        "# 6) Join with chapter and build pivots\n",
        "grouping = \"chapter\"\n",
        "base = df[[\"row_id\", grouping]].copy()\n",
        "joined = base.merge(sent_level, on=\"row_id\", how=\"left\").dropna(subset=[\"family\"])\n",
        "\n",
        "pivot_counts = (joined.groupby([grouping,\"family\"])[\"hits\"]\n",
        "                     .sum()\n",
        "                     .unstack(fill_value=0))\n",
        "\n",
        "chapter_sizes = df.groupby(grouping).size()\n",
        "pivot_rate = pivot_counts.div(chapter_sizes, axis=0) * 100.0  # per 100 sentences\n",
        "\n",
        "# 7) Plot and save\n",
        "def heatmap(mat, title, out_name, label):\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.imshow(mat.values, aspect='auto')\n",
        "    plt.xticks(range(mat.shape[1]), mat.columns, rotation=45, ha='right')\n",
        "    plt.yticks(range(mat.shape[0]), mat.index)\n",
        "    plt.title(title)\n",
        "    plt.colorbar(label=label)\n",
        "    plt.tight_layout()\n",
        "    out_png = os.path.join(BASE_DIR, out_name)\n",
        "    plt.savefig(out_png, dpi=160)\n",
        "    print(\"Wrote:\", out_png)\n",
        "\n",
        "heatmap(pivot_counts, \"Metaphor family density by chapter (counts)\", \"metaphor_heatmap_counts.png\", \"count\")\n",
        "heatmap(pivot_rate,   \"Metaphor family density by chapter (normalized)\", \"metaphor_heatmap_normalized.png\", \"per 100 sentences\")\n",
        "\n",
        "print(\"\\nTotals (raw counts):\")\n",
        "print(pivot_counts.sum(0).sort_values(ascending=False))\n",
        "print(\"\\nTotals (normalized sum):\")\n",
        "print(pivot_rate.sum(0).sort_values(ascending=False))\n"
      ],
      "metadata": {
        "id": "KMhoPRMWQOM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 5 (fixed): unique row_id, filtered CMI verbs, and new heatmaps ---\n",
        "\n",
        "import os, glob, pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
        "\n",
        "BASE_DIR = globals().get(\"BASE_DIR\", \"/content/drive/MyDrive/Chinese Philosophy/Mozi\")\n",
        "\n",
        "# 1) Load metaphor candidates\n",
        "meta_path_labeled = os.path.join(BASE_DIR, \"metaphor_enriched_labeled.csv\")\n",
        "meta_path_flagged = os.path.join(BASE_DIR, \"metaphor_enriched.csv\")\n",
        "meta = pd.read_csv(meta_path_labeled if os.path.exists(meta_path_labeled) else meta_path_flagged)\n",
        "meta = meta[(meta.get(\"is_metaphor_final\", meta.get(\"is_metaphor_candidate\", 0))==1)].copy()\n",
        "\n",
        "# 2) Load Mozi text and ensure a SINGLE, clean row_id that matches meta\n",
        "if \"df\" not in globals():\n",
        "    mozi_csv = sorted(glob.glob(os.path.join(BASE_DIR, \"Mozi*.csv\")), key=os.path.getmtime, reverse=True)[0]\n",
        "    df = pd.read_csv(mozi_csv)\n",
        "\n",
        "# normalize columns + drop duplicate-named columns\n",
        "df.columns = [c.strip().lower() for c in df.columns]\n",
        "df = df.loc[:, ~pd.Index(df.columns).duplicated()]\n",
        "\n",
        "# rebuild row_id as 0..n-1 (guaranteed unique & aligned with Step 1’s enumerate)\n",
        "df = df.reset_index(drop=True)\n",
        "df[\"row_id\"] = np.arange(len(df))\n",
        "\n",
        "# chapter fallback\n",
        "if \"chapter\" not in df.columns:\n",
        "    bins = 10\n",
        "    df[\"chapter\"] = pd.cut(df[\"row_id\"], bins=bins, labels=[f\"seg{i:02d}\" for i in range(bins)])\n",
        "\n",
        "# 3) Build/expand lexicons from CMI, but filter out generic/high-frequency verbs\n",
        "MEAS_NOUNS = {\"measure\",\"weight\",\"balance\",\"scale\",\"standard\",\"plumb\",\"square\",\"level\",\"rule\",\"gauge\",\"yardstick\"}\n",
        "CRAFT_MECH_NOUNS = {\"lever\",\"pulley\",\"axle\",\"wheel\",\"hinge\",\"gear\",\"tool\",\"chisel\",\"saw\",\"plane\",\"hammer\",\"drill\",\n",
        "                    \"timber\",\"wood\",\"carpenter\",\"craftsman\",\"craft\",\"wedge\",\"rope\",\"plank\",\"beam\",\"joinery\",\"mortise\",\"tenon\"}\n",
        "\n",
        "MEAS_LEM = {\"measure\",\"weigh\",\"balance\",\"scale\",\"standard\",\"gauge\",\"calibrate\",\"level\",\"square\",\"plumb\",\"rule\"}\n",
        "CRAFT_LEM = {\"build\",\"construct\",\"craft\",\"carpenter\",\"join\",\"chisel\",\"plane\",\"saw\",\"hammer\",\"drill\",\n",
        "             \"carve\",\"cut\",\"assemble\",\"erect\",\"repair\",\"polish\",\"shape\",\"file\",\"hew\",\"mortise\",\"tenon\"}\n",
        "\n",
        "STOP_VERBS = {\"be\",\"have\",\"do\",\"get\",\"take\",\"lose\",\"make\"}  # filter noisy generic verbs\n",
        "\n",
        "cmi_path = os.path.join(BASE_DIR, \"cmi_candidates.csv\")\n",
        "if os.path.exists(cmi_path):\n",
        "    cmi = pd.read_csv(cmi_path)\n",
        "    meas_auto = {v.lower() for v in cmi.loc[cmi[\"src_noun\"].str.lower().isin(MEAS_NOUNS), \"verb\"]\n",
        "                 if isinstance(v := str(v), str) and v and v.lower() not in STOP_VERBS}\n",
        "    craft_auto = {v.lower() for v in cmi.loc[cmi[\"src_noun\"].str.lower().isin(CRAFT_MECH_NOUNS), \"verb\"]\n",
        "                  if isinstance(v := str(v), str) and v and v.lower() not in STOP_VERBS}\n",
        "    MEAS_LEM |= meas_auto\n",
        "    CRAFT_LEM |= craft_auto\n",
        "    print(\"Added from CMI → MEAS:\", sorted(list(meas_auto))[:25])\n",
        "    print(\"Added from CMI → CRAFT:\", sorted(list(craft_auto))[:25])\n",
        "else:\n",
        "    print(\"No cmi_candidates.csv found; using seed lexicons only.\")\n",
        "\n",
        "# 4) Family classifier (lemma-first, then token fallback)\n",
        "def family_from_row(r):\n",
        "    lem = str(r.get(\"lemma\",\"\")).lower()\n",
        "    tok = str(r.get(\"token\",\"\")).lower()\n",
        "    if lem in MEAS_LEM or any(k in tok for k in MEAS_LEM): return \"MEASUREMENT\"\n",
        "    if lem in CRAFT_LEM or any(k in tok for k in CRAFT_LEM): return \"CRAFT_CONSTRUCTION\"\n",
        "    return \"OTHER\"\n",
        "\n",
        "meta[\"family\"] = meta.apply(family_from_row, axis=1)\n",
        "\n",
        "# 5) Aggregate to sentence-level (avoid multi-token inflation)\n",
        "sent_level = (meta.groupby([\"row_id\",\"family\"]).size().reset_index(name=\"hits\"))\n",
        "\n",
        "# 6) Join with chapters — now safe (unique row_id)\n",
        "base = df[[\"row_id\",\"chapter\"]].copy()\n",
        "joined = base.merge(sent_level, on=\"row_id\", how=\"left\").dropna(subset=[\"family\"])\n",
        "\n",
        "# 7) Pivots (counts and normalized per 100 sentences)\n",
        "pivot_counts = (joined.groupby([\"chapter\",\"family\"])[\"hits\"].sum().unstack(fill_value=0))\n",
        "chapter_sizes = df.groupby(\"chapter\").size()\n",
        "pivot_rate = pivot_counts.div(chapter_sizes, axis=0) * 100.0\n",
        "\n",
        "def heatmap(mat, title, out_name, label):\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.imshow(mat.values, aspect='auto')\n",
        "    plt.xticks(range(mat.shape[1]), mat.columns, rotation=45, ha='right')\n",
        "    plt.yticks(range(mat.shape[0]), mat.index)\n",
        "    plt.title(title)\n",
        "    plt.colorbar(label=label)\n",
        "    plt.tight_layout()\n",
        "    out_png = os.path.join(BASE_DIR, out_name)\n",
        "    plt.savefig(out_png, dpi=160)\n",
        "    print(\"Wrote:\", out_png)\n",
        "\n",
        "heatmap(pivot_counts, \"Metaphor family density by chapter (counts)\", \"metaphor_heatmap_counts.png\", \"count\")\n",
        "heatmap(pivot_rate,   \"Metaphor family density by chapter (normalized)\", \"metaphor_heatmap_normalized.png\", \"per 100 sentences\")\n",
        "\n",
        "print(\"\\nTotals (raw counts):\")\n",
        "print(pivot_counts.sum(0).sort_values(ascending=False))\n",
        "print(\"\\nTotals (normalized):\")\n",
        "print(pivot_rate.sum(0).sort_values(ascending=False))\n"
      ],
      "metadata": {
        "id": "BS_DhtoMQ8ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 5 (fixed v2): unique row_id, filtered CMI verbs, and new heatmaps ---\n",
        "\n",
        "import os, glob, pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
        "\n",
        "BASE_DIR = globals().get(\"BASE_DIR\", \"/content/drive/MyDrive/Chinese Philosophy/Mozi\")\n",
        "\n",
        "# 1) Load metaphor candidates\n",
        "meta_path_labeled = os.path.join(BASE_DIR, \"metaphor_enriched_labeled.csv\")\n",
        "meta_path_flagged = os.path.join(BASE_DIR, \"metaphor_enriched.csv\")\n",
        "meta = pd.read_csv(meta_path_labeled if os.path.exists(meta_path_labeled) else meta_path_flagged)\n",
        "flag_col = \"is_metaphor_final\" if \"is_metaphor_final\" in meta.columns else \"is_metaphor_candidate\"\n",
        "meta = meta[meta[flag_col]==1].copy()\n",
        "\n",
        "# 2) Load Mozi text and ensure a SINGLE, clean row_id aligned with Step 1\n",
        "if \"df\" not in globals():\n",
        "    mozi_csv = sorted(glob.glob(os.path.join(BASE_DIR, \"Mozi*.csv\")), key=os.path.getmtime, reverse=True)[0]\n",
        "    df = pd.read_csv(mozi_csv)\n",
        "\n",
        "df.columns = [c.strip().lower() for c in df.columns]\n",
        "df = df.loc[:, ~pd.Index(df.columns).duplicated()]\n",
        "df = df.reset_index(drop=True)\n",
        "df[\"row_id\"] = np.arange(len(df))\n",
        "\n",
        "if \"chapter\" not in df.columns:\n",
        "    bins = 10\n",
        "    df[\"chapter\"] = pd.cut(df[\"row_id\"], bins=bins, labels=[f\"seg{i:02d}\" for i in range(bins)])\n",
        "\n",
        "# 3) Build/expand lexicons from CMI, filtering generic/high-frequency verbs\n",
        "MEAS_NOUNS = {\"measure\",\"weight\",\"balance\",\"scale\",\"standard\",\"plumb\",\"square\",\"level\",\"rule\",\"gauge\",\"yardstick\"}\n",
        "CRAFT_MECH_NOUNS = {\"lever\",\"pulley\",\"axle\",\"wheel\",\"hinge\",\"gear\",\"tool\",\"chisel\",\"saw\",\"plane\",\"hammer\",\"drill\",\n",
        "                    \"timber\",\"wood\",\"carpenter\",\"craftsman\",\"craft\",\"wedge\",\"rope\",\"plank\",\"beam\",\"joinery\",\"mortise\",\"tenon\"}\n",
        "\n",
        "MEAS_LEM = {\"measure\",\"weigh\",\"balance\",\"scale\",\"standard\",\"gauge\",\"calibrate\",\"level\",\"square\",\"plumb\",\"rule\"}\n",
        "CRAFT_LEM = {\"build\",\"construct\",\"craft\",\"carpenter\",\"join\",\"chisel\",\"plane\",\"saw\",\"hammer\",\"drill\",\n",
        "             \"carve\",\"cut\",\"assemble\",\"erect\",\"repair\",\"polish\",\"shape\",\"file\",\"hew\",\"mortise\",\"tenon\"}\n",
        "\n",
        "STOP_VERBS = {\"be\",\"have\",\"do\",\"get\",\"take\",\"lose\",\"make\"}  # filter noisy generics\n",
        "\n",
        "cmi_path = os.path.join(BASE_DIR, \"cmi_candidates.csv\")\n",
        "if os.path.exists(cmi_path):\n",
        "    cmi = pd.read_csv(cmi_path)\n",
        "\n",
        "    src_noun_lower = cmi[\"src_noun\"].astype(str).str.lower()\n",
        "\n",
        "    meas_auto = set()\n",
        "    for v in cmi.loc[src_noun_lower.isin(MEAS_NOUNS), \"verb\"].dropna():\n",
        "        v_norm = str(v).strip().lower()\n",
        "        if v_norm and v_norm not in STOP_VERBS and v_norm.isalpha():\n",
        "            meas_auto.add(v_norm)\n",
        "\n",
        "    craft_auto = set()\n",
        "    for v in cmi.loc[src_noun_lower.isin(CRAFT_MECH_NOUNS), \"verb\"].dropna():\n",
        "        v_norm = str(v).strip().lower()\n",
        "        if v_norm and v_norm not in STOP_VERBS and v_norm.isalpha():\n",
        "            craft_auto.add(v_norm)\n",
        "\n",
        "    MEAS_LEM |= meas_auto\n",
        "    CRAFT_LEM |= craft_auto\n",
        "    print(\"Added from CMI → MEAS:\", sorted(list(meas_auto))[:25])\n",
        "    print(\"Added from CMI → CRAFT:\", sorted(list(craft_auto))[:25])\n",
        "else:\n",
        "    print(\"No cmi_candidates.csv found; using seed lexicons only.\")\n",
        "\n",
        "# 4) Family classifier (lemma-first, then token fallback)\n",
        "def family_from_row(r):\n",
        "    lem = str(r.get(\"lemma\",\"\")).lower()\n",
        "    tok = str(r.get(\"token\",\"\")).lower()\n",
        "    if lem in MEAS_LEM or any(k in tok for k in MEAS_LEM): return \"MEASUREMENT\"\n",
        "    if lem in CRAFT_LEM or any(k in tok for k in CRAFT_LEM): return \"CRAFT_CONSTRUCTION\"\n",
        "    return \"OTHER\"\n",
        "\n",
        "meta[\"family\"] = meta.apply(family_from_row, axis=1)\n",
        "\n",
        "# 5) Aggregate to sentence-level (avoid multi-token inflation)\n",
        "sent_level = meta.groupby([\"row_id\",\"family\"]).size().reset_index(name=\"hits\")\n",
        "\n",
        "# 6) Join with chapters — safe merge (unique row_id)\n",
        "base = df[[\"row_id\",\"chapter\"]].copy()\n",
        "joined = base.merge(sent_level, on=\"row_id\", how=\"left\").dropna(subset=[\"family\"])\n",
        "\n",
        "# 7) Pivots (counts and normalized per 100 sentences)\n",
        "pivot_counts = joined.groupby([\"chapter\",\"family\"])[\"hits\"].sum().unstack(fill_value=0)\n",
        "chapter_sizes = df.groupby(\"chapter\").size()\n",
        "pivot_rate = pivot_counts.div(chapter_sizes, axis=0) * 100.0\n",
        "\n",
        "# keep a tidy family order if present\n",
        "family_order = [f for f in [\"MEASUREMENT\",\"CRAFT_CONSTRUCTION\",\"OTHER\"] if f in pivot_counts.columns]\n",
        "pivot_counts = pivot_counts.reindex(columns=family_order)\n",
        "pivot_rate   = pivot_rate.reindex(columns=family_order)\n",
        "\n",
        "def heatmap(mat, title, out_name, label):\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.imshow(mat.values, aspect='auto')\n",
        "    plt.xticks(range(mat.shape[1]), mat.columns, rotation=45, ha='right')\n",
        "    plt.yticks(range(mat.shape[0]), mat.index)\n",
        "    plt.title(title)\n",
        "    plt.colorbar(label=label)\n",
        "    plt.tight_layout()\n",
        "    out_png = os.path.join(BASE_DIR, out_name)\n",
        "    plt.savefig(out_png, dpi=160)\n",
        "    print(\"Wrote:\", out_png)\n",
        "\n",
        "heatmap(pivot_counts, \"Metaphor family density by chapter (counts)\", \"metaphor_heatmap_counts.png\", \"count\")\n",
        "heatmap(pivot_rate,   \"Metaphor family density by chapter (normalized)\", \"metaphor_heatmap_normalized.png\", \"per 100 sentences\")\n",
        "\n",
        "print(\"\\nTotals (raw counts):\")\n",
        "print(pivot_counts.sum(0).sort_values(ascending=False))\n",
        "print(\"\\nTotals (normalized):\")\n",
        "print(pivot_rate.sum(0).sort_values(ascending=False))\n"
      ],
      "metadata": {
        "id": "RtarhPXMRZyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 0 — Setup + data loader\n",
        "# Installs (pinned), downloads models, mounts Drive, auto-finds Mozi CSV, normalizes columns, and sets BASE_DIR.\n",
        "!pip -q install \"spacy==3.8.2\" \"pandas==2.2.2\" \"matplotlib==3.9.0\" \"nltk==3.9.1\" \"wordfreq==3.1.1\" \"gradio>=4.41,<5\" \"openai>=1.40.0,<2\"\n",
        "\n",
        "import sys, os, pathlib, re, json, random, math, time, textwrap, itertools, collections\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NLTK setup\n",
        "import nltk\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "# Google Drive mount (defensive)\n",
        "BASE_DIR = pathlib.Path(\"/content/drive/MyDrive/Chinese Philosophy/Mozi\")\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    if not pathlib.Path(\"/content/drive\").exists():\n",
        "        os.makedirs(\"/content/drive\", exist_ok=True)\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Drive mount issue:\", e)\n",
        "        print(\"[HINT] Continuing without Drive; paths will default to /content only.\")\n",
        "        BASE_DIR = pathlib.Path(\"/content/Mozi\")\n",
        "except Exception as e:\n",
        "    print(\"[WARN] Not running in Colab or Drive not available:\", e)\n",
        "    BASE_DIR = pathlib.Path(\"/content/Mozi\")\n",
        "\n",
        "# Ensure BASE_DIR exists\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "(SRC_DIR := BASE_DIR / \"source_corpus\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# spaCy model check / install\n",
        "import spacy\n",
        "from spacy.util import is_package\n",
        "NEEDS_MODEL = False\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except Exception:\n",
        "    NEEDS_MODEL = True\n",
        "    print(\"[INFO] Installing spaCy model en_core_web_sm...\")\n",
        "    import spacy.cli as spacy_cli\n",
        "    spacy_cli.download(\"en_core_web_sm\")\n",
        "    # Will require restart; provided in the next cell.\n",
        "\n",
        "# WordNet\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Auto-discover Mozi CSV (case-insensitive)\n",
        "def find_mozi_csv():\n",
        "    candidates = []\n",
        "    roots = [pathlib.Path(\"/content/drive/MyDrive\"), BASE_DIR]\n",
        "    seen = set()\n",
        "    for root in roots:\n",
        "        if not root.exists():\n",
        "            continue\n",
        "        for p in root.rglob(\"*.csv\"):\n",
        "            name = p.name.lower()\n",
        "            if (\"mozi\" in name and \"corrected\" in name) or (name.startswith(\"mozi\") and \"corrected\" in name):\n",
        "                try:\n",
        "                    key = (p.resolve().as_posix())\n",
        "                    if key not in seen:\n",
        "                        seen.add(key)\n",
        "                        candidates.append(p)\n",
        "                except Exception:\n",
        "                    candidates.append(p)\n",
        "    # Fallback: any CSV under BASE_DIR\n",
        "    if not candidates:\n",
        "        for p in BASE_DIR.rglob(\"*.csv\"):\n",
        "            candidates.append(p)\n",
        "    # Pick the most recently modified\n",
        "    if candidates:\n",
        "        candidates = sorted(candidates, key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "        return candidates[0]\n",
        "    return None\n",
        "\n",
        "mozi_csv = find_mozi_csv()\n",
        "if mozi_csv is None:\n",
        "    print(f\"[WARN] Could not auto-find Mozi CSV. Expecting a file like 'Mozi_corrected.csv'.\")\n",
        "    print(f\"[HINT] Place it under: {BASE_DIR.as_posix()} or anywhere in Drive with 'Mozi' and 'corrected' in the name.\")\n",
        "else:\n",
        "    print(f\"[INFO] Using Mozi CSV: {mozi_csv}\")\n",
        "\n",
        "# Load and normalize\n",
        "def load_normalize_csv(pathlike):\n",
        "    assert pathlike is not None and pathlib.Path(pathlike).exists(), \"Mozi CSV not found.\"\n",
        "    df = pd.read_csv(pathlike)\n",
        "    # Normalize columns\n",
        "    df.columns = [str(c).strip().lower() for c in df.columns]\n",
        "    # Pick a text column\n",
        "    TEXT_CANDIDATES = [\"text\",\"translation\",\"english\",\"content\",\"verse_text\",\"line_text\"]\n",
        "    text_col = None\n",
        "    for c in TEXT_CANDIDATES:\n",
        "        if c in df.columns:\n",
        "            text_col = c\n",
        "            break\n",
        "    if text_col is None:\n",
        "        # Try single non-numeric column\n",
        "        nonnum = [c for c in df.columns if df[c].dtype == object]\n",
        "        assert nonnum, \"No suitable text column found.\"\n",
        "        text_col = nonnum[0]\n",
        "        print(f\"[WARN] Falling back to column '{text_col}' as text.\")\n",
        "    df = df.copy()\n",
        "    df[\"__text__\"] = df[text_col].astype(str)\n",
        "    # Chapter-like column, optional\n",
        "    CHAPTER_CAND = [\"chapter\",\"chap\",\"juan\",\"book\",\"section\",\"mozi_chapter\",\"mozi_book\",\"scroll\"]\n",
        "    chap_col = None\n",
        "    for c in CHAPTER_CAND:\n",
        "        if c in df.columns:\n",
        "            chap_col = c\n",
        "            break\n",
        "    if chap_col is None:\n",
        "        df[\"__chapter__\"] = \"Unknown\"\n",
        "    else:\n",
        "        df[\"__chapter__\"] = df[chap_col].astype(str).replace({np.nan:\"Unknown\"})\n",
        "    # Add row_id\n",
        "    if \"__row_id__\" not in df.columns:\n",
        "        df[\"__row_id__\"] = np.arange(len(df))\n",
        "    # Minimal clean\n",
        "    df[\"__text__\"] = df[\"__text__\"].fillna(\"\").map(lambda s: re.sub(r\"\\s+\", \" \", s).strip())\n",
        "    return df, text_col, chap_col\n",
        "\n",
        "df, TEXT_COL, CHAP_COL = (None, None, None)\n",
        "try:\n",
        "    if mozi_csv:\n",
        "        df, TEXT_COL, CHAP_COL = load_normalize_csv(mozi_csv)\n",
        "        print(f\"[INFO] Loaded {len(df)} rows; text='{TEXT_COL}', chapter='{CHAP_COL or '__chapter__'}'.\")\n",
        "        print(df[[ \"__row_id__\", \"__chapter__\", \"__text__\" ]].head(2))\n",
        "        # Cache normalized copy\n",
        "        df.to_csv(BASE_DIR / \"mozi_normalized_preview.csv\", index=False)\n",
        "        print(f\"[OK] Preview written: { (BASE_DIR / 'mozi_normalized_preview.csv').as_posix() }\")\n",
        "    else:\n",
        "        print(\"[ERR] No CSV loaded.\")\n",
        "except AssertionError as e:\n",
        "    print(\"[ERR]\", e)\n",
        "    print(\"[HINT] Ensure your CSV exists and has a recognizable text column.\")\n",
        "\n",
        "# Save a small env manifest for reproducibility\n",
        "manifest = {\n",
        "    \"python\": sys.version,\n",
        "    \"packages\": {\n",
        "        \"spacy\": spacy.__version__,\n",
        "        \"pandas\": pd.__version__,\n",
        "        \"matplotlib\": plt.matplotlib.__version__,\n",
        "        \"nltk\": nltk.__version__,\n",
        "        \"wordfreq\": __import__(\"wordfreq\").__version__,\n",
        "    },\n",
        "    \"mozi_csv\": str(mozi_csv) if mozi_csv else None,\n",
        "    \"base_dir\": BASE_DIR.as_posix()\n",
        "}\n",
        "with open(BASE_DIR / \"env_manifest.json\",\"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "print(\"[OK] Wrote env_manifest.json\")\n",
        "print(\"[NOTE] If the spaCy model was just installed, run the next cell once to restart the runtime.\")\n"
      ],
      "metadata": {
        "id": "QgREIX9QTtr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 0 — Setup + data loader (patched: safe version detection)\n",
        "# Installs (pinned), downloads models, mounts Drive, auto-finds Mozi CSV, normalizes columns, and sets BASE_DIR.\n",
        "!pip -q install \"spacy==3.8.2\" \"pandas==2.2.2\" \"matplotlib==3.9.0\" \"nltk==3.9.1\" \"wordfreq==3.1.1\" \"gradio>=4.41,<5\" \"openai>=1.40.0,<2\"\n",
        "\n",
        "import sys, os, pathlib, re, json, random, math, time, textwrap, itertools, collections\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NLTK setup\n",
        "import nltk\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "# Google Drive mount (defensive)\n",
        "BASE_DIR = pathlib.Path(\"/content/drive/MyDrive/Chinese Philosophy/Mozi\")\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    if not pathlib.Path(\"/content/drive\").exists():\n",
        "        os.makedirs(\"/content/drive\", exist_ok=True)\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Drive mount issue:\", e)\n",
        "        print(\"[HINT] Continuing without Drive; paths will default to /content only.\")\n",
        "        BASE_DIR = pathlib.Path(\"/content/Mozi\")\n",
        "except Exception as e:\n",
        "    print(\"[WARN] Not running in Colab or Drive not available:\", e)\n",
        "    BASE_DIR = pathlib.Path(\"/content/Mozi\")\n",
        "\n",
        "# Ensure BASE_DIR exists\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "(SRC_DIR := BASE_DIR / \"source_corpus\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# spaCy model check / install\n",
        "import spacy\n",
        "NEEDS_MODEL = False\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except Exception:\n",
        "    NEEDS_MODEL = True\n",
        "    print(\"[INFO] Installing spaCy model en_core_web_sm...\")\n",
        "    import spacy.cli as spacy_cli\n",
        "    spacy_cli.download(\"en_core_web_sm\")\n",
        "    # Will require restart; provided in the next cell.\n",
        "\n",
        "# WordNet\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Auto-discover Mozi CSV (case-insensitive)\n",
        "def find_mozi_csv():\n",
        "    candidates = []\n",
        "    roots = [pathlib.Path(\"/content/drive/MyDrive\"), BASE_DIR]\n",
        "    seen = set()\n",
        "    for root in roots:\n",
        "        if not root.exists():\n",
        "            continue\n",
        "        for p in root.rglob(\"*.csv\"):\n",
        "            name = p.name.lower()\n",
        "            if (\"mozi\" in name and \"corrected\" in name) or (name.startswith(\"mozi\") and \"corrected\" in name):\n",
        "                try:\n",
        "                    key = (p.resolve().as_posix())\n",
        "                    if key not in seen:\n",
        "                        seen.add(key)\n",
        "                        candidates.append(p)\n",
        "                except Exception:\n",
        "                    candidates.append(p)\n",
        "    # Fallback: any CSV under BASE_DIR\n",
        "    if not candidates:\n",
        "        for p in BASE_DIR.rglob(\"*.csv\"):\n",
        "            candidates.append(p)\n",
        "    # Pick the most recently modified\n",
        "    if candidates:\n",
        "        candidates = sorted(candidates, key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "        return candidates[0]\n",
        "    return None\n",
        "\n",
        "mozi_csv = find_mozi_csv()\n",
        "if mozi_csv is None:\n",
        "    print(f\"[WARN] Could not auto-find Mozi CSV. Expecting a file like 'Mozi_corrected.csv'.\")\n",
        "    print(f\"[HINT] Place it under: {BASE_DIR.as_posix()} or anywhere in Drive with 'Mozi' and 'corrected' in the name.\")\n",
        "else:\n",
        "    print(f\"[INFO] Using Mozi CSV: {mozi_csv}\")\n",
        "\n",
        "# Load and normalize\n",
        "def load_normalize_csv(pathlike):\n",
        "    assert pathlike is not None and pathlib.Path(pathlike).exists(), \"Mozi CSV not found.\"\n",
        "    df = pd.read_csv(pathlike)\n",
        "    # Normalize columns\n",
        "    df.columns = [str(c).strip().lower() for c in df.columns]\n",
        "    # Pick a text column\n",
        "    TEXT_CANDIDATES = [\"text\",\"translation\",\"english\",\"content\",\"verse_text\",\"line_text\"]\n",
        "    text_col = next((c for c in TEXT_CANDIDATES if c in df.columns), None)\n",
        "    if text_col is None:\n",
        "        nonnum = [c for c in df.columns if df[c].dtype == object]\n",
        "        assert nonnum, \"No suitable text column found.\"\n",
        "        text_col = nonnum[0]\n",
        "        print(f\"[WARN] Falling back to column '{text_col}' as text.\")\n",
        "    df = df.copy()\n",
        "    df[\"__text__\"] = df[text_col].astype(str)\n",
        "    # Chapter-like column, optional\n",
        "    CHAPTER_CAND = [\"chapter\",\"chap\",\"juan\",\"book\",\"section\",\"mozi_chapter\",\"mozi_book\",\"scroll\"]\n",
        "    chap_col = next((c for c in CHAPTER_CAND if c in df.columns), None)\n",
        "    if chap_col is None:\n",
        "        df[\"__chapter__\"] = \"Unknown\"\n",
        "    else:\n",
        "        df[\"__chapter__\"] = df[chap_col].astype(str).replace({np.nan:\"Unknown\"})\n",
        "    # Add row_id\n",
        "    if \"__row_id__\" not in df.columns:\n",
        "        df[\"__row_id__\"] = np.arange(len(df))\n",
        "    # Minimal clean\n",
        "    df[\"__text__\"] = df[\"__text__\"].fillna(\"\").map(lambda s: re.sub(r\"\\s+\", \" \", s).strip())\n",
        "    return df, text_col, chap_col\n",
        "\n",
        "df, TEXT_COL, CHAP_COL = (None, None, None)\n",
        "try:\n",
        "    if mozi_csv:\n",
        "        df, TEXT_COL, CHAP_COL = load_normalize_csv(mozi_csv)\n",
        "        print(f\"[INFO] Loaded {len(df)} rows; text='{TEXT_COL}', chapter='{CHAP_COL or '__chapter__'}'.\")\n",
        "        print(df[[ \"__row_id__\", \"__chapter__\", \"__text__\" ]].head(2))\n",
        "        # Cache normalized copy\n",
        "        df.to_csv(BASE_DIR / \"mozi_normalized_preview.csv\", index=False)\n",
        "        print(f\"[OK] Preview written: { (BASE_DIR / 'mozi_normalized_preview.csv').as_posix() }\")\n",
        "    else:\n",
        "        print(\"[ERR] No CSV loaded.\")\n",
        "except AssertionError as e:\n",
        "    print(\"[ERR]\", e)\n",
        "    print(\"[HINT] Ensure your CSV exists and has a recognizable text column.\")\n",
        "\n",
        "# ---- Safe package version reporting (fix for wordfreq lacking __version__) ----\n",
        "try:\n",
        "    from importlib.metadata import version as _pkg_version\n",
        "except Exception:\n",
        "    _pkg_version = None\n",
        "\n",
        "def get_version(pkg: str, module_name: str | None = None) -> str:\n",
        "    try:\n",
        "        mod = __import__(module_name or pkg)\n",
        "        v = getattr(mod, \"__version__\", None)\n",
        "        if v:\n",
        "            return str(v)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if _pkg_version is not None:\n",
        "        try:\n",
        "            return _pkg_version(pkg)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return \"unknown\"\n",
        "\n",
        "# Save a small env manifest for reproducibility\n",
        "manifest = {\n",
        "    \"python\": sys.version,\n",
        "    \"packages\": {\n",
        "        \"spacy\": get_version(\"spacy\"),\n",
        "        \"pandas\": get_version(\"pandas\"),\n",
        "        \"matplotlib\": get_version(\"matplotlib\", \"matplotlib\"),\n",
        "        \"nltk\": get_version(\"nltk\"),\n",
        "        \"wordfreq\": get_version(\"wordfreq\"),\n",
        "        \"gradio\": get_version(\"gradio\"),\n",
        "        \"openai\": get_version(\"openai\"),\n",
        "    },\n",
        "    \"mozi_csv\": str(mozi_csv) if mozi_csv else None,\n",
        "    \"base_dir\": BASE_DIR.as_posix()\n",
        "}\n",
        "with open(BASE_DIR / \"env_manifest.json\",\"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "print(\"[OK] Wrote env_manifest.json\")\n",
        "print(\"[NOTE] If the spaCy model was just installed, run the next cell once to restart the runtime.\")\n"
      ],
      "metadata": {
        "id": "yItIFjHlU-hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (Optional) Restart runtime (run once if en_core_web_sm was just installed)\n",
        "import os, sys\n",
        "print(\"[INFO] Restarting runtime now...\")\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "id": "HWyzRAI7VI-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 1 — Heuristic metaphor candidates → CSV\n",
        "# spaCy parse with nlp.pipe; SPV/CCO rules; WordNet-based abstractness; writes:\n",
        "# - metaphor_enriched_full.csv (all tokens)\n",
        "# - metaphor_enriched.csv (flagged tokens only)\n",
        "import os, pathlib, json, re, math, random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "from wordfreq import zipf_frequency\n",
        "\n",
        "assert (BASE_DIR / \"env_manifest.json\").exists(), \"[ERR] Setup not completed. Run Step 0.\"\n",
        "assert (BASE_DIR / \"mozi_normalized_preview.csv\").exists(), \"[ERR] No normalized preview. Re-run Step 0.\"\n",
        "df = pd.read_csv(BASE_DIR / \"mozi_normalized_preview.csv\")\n",
        "\n",
        "# Load spaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"en_core_web_sm missing. Run the restart cell after installing the model.\") from e\n",
        "\n",
        "# Heuristic resources\n",
        "MEASUREMENT_VERBS = {\n",
        "    \"measure\",\"weigh\",\"gauge\",\"calibrate\",\"benchmark\",\"assess\",\"evaluate\",\"estimate\",\"calculate\",\"count\",\"scale\",\"standardize\",\"balance\",\"level\",\"meter\",\"rate\",\"score\"\n",
        "}\n",
        "MEASUREMENT_NOUNS = {\n",
        "    \"measure\",\"measurement\",\"weight\",\"scale\",\"ruler\",\"square\",\"compass\",\"yardstick\",\"standard\",\"gauge\",\"balance\",\"meter\",\"rule\",\"benchmark\",\"calibration\"\n",
        "}\n",
        "CRAFT_VERBS = {\n",
        "    \"build\",\"construct\",\"forge\",\"craft\",\"hammer\",\"chisel\",\"cut\",\"saw\",\"drill\",\"weld\",\"mold\",\"carve\",\"join\",\"fit\",\"assemble\",\"fabricate\",\"repair\",\"mend\",\"nail\",\"bolt\",\"plane\",\"lathe\",\"engineer\",\"design\",\"hewn\",\"hew\"\n",
        "}\n",
        "CRAFT_NOUNS = {\n",
        "    \"tool\",\"machine\",\"gear\",\"axle\",\"wheel\",\"lever\",\"pulley\",\"square\",\"ruler\",\"compass\",\"chisel\",\"hammer\",\"saw\",\"drill\",\"plumb\",\"line\",\"string\",\"inkline\",\"compass\"\n",
        "}\n",
        "\n",
        "ABSTRACT_LEXNAMES = {\n",
        "    # WordNet lexnames that tend to be abstract\n",
        "    \"noun.cognition\",\"noun.attribute\",\"noun.communication\",\"noun.act\",\"noun.event\",\"noun.feeling\",\"noun.motive\",\"noun.state\",\"noun.time\",\"noun.location\",\"noun.group\"\n",
        "}\n",
        "CONCRETE_LEXNAMES = {\n",
        "    \"noun.artifact\",\"noun.object\",\"noun.body\",\"noun.animal\",\"noun.plant\",\"noun.food\",\"noun.substance\",\"noun.shape\",\"noun.quantity\"\n",
        "}\n",
        "\n",
        "INTANGIBLE_OVERRIDES = {\n",
        "    \"virtue\",\"kindness\",\"righteousness\",\"faithfulness\",\"justice\",\"policy\",\"plan\",\"argument\",\"doctrine\",\"speech\",\"language\",\"order\",\"disorder\",\"governance\",\"law\",\"heaven\",\"heaven's\",\"heavens\",\"profit\",\"benefit\",\"harm\",\"intention\"\n",
        "}\n",
        "\n",
        "STOP_VERBS = {\n",
        "    \"be\",\"am\",\"is\",\"are\",\"was\",\"were\",\"been\",\"being\",\n",
        "    \"have\",\"has\",\"had\",\n",
        "    \"do\",\"does\",\"did\",\"done\",\n",
        "    \"get\",\"gets\",\"got\",\"become\",\n",
        "    \"make\",\"makes\",\"made\",\n",
        "    \"take\",\"takes\",\"took\",\"taken\",\n",
        "    \"give\",\"gives\",\"gave\",\"given\",\n",
        "    \"go\",\"goes\",\"went\",\"gone\",\"come\",\"comes\",\"came\",\n",
        "    \"put\",\"puts\",\"set\",\"sets\",\"keep\",\"keeps\",\"kept\",\"let\",\"lets\",\"seem\",\"seems\",\n",
        "    \"see\",\"sees\",\"saw\",\"seen\",\"look\",\"looks\",\n",
        "    \"want\",\"wants\",\"need\",\"needs\",\n",
        "    \"use\",\"uses\",\"try\",\"tries\",\"tried\",\n",
        "    \"find\",\"finds\",\"found\",\"think\",\"thinks\",\"thought\",\n",
        "    \"tell\",\"tells\",\"told\",\"ask\",\"asks\",\"asked\",\n",
        "    \"work\",\"works\",\"feel\",\"feels\",\"felt\",\n",
        "    \"leave\",\"leaves\",\"call\",\"calls\",\"called\",\n",
        "    \"like\",\"likes\",\"follow\",\"follows\",\"followed\",\n",
        "    \"begin\",\"begins\",\"began\",\"begun\",\"happen\",\"happens\",\"include\",\"includes\",\"continue\",\"continues\",\n",
        "    \"lose\",\"loses\",\"lost\",\"join\",\"joins\",\"joined\",\"hold\",\"holds\",\"show\",\"shows\",\"provide\",\"provides\"\n",
        "}\n",
        "\n",
        "def wordnet_lexnames(word, pos=None):\n",
        "    try:\n",
        "        syns = wn.synsets(word, pos=pos)\n",
        "        return {s.lexname() for s in syns}\n",
        "    except Exception:\n",
        "        return set()\n",
        "\n",
        "def is_abstract_noun(noun_lemma: str) -> bool:\n",
        "    noun_lemma = noun_lemma.lower()\n",
        "    if noun_lemma in INTANGIBLE_OVERRIDES:\n",
        "        return True\n",
        "    lex = wordnet_lexnames(noun_lemma, pos=wn.NOUN)\n",
        "    if not lex:\n",
        "        # Frequency fallback: treat rare words as unknown (not abstract by default)\n",
        "        return False\n",
        "    return any(l in ABSTRACT_LEXNAMES for l in lex) and not any(l in CONCRETE_LEXNAMES for l in lex)\n",
        "\n",
        "def family_guess_for(verb_lemma: str, obj_lemma: str) -> str:\n",
        "    if verb_lemma in MEASUREMENT_VERBS or obj_lemma in MEASUREMENT_NOUNS:\n",
        "        return \"MEASUREMENT\"\n",
        "    if verb_lemma in CRAFT_VERBS or obj_lemma in CRAFT_NOUNS:\n",
        "        return \"CRAFT_CONSTRUCTION\"\n",
        "    return \"OTHER\"\n",
        "\n",
        "def zipf_ok(token_text: str, min_zipf=1.5):\n",
        "    # Avoid ultra-rare junk when flagging\n",
        "    try:\n",
        "        return zipf_frequency(token_text, \"en\") >= min_zipf\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "# Parse texts\n",
        "texts = df[\"__text__\"].fillna(\"\").tolist()\n",
        "chapters = df[\"__chapter__\"].fillna(\"Unknown\").tolist()\n",
        "row_ids = df[\"__row_id__\"].tolist()\n",
        "\n",
        "docs = list(nlp.pipe(texts, disable=[\"ner\"]))\n",
        "print(f\"[INFO] Parsed {len(docs)} documents with spaCy.\")\n",
        "\n",
        "full_rows = []\n",
        "flag_rows = []\n",
        "\n",
        "for rid, chap, doc, raw in zip(row_ids, chapters, docs, texts):\n",
        "    for sent_i, sent in enumerate(doc.sents):\n",
        "        # Collect candidates on verbs with subj/obj relationships\n",
        "        for tok in sent:\n",
        "            if tok.pos_ == \"VERB\" and tok.lemma_.lower() not in STOP_VERBS:\n",
        "                reasons = []\n",
        "                fam = \"OTHER\"\n",
        "                # Check subjects/objects\n",
        "                obj_nouns = [c for c in tok.children if c.dep_ in (\"dobj\",\"obj\",\"attr\",\"pobj\") and c.pos_ in (\"NOUN\",\"PROPN\",\"PRON\")]\n",
        "                subj_nouns = [c for c in tok.children if c.dep_ in (\"nsubj\",\"nsubjpass\") and c.pos_ in (\"NOUN\",\"PROPN\",\"PRON\")]\n",
        "\n",
        "                def noun_lemma(n):\n",
        "                    return n.lemma_.lower() if n.lemma_ else n.text.lower()\n",
        "\n",
        "                # Core SPV/CCO heuristics\n",
        "                for n in obj_nouns + subj_nouns:\n",
        "                    v = tok.lemma_.lower()\n",
        "                    nlem = noun_lemma(n)\n",
        "                    fam = family_guess_for(v, nlem)\n",
        "                    # Measurement/Craft verbs applied to abstract nouns\n",
        "                    if (v in MEASUREMENT_VERBS or v in CRAFT_VERBS) and is_abstract_noun(nlem) and zipf_ok(n.text):\n",
        "                        if v in MEASUREMENT_VERBS:\n",
        "                            reasons.append(f\"MEASUREMENT_on_abstract_{n.dep_}:{nlem}\")\n",
        "                        else:\n",
        "                            reasons.append(f\"CRAFT_on_abstract_{n.dep_}:{nlem}\")\n",
        "\n",
        "                    # Artifacts (measurement/craft nouns) modifying abstract heads (e.g., 'square' of 'virtue')\n",
        "                    if nlem in (MEASUREMENT_NOUNS | CRAFT_NOUNS) and any(is_abstract_noun(h.lemma_.lower()) for h in [tok.head] if h is not None):\n",
        "                        reasons.append(f\"ARTIFACT_modifier_to_abstract:{nlem}\")\n",
        "\n",
        "                # Token-level row (full)\n",
        "                row = {\n",
        "                    \"row_id\": rid,\n",
        "                    \"chapter\": chap,\n",
        "                    \"sent_id\": f\"{rid}:{sent_i}\",\n",
        "                    \"sentence\": sent.text.strip(),\n",
        "                    \"focus_token\": tok.text,\n",
        "                    \"focus_lemma\": tok.lemma_.lower(),\n",
        "                    \"focus_pos\": tok.pos_,\n",
        "                    \"focus_dep\": tok.dep_,\n",
        "                    \"is_metaphor_heuristic\": 1 if reasons else 0,\n",
        "                    \"family_guess\": fam if reasons else \"OTHER\",\n",
        "                    \"reasons\": \";\".join(sorted(set(reasons))) if reasons else \"\",\n",
        "                }\n",
        "                full_rows.append(row)\n",
        "                if reasons:\n",
        "                    flag_rows.append(row)\n",
        "            else:\n",
        "                # Record other tokens minimally for the \"full\" output?\n",
        "                # Keep minimal footprint: skip non-verb tokens in full.\n",
        "                pass\n",
        "\n",
        "full_df = pd.DataFrame(full_rows)\n",
        "flag_df = pd.DataFrame(flag_rows)\n",
        "\n",
        "out_full = BASE_DIR / \"metaphor_enriched_full.csv\"\n",
        "out_flag = BASE_DIR / \"metaphor_enriched.csv\"\n",
        "full_df.to_csv(out_full, index=False)\n",
        "flag_df.to_csv(out_flag, index=False)\n",
        "\n",
        "print(f\"[OK] Wrote {out_full.name} (rows: {len(full_df)}) and {out_flag.name} (flagged rows: {len(flag_df)}).\")\n",
        "if len(flag_df)==0:\n",
        "    print(\"[HINT] No flagged items found. You may broaden heuristics or proceed to CMI discovery.\")\n"
      ],
      "metadata": {
        "id": "bB2yZAh9VNC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 1 — Heuristic metaphor candidates → CSV (patched: self-contained, rebuilds BASE_DIR if missing)\n",
        "# spaCy parse with nlp.pipe; SPV/CCO rules; WordNet-based abstractness; writes:\n",
        "# - metaphor_enriched_full.csv (all tokens)\n",
        "# - metaphor_enriched.csv (flagged tokens only)\n",
        "\n",
        "import os, json, re, math, pathlib, collections\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ---------- Recover BASE_DIR (works even after runtime restart) ----------\n",
        "from pathlib import Path\n",
        "\n",
        "def recover_base_dir() -> Path:\n",
        "    # Try env manifest anywhere under MyDrive first\n",
        "    root = Path(\"/content/drive/MyDrive\")\n",
        "    if root.exists():\n",
        "        for p in root.rglob(\"env_manifest.json\"):\n",
        "            try:\n",
        "                meta = json.loads(p.read_text())\n",
        "                bd = meta.get(\"base_dir\")\n",
        "                if bd and Path(bd).exists():\n",
        "                    return Path(bd)\n",
        "            except Exception:\n",
        "                pass\n",
        "    # Fallbacks\n",
        "    c1 = Path(\"/content/drive/MyDrive/Chinese Philosophy/Mozi\")\n",
        "    c2 = Path(\"/content/Mozi\")\n",
        "    if (c1 / \"mozi_normalized_preview.csv\").exists() or c1.exists():\n",
        "        c1.mkdir(parents=True, exist_ok=True)\n",
        "        return c1\n",
        "    c2.mkdir(parents=True, exist_ok=True)\n",
        "    return c2\n",
        "\n",
        "BASE_DIR = recover_base_dir()\n",
        "print(f\"[INFO] BASE_DIR = {BASE_DIR}\")\n",
        "\n",
        "# ---------- Ensure normalized preview exists; rebuild if missing ----------\n",
        "def find_mozi_csv():\n",
        "    # Prefer a 'Mozi*corrected*.csv' anywhere in Drive; else any CSV in BASE_DIR\n",
        "    candidates = []\n",
        "    roots = [Path(\"/content/drive/MyDrive\"), BASE_DIR]\n",
        "    seen = set()\n",
        "    for root in roots:\n",
        "        if not root.exists():\n",
        "            continue\n",
        "        for p in root.rglob(\"*.csv\"):\n",
        "            name = p.name.lower()\n",
        "            if (\"mozi\" in name and \"corrected\" in name) or (name.startswith(\"mozi\") and \"corrected\" in name):\n",
        "                key = str(p.resolve())\n",
        "                if key not in seen:\n",
        "                    seen.add(key)\n",
        "                    candidates.append(p)\n",
        "    if not candidates:\n",
        "        for p in BASE_DIR.rglob(\"*.csv\"):\n",
        "            candidates.append(p)\n",
        "    if candidates:\n",
        "        candidates = sorted(candidates, key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "        return candidates[0]\n",
        "    return None\n",
        "\n",
        "def load_normalize_csv(pathlike: Path):\n",
        "    df = pd.read_csv(pathlike)\n",
        "    df.columns = [str(c).strip().lower() for c in df.columns]\n",
        "    TEXT_CANDIDATES = [\"text\",\"translation\",\"english\",\"content\",\"verse_text\",\"line_text\"]\n",
        "    text_col = next((c for c in TEXT_CANDIDATES if c in df.columns), None)\n",
        "    if text_col is None:\n",
        "        nonnum = [c for c in df.columns if df[c].dtype == object]\n",
        "        assert nonnum, \"No suitable text column found.\"\n",
        "        text_col = nonnum[0]\n",
        "        print(f\"[WARN] Falling back to column '{text_col}' as text.\")\n",
        "    CHAPTER_CAND = [\"chapter\",\"chap\",\"juan\",\"book\",\"section\",\"mozi_chapter\",\"mozi_book\",\"scroll\"]\n",
        "    chap_col = next((c for c in CHAPTER_CAND if c in df.columns), None)\n",
        "\n",
        "    df = df.copy()\n",
        "    df[\"__text__\"] = df[text_col].astype(str)\n",
        "    df[\"__chapter__\"] = (df[chap_col].astype(str) if chap_col else \"Unknown\")\n",
        "    if \"__row_id__\" not in df.columns:\n",
        "        df[\"__row_id__\"] = np.arange(len(df))\n",
        "    df[\"__text__\"] = df[\"__text__\"].fillna(\"\").map(lambda s: re.sub(r\"\\s+\", \" \", s).strip())\n",
        "    return df\n",
        "\n",
        "preview_path = BASE_DIR / \"mozi_normalized_preview.csv\"\n",
        "if not preview_path.exists():\n",
        "    print(\"[INFO] No mozi_normalized_preview.csv found — rebuilding from source CSV...\")\n",
        "    src_csv = find_mozi_csv()\n",
        "    assert src_csv is not None and src_csv.exists(), (\n",
        "        \"[ERR] Could not find a Mozi CSV. Place a file like 'Mozi_corrected.csv' under \"\n",
        "        f\"{BASE_DIR} or anywhere in Drive, then re-run.\"\n",
        "    )\n",
        "    print(f\"[INFO] Using Mozi CSV: {src_csv}\")\n",
        "    df_norm = load_normalize_csv(src_csv)\n",
        "    df_norm.to_csv(preview_path, index=False)\n",
        "    print(f\"[OK] Preview written: {preview_path}\")\n",
        "\n",
        "assert preview_path.exists(), \"[ERR] No normalized preview. Re-run Step 0 if this persists.\"\n",
        "df = pd.read_csv(preview_path)\n",
        "print(f\"[INFO] Loaded normalized preview: {len(df)} rows\")\n",
        "\n",
        "# ---------- Dependencies ----------\n",
        "try:\n",
        "    import spacy\n",
        "except Exception:\n",
        "    raise RuntimeError(\"[ERR] spaCy missing. Please run Step 0 to install pinned packages.\") from None\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "try:\n",
        "    from wordfreq import zipf_frequency\n",
        "except Exception:\n",
        "    # Safe fallback if wordfreq not available (treat as frequent enough)\n",
        "    def zipf_frequency(token_text: str, lang: str) -> float:\n",
        "        return 5.0\n",
        "\n",
        "# Load spaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"[ERR] spaCy model 'en_core_web_sm' missing. Run Step 0 and the optional restart cell.\") from e\n",
        "\n",
        "# ---------- Heuristic resources ----------\n",
        "MEASUREMENT_VERBS = {\n",
        "    \"measure\",\"weigh\",\"gauge\",\"calibrate\",\"benchmark\",\"assess\",\"evaluate\",\"estimate\",\"calculate\",\"count\",\"scale\",\"standardize\",\"balance\",\"level\",\"meter\",\"rate\",\"score\"\n",
        "}\n",
        "MEASUREMENT_NOUNS = {\n",
        "    \"measure\",\"measurement\",\"weight\",\"scale\",\"ruler\",\"square\",\"compass\",\"yardstick\",\"standard\",\"gauge\",\"balance\",\"meter\",\"rule\",\"benchmark\",\"calibration\"\n",
        "}\n",
        "CRAFT_VERBS = {\n",
        "    \"build\",\"construct\",\"forge\",\"craft\",\"hammer\",\"chisel\",\"cut\",\"saw\",\"drill\",\"weld\",\"mold\",\"carve\",\"join\",\"fit\",\"assemble\",\"fabricate\",\"repair\",\"mend\",\"nail\",\"bolt\",\"plane\",\"lathe\",\"engineer\",\"design\",\"hew\",\"hewn\"\n",
        "}\n",
        "CRAFT_NOUNS = {\n",
        "    \"tool\",\"machine\",\"gear\",\"axle\",\"wheel\",\"lever\",\"pulley\",\"square\",\"ruler\",\"compass\",\"chisel\",\"hammer\",\"saw\",\"drill\",\"plumb\",\"line\",\"string\",\"inkline\",\"compass\"\n",
        "}\n",
        "ABSTRACT_LEXNAMES = {\n",
        "    \"noun.cognition\",\"noun.attribute\",\"noun.communication\",\"noun.act\",\"noun.event\",\"noun.feeling\",\"noun.motive\",\"noun.state\",\"noun.time\",\"noun.location\",\"noun.group\"\n",
        "}\n",
        "CONCRETE_LEXNAMES = {\n",
        "    \"noun.artifact\",\"noun.object\",\"noun.body\",\"noun.animal\",\"noun.plant\",\"noun.food\",\"noun.substance\",\"noun.shape\",\"noun.quantity\"\n",
        "}\n",
        "INTANGIBLE_OVERRIDES = {\n",
        "    \"virtue\",\"kindness\",\"righteousness\",\"faithfulness\",\"justice\",\"policy\",\"plan\",\"argument\",\"doctrine\",\"speech\",\"language\",\"order\",\"disorder\",\"governance\",\"law\",\"heaven\",\"heaven's\",\"heavens\",\"profit\",\"benefit\",\"harm\",\"intention\"\n",
        "}\n",
        "STOP_VERBS = {\n",
        "    \"be\",\"am\",\"is\",\"are\",\"was\",\"were\",\"been\",\"being\",\n",
        "    \"have\",\"has\",\"had\",\n",
        "    \"do\",\"does\",\"did\",\"done\",\n",
        "    \"get\",\"gets\",\"got\",\"become\",\n",
        "    \"make\",\"makes\",\"made\",\n",
        "    \"take\",\"takes\",\"took\",\"taken\",\n",
        "    \"give\",\"gives\",\"gave\",\"given\",\n",
        "    \"go\",\"goes\",\"went\",\"gone\",\"come\",\"comes\",\"came\",\n",
        "    \"put\",\"puts\",\"set\",\"sets\",\"keep\",\"keeps\",\"kept\",\"let\",\"lets\",\"seem\",\"seems\",\n",
        "    \"see\",\"sees\",\"saw\",\"seen\",\"look\",\"looks\",\n",
        "    \"want\",\"wants\",\"need\",\"needs\",\n",
        "    \"use\",\"uses\",\"try\",\"tries\",\"tried\",\n",
        "    \"find\",\"finds\",\"found\",\"think\",\"thinks\",\"thought\",\n",
        "    \"tell\",\"tells\",\"told\",\"ask\",\"asks\",\"asked\",\n",
        "    \"work\",\"works\",\"feel\",\"feels\",\"felt\",\n",
        "    \"leave\",\"leaves\",\"call\",\"calls\",\"called\",\n",
        "    \"like\",\"likes\",\"follow\",\"follows\",\"followed\",\n",
        "    \"begin\",\"begins\",\"began\",\"begun\",\"happen\",\"happens\",\"include\",\"includes\",\"continue\",\"continues\",\n",
        "    \"lose\",\"loses\",\"lost\",\"join\",\"joins\",\"joined\",\"hold\",\"holds\",\"show\",\"shows\",\"provide\",\"provides\"\n",
        "}\n",
        "\n",
        "def wordnet_lexnames(word, pos=None):\n",
        "    try:\n",
        "        syns = wn.synsets(word, pos=pos)\n",
        "        return {s.lexname() for s in syns}\n",
        "    except Exception:\n",
        "        return set()\n",
        "\n",
        "def is_abstract_noun(noun_lemma: str) -> bool:\n",
        "    noun_lemma = noun_lemma.lower()\n",
        "    if noun_lemma in INTANGIBLE_OVERRIDES:\n",
        "        return True\n",
        "    lex = wordnet_lexnames(noun_lemma, pos=wn.NOUN)\n",
        "    if not lex:\n",
        "        return False\n",
        "    return any(l in ABSTRACT_LEXNAMES for l in lex) and not any(l in CONCRETE_LEXNAMES for l in lex)\n",
        "\n",
        "def family_guess_for(verb_lemma: str, obj_lemma: str) -> str:\n",
        "    if verb_lemma in MEASUREMENT_VERBS or obj_lemma in MEASUREMENT_NOUNS:\n",
        "        return \"MEASUREMENT\"\n",
        "    if verb_lemma in CRAFT_VERBS or obj_lemma in CRAFT_NOUNS:\n",
        "        return \"CRAFT_CONSTRUCTION\"\n",
        "    return \"OTHER\"\n",
        "\n",
        "def zipf_ok(token_text: str, min_zipf=1.5):\n",
        "    try:\n",
        "        return zipf_frequency(token_text, \"en\") >= min_zipf\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "# ---------- Parse & flag ----------\n",
        "texts = df[\"__text__\"].fillna(\"\").tolist()\n",
        "chapters = df[\"__chapter__\"].fillna(\"Unknown\").tolist()\n",
        "row_ids = df[\"__row_id__\"].tolist()\n",
        "\n",
        "docs = list(nlp.pipe(texts, disable=[\"ner\"]))\n",
        "print(f\"[INFO] Parsed {len(docs)} documents with spaCy.\")\n",
        "\n",
        "full_rows = []\n",
        "flag_rows = []\n",
        "\n",
        "for rid, chap, doc, raw in zip(row_ids, chapters, docs, texts):\n",
        "    for sent_i, sent in enumerate(doc.sents):\n",
        "        for tok in sent:\n",
        "            if tok.pos_ == \"VERB\" and tok.lemma_.lower() not in STOP_VERBS:\n",
        "                reasons = []\n",
        "                fam = \"OTHER\"\n",
        "                obj_nouns = [c for c in tok.children if c.dep_ in (\"dobj\",\"obj\",\"attr\",\"pobj\") and c.pos_ in (\"NOUN\",\"PROPN\",\"PRON\")]\n",
        "                subj_nouns = [c for c in tok.children if c.dep_ in (\"nsubj\",\"nsubjpass\") and c.pos_ in (\"NOUN\",\"PROPN\",\"PRON\")]\n",
        "\n",
        "                def noun_lemma(n):\n",
        "                    return (n.lemma_ or n.text).lower()\n",
        "\n",
        "                for n in obj_nouns + subj_nouns:\n",
        "                    v = tok.lemma_.lower()\n",
        "                    nlem = noun_lemma(n)\n",
        "                    fam = family_guess_for(v, nlem)\n",
        "                    if (v in MEASUREMENT_VERBS or v in CRAFT_VERBS) and is_abstract_noun(nlem) and zipf_ok(n.text):\n",
        "                        if v in MEASUREMENT_VERBS:\n",
        "                            reasons.append(f\"MEASUREMENT_on_abstract_{n.dep_}:{nlem}\")\n",
        "                        else:\n",
        "                            reasons.append(f\"CRAFT_on_abstract_{n.dep_}:{nlem}\")\n",
        "                    if nlem in (MEASUREMENT_NOUNS | CRAFT_NOUNS) and any(is_abstract_noun(h.lemma_.lower()) for h in [tok.head] if h is not None):\n",
        "                        reasons.append(f\"ARTIFACT_modifier_to_abstract:{nlem}\")\n",
        "\n",
        "                row = {\n",
        "                    \"row_id\": rid,\n",
        "                    \"chapter\": chap,\n",
        "                    \"sent_id\": f\"{rid}:{sent_i}\",\n",
        "                    \"sentence\": sent.text.strip(),\n",
        "                    \"focus_token\": tok.text,\n",
        "                    \"focus_lemma\": tok.lemma_.lower(),\n",
        "                    \"focus_pos\": tok.pos_,\n",
        "                    \"focus_dep\": tok.dep_,\n",
        "                    \"is_metaphor_heuristic\": 1 if reasons else 0,\n",
        "                    \"family_guess\": fam if reasons else \"OTHER\",\n",
        "                    \"reasons\": \";\".join(sorted(set(reasons))) if reasons else \"\",\n",
        "                }\n",
        "                full_rows.append(row)\n",
        "                if reasons:\n",
        "                    flag_rows.append(row)\n",
        "\n",
        "full_df = pd.DataFrame(full_rows)\n",
        "flag_df = pd.DataFrame(flag_rows)\n",
        "\n",
        "out_full = BASE_DIR / \"metaphor_enriched_full.csv\"\n",
        "out_flag = BASE_DIR / \"metaphor_enriched.csv\"\n",
        "full_df.to_csv(out_full, index=False)\n",
        "flag_df.to_csv(out_flag, index=False)\n",
        "\n",
        "print(f\"[OK] Wrote {out_full.name} (rows: {len(full_df)}) and {out_flag.name} (flagged rows: {len(flag_df)}).\")\n",
        "if len(flag_df) == 0:\n",
        "    print(\"[HINT] No flagged items found. You can broaden heuristics or proceed to CMI discovery.\")\n"
      ],
      "metadata": {
        "id": "du0jIy_TVnlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 2 — Candidate QA + optional sampling for labeling\n",
        "# Builds quick QA summaries, deduplicates candidates, and (optionally) creates a stratified sample\n",
        "# so you can label a subset. Outputs:\n",
        "#   - metaphor_QA_summary.csv\n",
        "#   - metaphor_candidates_dedup.csv\n",
        "#   - metaphor_label_queue.csv  (stratified sample; size via LABEL_SAMPLE env, default=300)\n",
        "# Also ensures 'idx' exists in metaphor_enriched.csv for stable labeling.\n",
        "\n",
        "import os, pathlib, math, json, random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Recover BASE_DIR defensively\n",
        "from pathlib import Path\n",
        "def recover_base_dir() -> Path:\n",
        "    root = Path(\"/content/drive/MyDrive\")\n",
        "    if root.exists():\n",
        "        for p in root.rglob(\"env_manifest.json\"):\n",
        "            try:\n",
        "                meta = json.loads(p.read_text())\n",
        "                bd = meta.get(\"base_dir\")\n",
        "                if bd and Path(bd).exists():\n",
        "                    return Path(bd)\n",
        "            except Exception:\n",
        "                pass\n",
        "    c1 = Path(\"/content/drive/MyDrive/Chinese Philosophy/Mozi\")\n",
        "    c2 = Path(\"/content/Mozi\")\n",
        "    if (c1 / \"metaphor_enriched.csv\").exists() or c1.exists():\n",
        "        c1.mkdir(parents=True, exist_ok=True)\n",
        "        return c1\n",
        "    c2.mkdir(parents=True, exist_ok=True)\n",
        "    return c2\n",
        "\n",
        "BASE_DIR = recover_base_dir()\n",
        "FLAG_PATH = BASE_DIR / \"metaphor_enriched.csv\"\n",
        "FULL_PATH = BASE_DIR / \"metaphor_enriched_full.csv\"\n",
        "\n",
        "assert FLAG_PATH.exists(), \"[ERR] Run Step 1 first to produce metaphor_enriched.csv\"\n",
        "\n",
        "df = pd.read_csv(FLAG_PATH)\n",
        "if df.empty:\n",
        "    print(\"[WARN] No flagged candidates to summarize. You can still proceed to CMI (Step 4).\")\n",
        "\n",
        "# Ensure stable 'idx' for labeler\n",
        "if \"idx\" not in df.columns:\n",
        "    df[\"idx\"] = np.arange(len(df))\n",
        "    df.to_csv(FLAG_PATH, index=False)\n",
        "    print(\"[OK] Added 'idx' column to metaphor_enriched.csv\")\n",
        "\n",
        "# Deduplicate by (sentence, focus_lemma) to reduce near-duplicates\n",
        "dedup_cols = [\"sentence\",\"focus_lemma\"]\n",
        "dedup_df = (df\n",
        "            .sort_values([\"sentence\",\"focus_lemma\",\"idx\"])\n",
        "            .drop_duplicates(subset=dedup_cols, keep=\"first\")\n",
        "            .reset_index(drop=True))\n",
        "dedup_out = BASE_DIR / \"metaphor_candidates_dedup.csv\"\n",
        "dedup_df.to_csv(dedup_out, index=False)\n",
        "\n",
        "# QA summary: counts by family and by (chapter,family)\n",
        "fam_counts = df.groupby(\"family_guess\")[\"row_id\"].nunique().sort_values(ascending=False)\n",
        "chap_fam = df.groupby([\"chapter\",\"family_guess\"])[\"row_id\"].nunique().unstack(fill_value=0)\n",
        "summary_rows = []\n",
        "for fam, cnt in fam_counts.items():\n",
        "    summary_rows.append({\"level\":\"family\",\"key\":str(fam),\"rows\":int(cnt)})\n",
        "\n",
        "for chap, row in chap_fam.iterrows():\n",
        "    for fam in row.index:\n",
        "        summary_rows.append({\"level\":\"chapter×family\",\"key\":f\"{chap}×{fam}\",\"rows\":int(row[fam])})\n",
        "\n",
        "qa_df = pd.DataFrame(summary_rows).sort_values([\"level\",\"rows\"], ascending=[True, False])\n",
        "qa_out = BASE_DIR / \"metaphor_QA_summary.csv\"\n",
        "qa_df.to_csv(qa_out, index=False)\n",
        "\n",
        "print(f\"[OK] Wrote {qa_out.name} (rows: {len(qa_df)})\")\n",
        "print(f\"[OK] Wrote {dedup_out.name} (rows: {len(dedup_df)})\")\n",
        "\n",
        "# Optional: create a stratified sample for labeling\n",
        "LABEL_SAMPLE = int(os.environ.get(\"LABEL_SAMPLE\", \"300\"))  # set env to change size\n",
        "if len(dedup_df) == 0:\n",
        "    print(\"[HINT] No items to sample from.\")\n",
        "else:\n",
        "    random.seed(42)\n",
        "    # Use 'family_guess' for simple stratification; ensure presence\n",
        "    if \"family_guess\" not in dedup_df.columns:\n",
        "        dedup_df[\"family_guess\"] = \"OTHER\"\n",
        "    fam_sizes = dedup_df[\"family_guess\"].value_counts().to_dict()\n",
        "    total = len(dedup_df)\n",
        "    # Compute target per family proportional to frequency, at least 1 each if possible\n",
        "    targets = {fam: max(1, round(LABEL_SAMPLE * cnt / total)) for fam, cnt in fam_sizes.items()}\n",
        "    # Adjust totals if rounding drift\n",
        "    diff = LABEL_SAMPLE - sum(targets.values())\n",
        "    if diff != 0:\n",
        "        # Distribute remainder to largest families (positive) or subtract from largest (negative)\n",
        "        order = sorted(fam_sizes.items(), key=lambda kv: kv[1], reverse=True)\n",
        "        i = 0\n",
        "        while diff != 0 and order:\n",
        "            fam = order[i % len(order)][0]\n",
        "            if diff > 0:\n",
        "                targets[fam] += 1\n",
        "                diff -= 1\n",
        "            else:\n",
        "                if targets[fam] > 1:\n",
        "                    targets[fam] -= 1\n",
        "                    diff += 1\n",
        "            i += 1\n",
        "\n",
        "    parts = []\n",
        "    for fam, k in targets.items():\n",
        "        sub = dedup_df[dedup_df[\"family_guess\"]==fam]\n",
        "        if len(sub) <= k:\n",
        "            parts.append(sub)\n",
        "        else:\n",
        "            parts.append(sub.sample(n=k, random_state=42))\n",
        "    sample_df = pd.concat(parts, ignore_index=True).sort_values(\"idx\").reset_index(drop=True)\n",
        "\n",
        "    # Ensure columns that labeler expects are present\n",
        "    keep_cols = [\"idx\",\"row_id\",\"sent_id\",\"sentence\",\"focus_token\",\"focus_lemma\",\"chapter\",\"is_metaphor_heuristic\",\"family_guess\",\"reasons\"]\n",
        "    for c in keep_cols:\n",
        "        if c not in sample_df.columns:\n",
        "            sample_df[c] = \"\" if c not in (\"is_metaphor_heuristic\",) else 0\n",
        "\n",
        "    sample_out = BASE_DIR / \"metaphor_label_queue.csv\"\n",
        "    sample_df.to_csv(sample_out, index=False)\n",
        "    print(f\"[OK] Wrote {sample_out.name} (rows: {len(sample_df)}).\")\n",
        "\n",
        "print(\"[NOTE] To label only the sample, you can edit Step 3 to set FLAG_PATH = BASE_DIR/'metaphor_label_queue.csv'. Otherwise Step 3 will load all flagged items.\")\n"
      ],
      "metadata": {
        "id": "HC2iSDIbWGEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 3 — Labeler (Gradio UI) → metaphor_labels.csv\n",
        "# Minimal labeling UI: load flagged rows; label 0–3 + source & target domains; saves on each click.\n",
        "import os, time, json, pathlib\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "\n",
        "assert (BASE_DIR / \"metaphor_enriched.csv\").exists(), \"[ERR] Run Step 1 first.\"\n",
        "FLAG_PATH = BASE_DIR / \"metaphor_enriched.csv\"\n",
        "LABEL_PATH = BASE_DIR / \"metaphor_labels.csv\"\n",
        "\n",
        "data = pd.read_csv(FLAG_PATH)\n",
        "if \"idx\" not in data.columns:\n",
        "    data[\"idx\"] = np.arange(len(data))\n",
        "# Desired label schema\n",
        "LABEL_COLUMNS = [\"row_id\",\"idx\",\"sent_id\",\"sentence\",\"focus_token\",\"focus_lemma\",\"human_label\",\"source_domain\",\"target_domain\",\"timestamp\"]\n",
        "\n",
        "# Initialize label file if missing\n",
        "if not LABEL_PATH.exists():\n",
        "    pd.DataFrame(columns=LABEL_COLUMNS).to_csv(LABEL_PATH, index=False)\n",
        "    print(f\"[OK] Initialized {LABEL_PATH.name}\")\n",
        "\n",
        "def get_item(i):\n",
        "    try:\n",
        "        i = int(i)\n",
        "    except:\n",
        "        i = 0\n",
        "    i = max(0, min(i, len(data)-1))\n",
        "    row = data.iloc[i].to_dict()\n",
        "    return (\n",
        "        i,\n",
        "        f\"RowID: {row['row_id']} | Index: {row['idx']} | Chapter: {row.get('chapter','Unknown')}\",\n",
        "        row[\"sentence\"],\n",
        "        f\"Focus: {row['focus_token']} (lemma={row['focus_lemma']}) | Heuristic={row['is_metaphor_heuristic']} | Family={row['family_guess']} | Reasons={row['reasons']}\",\n",
        "        \"\"\n",
        "    )\n",
        "\n",
        "def save_label(i, human_label, src_dom, tgt_dom):\n",
        "    i = int(i)\n",
        "    i = max(0, min(i, len(data)-1))\n",
        "    row = data.iloc[i]\n",
        "    stamp = pd.Timestamp.utcnow().isoformat()\n",
        "    rec = {\n",
        "        \"row_id\": int(row[\"row_id\"]),\n",
        "        \"idx\": int(row[\"idx\"]),\n",
        "        \"sent_id\": row[\"sent_id\"],\n",
        "        \"sentence\": row[\"sentence\"],\n",
        "        \"focus_token\": row[\"focus_token\"],\n",
        "        \"focus_lemma\": row[\"focus_lemma\"],\n",
        "        \"human_label\": int(human_label),\n",
        "        \"source_domain\": (src_dom or \"\").strip(),\n",
        "        \"target_domain\": (tgt_dom or \"\").strip(),\n",
        "        \"timestamp\": stamp\n",
        "    }\n",
        "    # Append to CSV (defensive header handling)\n",
        "    header_needed = not LABEL_PATH.exists() or os.path.getsize(LABEL_PATH)==0\n",
        "    pd.DataFrame([rec]).to_csv(LABEL_PATH, mode=\"a\", header=header_needed, index=False)\n",
        "    return f\"[SAVED] idx={i} label={human_label} src={src_dom} tgt={tgt_dom} at {stamp}\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"### Metaphor Labeler\")\n",
        "    with gr.Row():\n",
        "        i_in = gr.Number(value=0, label=\"Index (0..N-1)\", precision=0)\n",
        "        load_btn = gr.Button(\"Load\")\n",
        "        status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "    sent = gr.Textbox(label=\"Sentence\", interactive=False, lines=3)\n",
        "    meta = gr.Textbox(label=\"Heuristic Info\", interactive=False, lines=3)\n",
        "    with gr.Row():\n",
        "        human = gr.Slider(0,3,step=1,value=0,label=\"Metaphoricity (0–3)\")\n",
        "        src = gr.Textbox(label=\"Source Domain (e.g., measurement, craft)\")\n",
        "        tgt = gr.Textbox(label=\"Target Domain (e.g., ethics, governance)\")\n",
        "    with gr.Row():\n",
        "        save_btn = gr.Button(\"Save Label\")\n",
        "        next_btn = gr.Button(\"Next ▶\")\n",
        "        prev_btn = gr.Button(\"◀ Prev\")\n",
        "\n",
        "    def _next(i): return min(int(i)+1, len(data)-1)\n",
        "    def _prev(i): return max(int(i)-1, 0)\n",
        "\n",
        "    load_btn.click(get_item, [i_in], [i_in, status, sent, meta, ])\n",
        "    next_btn.click(lambda i: get_item(_next(i)), [i_in], [i_in, status, sent, meta])\n",
        "    prev_btn.click(lambda i: get_item(_prev(i)), [i_in], [i_in, status, sent, meta])\n",
        "    save_btn.click(save_label, [i_in, human, src, tgt], [status])\n",
        "\n",
        "print(f\"[INFO] Loaded {len(data)} flagged items. Labels will be saved to: {LABEL_PATH}\")\n",
        "try:\n",
        "    demo.launch(share=True)\n",
        "except Exception as e:\n",
        "    print(\"[WARN] Gradio launch failed:\", e)\n",
        "    print(\"[HINT] If running headless, skip labeling or run locally.\")\n"
      ],
      "metadata": {
        "id": "Rzy82n-IWU2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 3 — Labeler (Gradio UI) → metaphor_labels.csv\n",
        "# Minimal labeling UI: load flagged rows; label 0–3 + source & target domains; saves on each click.\n",
        "import os, time, json, pathlib\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "\n",
        "assert (BASE_DIR / \"metaphor_enriched.csv\").exists(), \"[ERR] Run Step 1 first.\"\n",
        "FLAG_PATH = BASE_DIR / \"metaphor_enriched.csv\"\n",
        "LABEL_PATH = BASE_DIR / \"metaphor_labels.csv\"\n",
        "\n",
        "data = pd.read_csv(FLAG_PATH)\n",
        "if \"idx\" not in data.columns:\n",
        "    data[\"idx\"] = np.arange(len(data))\n",
        "# Desired label schema\n",
        "LABEL_COLUMNS = [\"row_id\",\"idx\",\"sent_id\",\"sentence\",\"focus_token\",\"focus_lemma\",\"human_label\",\"source_domain\",\"target_domain\",\"timestamp\"]\n",
        "\n",
        "# Initialize label file if missing\n",
        "if not LABEL_PATH.exists():\n",
        "    pd.DataFrame(columns=LABEL_COLUMNS).to_csv(LABEL_PATH, index=False)\n",
        "    print(f\"[OK] Initialized {LABEL_PATH.name}\")\n",
        "\n",
        "def get_item(i):\n",
        "    try:\n",
        "        i = int(i)\n",
        "    except:\n",
        "        i = 0\n",
        "    i = max(0, min(i, len(data)-1))\n",
        "    row = data.iloc[i].to_dict()\n",
        "    return (\n",
        "        i,\n",
        "        f\"RowID: {row['row_id']} | Index: {row['idx']} | Chapter: {row.get('chapter','Unknown')}\",\n",
        "        row[\"sentence\"],\n",
        "        f\"Focus: {row['focus_token']} (lemma={row['focus_lemma']}) | Heuristic={row['is_metaphor_heuristic']} | Family={row['family_guess']} | Reasons={row['reasons']}\",\n",
        "        \"\"\n",
        "    )\n",
        "\n",
        "def save_label(i, human_label, src_dom, tgt_dom):\n",
        "    i = int(i)\n",
        "    i = max(0, min(i, len(data)-1))\n",
        "    row = data.iloc[i]\n",
        "    stamp = pd.Timestamp.utcnow().isoformat()\n",
        "    rec = {\n",
        "        \"row_id\": int(row[\"row_id\"]),\n",
        "        \"idx\": int(row[\"idx\"]),\n",
        "        \"sent_id\": row[\"sent_id\"],\n",
        "        \"sentence\": row[\"sentence\"],\n",
        "        \"focus_token\": row[\"focus_token\"],\n",
        "        \"focus_lemma\": row[\"focus_lemma\"],\n",
        "        \"human_label\": int(human_label),\n",
        "        \"source_domain\": (src_dom or \"\").strip(),\n",
        "        \"target_domain\": (tgt_dom or \"\").strip(),\n",
        "        \"timestamp\": stamp\n",
        "    }\n",
        "    # Append to CSV (defensive header handling)\n",
        "    header_needed = not LABEL_PATH.exists() or os.path.getsize(LABEL_PATH)==0\n",
        "    pd.DataFrame([rec]).to_csv(LABEL_PATH, mode=\"a\", header=header_needed, index=False)\n",
        "    return f\"[SAVED] idx={i} label={human_label} src={src_dom} tgt={tgt_dom} at {stamp}\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"### Metaphor Labeler\")\n",
        "    with gr.Row():\n",
        "        i_in = gr.Number(value=0, label=\"Index (0..N-1)\", precision=0)\n",
        "        load_btn = gr.Button(\"Load\")\n",
        "        status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "    sent = gr.Textbox(label=\"Sentence\", interactive=False, lines=3)\n",
        "    meta = gr.Textbox(label=\"Heuristic Info\", interactive=False, lines=3)\n",
        "    with gr.Row():\n",
        "        human = gr.Slider(0,3,step=1,value=0,label=\"Metaphoricity (0–3)\")\n",
        "        src = gr.Textbox(label=\"Source Domain (e.g., measurement, craft)\")\n",
        "        tgt = gr.Textbox(label=\"Target Domain (e.g., ethics, governance)\")\n",
        "    with gr.Row():\n",
        "        save_btn = gr.Button(\"Save Label\")\n",
        "        next_btn = gr.Button(\"Next ▶\")\n",
        "        prev_btn = gr.Button(\"◀ Prev\")\n",
        "\n",
        "    def _next(i): return min(int(i)+1, len(data)-1)\n",
        "    def _prev(i): return max(int(i)-1, 0)\n",
        "\n",
        "    load_btn.click(get_item, [i_in], [i_in, status, sent, meta, ])\n",
        "    next_btn.click(lambda i: get_item(_next(i)), [i_in], [i_in, status, sent, meta])\n",
        "    prev_btn.click(lambda i: get_item(_prev(i)), [i_in], [i_in, status, sent, meta])\n",
        "    save_btn.click(save_label, [i_in, human, src, tgt], [status])\n",
        "\n",
        "print(f\"[INFO] Loaded {len(data)} flagged items. Labels will be saved to: {LABEL_PATH}\")\n",
        "try:\n",
        "    demo.launch(share=True)\n",
        "except Exception as e:\n",
        "    print(\"[WARN] Gradio launch failed:\", e)\n",
        "    print(\"[HINT] If running headless, skip labeling or run locally.\")\n"
      ],
      "metadata": {
        "id": "r3zPLqEyWYsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 5 — Heatmaps (counts + normalized) → PNGs\n",
        "# chapter × family; also topic × family if a BERTopic assignments CSV is found\n",
        "import os, pathlib, re, json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "assert (BASE_DIR / \"metaphor_enriched.csv\").exists(), \"[ERR] Run Step 1.\"\n",
        "flag_df = pd.read_csv(BASE_DIR / \"metaphor_enriched.csv\")\n",
        "\n",
        "# Try to incorporate human labels and domains\n",
        "labs_path = BASE_DIR / \"metaphor_enriched_labeled.csv\"\n",
        "if labs_path.exists():\n",
        "    use_df = pd.read_csv(labs_path)\n",
        "    print(\"[INFO] Using human-merged labels where available.\")\n",
        "else:\n",
        "    use_df = flag_df.copy()\n",
        "    use_df[\"human_label\"] = 0\n",
        "    use_df[\"source_domain\"] = \"\"\n",
        "    use_df[\"target_domain\"] = \"\"\n",
        "    use_df[\"is_metaphor_final\"] = use_df[\"is_metaphor_heuristic\"]\n",
        "\n",
        "# Load CMI expansions (optional)\n",
        "cmi_path = BASE_DIR / \"cmi_candidates.csv\"\n",
        "cmidf = pd.read_csv(cmi_path) if cmi_path.exists() else pd.DataFrame()\n",
        "\n",
        "MEASUREMENT_SEEDS = {\"measure\",\"weigh\",\"gauge\",\"calibrate\",\"standard\",\"scale\",\"ruler\",\"square\",\"compass\",\"yardstick\",\"benchmark\",\"balance\",\"meter\",\"rule\"}\n",
        "CRAFT_SEEDS = {\"build\",\"construct\",\"forge\",\"craft\",\"hammer\",\"chisel\",\"cut\",\"saw\",\"drill\",\"weld\",\"mold\",\"carve\",\"join\",\"fit\",\"assemble\",\"fabricate\",\"repair\",\"machine\",\"tool\"}\n",
        "\n",
        "def derive_family(row):\n",
        "    # 1) Human label domains\n",
        "    sd = str(row.get(\"source_domain\",\"\")).lower()\n",
        "    td = str(row.get(\"target_domain\",\"\")).lower()\n",
        "    doms = sd + \" \" + td\n",
        "    if any(k in doms for k in [\"measure\",\"scale\",\"standard\",\"ruler\",\"square\",\"compass\",\"gauge\",\"calibr\"]):\n",
        "        return \"MEASUREMENT\"\n",
        "    if any(k in doms for k in [\"craft\",\"construct\",\"build\",\"tool\",\"machine\",\"mechanic\",\"carpenter\",\"artisan\"]):\n",
        "        return \"CRAFT_CONSTRUCTION\"\n",
        "    # 2) Expanded lexicons from CMI\n",
        "    lemma = str(row.get(\"focus_lemma\",\"\")).lower()\n",
        "    if lemma in measurement_lexemes:\n",
        "        return \"MEASUREMENT\"\n",
        "    if lemma in craft_lexemes:\n",
        "        return \"CRAFT_CONSTRUCTION\"\n",
        "    # 3) Fallback to heuristic\n",
        "    return str(row.get(\"family_guess\",\"OTHER\"))\n",
        "\n",
        "# Expanded lexemes from top CMI (simple heuristic buckets)\n",
        "measurement_lexemes = set(MEASUREMENT_SEEDS)\n",
        "craft_lexemes = set(CRAFT_SEEDS)\n",
        "if not cmidf.empty:\n",
        "    # If noun looks like measurement artifact, treat its verb as measurement-aligned\n",
        "    m_artifacts = re.compile(r\"(measure|scale|ruler|square|compass|yardstick|standard|gauge|meter|benchmark|rule)s?$\", re.I)\n",
        "    c_artifacts = re.compile(r\"(tool|machine|gear|axle|wheel|lever|pulley|chisel|hammer|saw|drill|plumb|lathe)s?$\", re.I)\n",
        "    for _, r in cmidf.iterrows():\n",
        "        v = str(r[\"verb\"]).lower()\n",
        "        n = str(r[\"noun\"]).lower()\n",
        "        if re.search(m_artifacts, n):\n",
        "            measurement_lexemes.add(v)\n",
        "        if re.search(c_artifacts, n):\n",
        "            craft_lexemes.add(v)\n",
        "\n",
        "# Prune generic verbs\n",
        "STOP_VERBS = {\n",
        "    \"be\",\"am\",\"is\",\"are\",\"was\",\"were\",\"been\",\"being\",\n",
        "    \"have\",\"has\",\"had\",\"do\",\"does\",\"did\",\"done\",\"get\",\"gets\",\"got\",\"become\",\n",
        "    \"make\",\"makes\",\"made\",\"take\",\"takes\",\"took\",\"taken\",\"give\",\"gives\",\"gave\",\"given\",\n",
        "    \"go\",\"goes\",\"went\",\"gone\",\"come\",\"comes\",\"came\",\"put\",\"puts\",\"set\",\"sets\",\"keep\",\"keeps\",\"kept\",\n",
        "    \"let\",\"lets\",\"seem\",\"seems\",\"see\",\"sees\",\"saw\",\"seen\",\"look\",\"looks\",\"want\",\"wants\",\"need\",\"needs\",\n",
        "    \"use\",\"uses\",\"try\",\"tries\",\"tried\",\"find\",\"finds\",\"found\",\"think\",\"thinks\",\"thought\",\"tell\",\"tells\",\"told\",\n",
        "    \"ask\",\"asks\",\"asked\",\"work\",\"works\",\"feel\",\"feels\",\"felt\",\"leave\",\"leaves\",\"call\",\"calls\",\"called\",\"like\",\"likes\",\n",
        "    \"follow\",\"follows\",\"followed\",\"begin\",\"begins\",\"began\",\"begun\",\"happen\",\"happens\",\"include\",\"includes\",\"continue\",\"continues\",\n",
        "    \"lose\",\"loses\",\"lost\",\"join\",\"joins\",\"joined\",\"hold\",\"holds\",\"show\",\"shows\",\"provide\",\"provides\"\n",
        "}\n",
        "measurement_lexemes = {w for w in measurement_lexemes if w not in STOP_VERBS}\n",
        "craft_lexemes = {w for w in craft_lexemes if w not in STOP_VERBS}\n",
        "\n",
        "use_df[\"family\"] = use_df.apply(derive_family, axis=1)\n",
        "\n",
        "# Build chapter × family counts & normalized per 100 sentences (approx by row_id uniqueness)\n",
        "chap_counts = use_df.groupby([\"chapter\",\"family\"])[\"row_id\"].nunique().unstack(fill_value=0)\n",
        "chap_totals = use_df.groupby(\"chapter\")[\"row_id\"].nunique().reindex(chap_counts.index)\n",
        "chap_norm = (chap_counts.div(chap_totals, axis=0) * 100).fillna(0)\n",
        "\n",
        "# Optional: Topic-based heatmap (auto-detect)\n",
        "topic_counts = None\n",
        "topic_norm = None\n",
        "def find_topic_csv():\n",
        "    # Search anywhere in Drive for BERTopic assignments\n",
        "    root = pathlib.Path(\"/content/drive/MyDrive\")\n",
        "    if not root.exists():\n",
        "        return None\n",
        "    for p in root.rglob(\"*.csv\"):\n",
        "        name = p.name.lower()\n",
        "        if \"bertopic\" in name and (\"assign\" in name or \"topics\" in name):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "topic_csv = find_topic_csv()\n",
        "if topic_csv:\n",
        "    try:\n",
        "        tdf = pd.read_csv(topic_csv)\n",
        "        # Try to join by row_id first, else by exact text\n",
        "        key = None\n",
        "        if \"__row_id__\" in tdf.columns:\n",
        "            key = \"__row_id__\"\n",
        "        elif \"row_id\" in tdf.columns:\n",
        "            key = \"row_id\"\n",
        "        elif \"__text__\" in tdf.columns:\n",
        "            key = \"__text__\"\n",
        "        elif \"text\" in tdf.columns:\n",
        "            key = \"text\"\n",
        "        if key is not None and (\"topic\" in tdf.columns or \"Topic\" in tdf.columns):\n",
        "            tcol = \"topic\" if \"topic\" in tdf.columns else \"Topic\"\n",
        "            left = use_df.copy()\n",
        "            if key in (\"__text__\",\"text\"):\n",
        "                left = left.merge(tdf[[key,tcol]], left_on=\"sentence\", right_on=key, how=\"left\")\n",
        "            else:\n",
        "                left = left.merge(tdf[[key,tcol]], left_on=\"row_id\", right_on=key, how=\"left\")\n",
        "            left[\"topic\"] = left[tcol].fillna(-1).astype(int)\n",
        "            topic_counts = left.groupby([\"topic\",\"family\"])[\"row_id\"].nunique().unstack(fill_value=0)\n",
        "            topic_totals = left.groupby(\"topic\")[\"row_id\"].nunique().reindex(topic_counts.index)\n",
        "            topic_norm = (topic_counts.div(topic_totals, axis=0) * 100).fillna(0)\n",
        "            print(f\"[OK] Joined topic assignments from: {topic_csv.name}\")\n",
        "        else:\n",
        "            print(f\"[WARN] Could not detect join keys in {topic_csv.name}. Skipping topic heatmaps.\")\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Topic heatmap join failed:\", e)\n",
        "\n",
        "# Plot helper\n",
        "def plot_heatmap(df_counts, title, out_path):\n",
        "    if df_counts is None or df_counts.empty:\n",
        "        print(f\"[WARN] Skipping heatmap: {title} (no data).\")\n",
        "        return\n",
        "    plt.figure(figsize=(max(6, df_counts.shape[1]*1.4), max(4, df_counts.shape[0]*0.6)))\n",
        "    im = plt.imshow(df_counts.values, aspect=\"auto\")\n",
        "    plt.colorbar(im, fraction=0.025, pad=0.02)\n",
        "    plt.yticks(range(df_counts.shape[0]), df_counts.index.tolist())\n",
        "    plt.xticks(range(df_counts.shape[1]), df_counts.columns.tolist(), rotation=45, ha=\"right\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"[OK] Saved heatmap: {out_path.name}\")\n",
        "\n",
        "# Save chapter heatmaps\n",
        "plot_heatmap(chap_counts, \"Metaphor counts per chapter × family\", BASE_DIR / \"metaphor_heatmap_counts.png\")\n",
        "plot_heatmap(chap_norm.round(2), \"Metaphor normalized (per 100 rows) per chapter × family\", BASE_DIR / \"metaphor_heatmap_normalized.png\")\n",
        "\n",
        "# Topic heatmaps if available\n",
        "if topic_counts is not None:\n",
        "    plot_heatmap(topic_counts, \"Metaphor counts per topic × family\", BASE_DIR / \"metaphor_heatmap_counts_by_topic.png\")\n",
        "if topic_norm is not None:\n",
        "    plot_heatmap(topic_norm.round(2), \"Metaphor normalized (per 100 rows) per topic × family\", BASE_DIR / \"metaphor_heatmap_normalized_by_topic.png\")\n",
        "\n",
        "print(\"[OK] Heatmaps done.\")\n"
      ],
      "metadata": {
        "id": "v6ai33iDWdl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 5 — Heatmaps (counts + normalized) [patched: robust to missing CMI columns]\n",
        "# Builds chapter × family (and topic × family if BERTopic CSV is found) heatmaps.\n",
        "# Defensive against empty/malformed cmi_candidates.csv (e.g., missing 'noun'/'verb').\n",
        "\n",
        "import os, re, json, pathlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------------- Recover BASE_DIR ----------------\n",
        "def recover_base_dir() -> Path:\n",
        "    root = Path(\"/content/drive/MyDrive\")\n",
        "    if root.exists():\n",
        "        for p in root.rglob(\"env_manifest.json\"):\n",
        "            try:\n",
        "                meta = json.loads(p.read_text())\n",
        "                bd = meta.get(\"base_dir\")\n",
        "                if bd and Path(bd).exists():\n",
        "                    return Path(bd)\n",
        "            except Exception:\n",
        "                pass\n",
        "    c1 = Path(\"/content/drive/MyDrive/Chinese Philosophy/Mozi\")\n",
        "    c2 = Path(\"/content/Mozi\")\n",
        "    if (c1 / \"metaphor_enriched.csv\").exists() or c1.exists():\n",
        "        c1.mkdir(parents=True, exist_ok=True)\n",
        "        return c1\n",
        "    c2.mkdir(parents=True, exist_ok=True)\n",
        "    return c2\n",
        "\n",
        "BASE_DIR = recover_base_dir()\n",
        "print(f\"[INFO] BASE_DIR = {BASE_DIR}\")\n",
        "\n",
        "# ---------------- Load labeled (if present) ----------------\n",
        "flag_path = BASE_DIR / \"metaphor_enriched.csv\"\n",
        "lab_path  = BASE_DIR / \"metaphor_enriched_labeled.csv\"\n",
        "assert flag_path.exists(), \"[ERR] Run Step 1 first.\"\n",
        "\n",
        "flag_df = pd.read_csv(flag_path)\n",
        "\n",
        "if lab_path.exists():\n",
        "    use_df = pd.read_csv(lab_path)\n",
        "    print(\"[INFO] Using human-merged labels where available.\")\n",
        "else:\n",
        "    use_df = flag_df.copy()\n",
        "    if \"human_label\" not in use_df.columns:\n",
        "        use_df[\"human_label\"] = 0\n",
        "    if \"source_domain\" not in use_df.columns:\n",
        "        use_df[\"source_domain\"] = \"\"\n",
        "    if \"target_domain\" not in use_df.columns:\n",
        "        use_df[\"target_domain\"] = \"\"\n",
        "    if \"is_metaphor_final\" not in use_df.columns:\n",
        "        use_df[\"is_metaphor_final\"] = use_df.get(\"is_metaphor_heuristic\", 0)\n",
        "\n",
        "# Ensure required columns\n",
        "for need in [\"row_id\",\"chapter\",\"family_guess\",\"focus_lemma\",\"sentence\"]:\n",
        "    if need not in use_df.columns:\n",
        "        use_df[need] = \"\" if need not in (\"row_id\",) else -1\n",
        "\n",
        "# ---------------- Seed lexicons & optional expansion from CMI ----------------\n",
        "MEASUREMENT_SEEDS = {\n",
        "    \"measure\",\"weigh\",\"gauge\",\"calibrate\",\"benchmark\",\"assess\",\"evaluate\",\"estimate\",\"calculate\",\n",
        "    \"count\",\"scale\",\"standardize\",\"balance\",\"level\",\"meter\",\"rate\",\"score\",\"ruler\",\"square\",\"compass\",\"yardstick\",\"standard\",\"gauge\",\"meter\",\"rule\"\n",
        "}\n",
        "CRAFT_SEEDS = {\n",
        "    \"build\",\"construct\",\"forge\",\"craft\",\"hammer\",\"chisel\",\"cut\",\"saw\",\"drill\",\"weld\",\"mold\",\"carve\",\n",
        "    \"join\",\"fit\",\"assemble\",\"fabricate\",\"repair\",\"mend\",\"nail\",\"bolt\",\"plane\",\"lathe\",\"engineer\",\"design\",\"machine\",\"tool\"\n",
        "}\n",
        "\n",
        "measurement_lexemes = set(MEASUREMENT_SEEDS)\n",
        "craft_lexemes = set(CRAFT_SEEDS)\n",
        "\n",
        "STOP_VERBS = {\n",
        "    \"be\",\"am\",\"is\",\"are\",\"was\",\"were\",\"been\",\"being\",\n",
        "    \"have\",\"has\",\"had\",\"do\",\"does\",\"did\",\"done\",\"get\",\"gets\",\"got\",\"become\",\n",
        "    \"make\",\"makes\",\"made\",\"take\",\"takes\",\"took\",\"taken\",\"give\",\"gives\",\"gave\",\"given\",\n",
        "    \"go\",\"goes\",\"went\",\"gone\",\"come\",\"comes\",\"came\",\"put\",\"puts\",\"set\",\"sets\",\"keep\",\"keeps\",\"kept\",\n",
        "    \"let\",\"lets\",\"seem\",\"seems\",\"see\",\"sees\",\"saw\",\"seen\",\"look\",\"looks\",\"want\",\"wants\",\"need\",\"needs\",\n",
        "    \"use\",\"uses\",\"try\",\"tries\",\"tried\",\"find\",\"finds\",\"found\",\"think\",\"thinks\",\"thought\",\"tell\",\"tells\",\"told\",\n",
        "    \"ask\",\"asks\",\"asked\",\"work\",\"works\",\"feel\",\"feels\",\"felt\",\"leave\",\"leaves\",\"call\",\"calls\",\"called\",\"like\",\"likes\",\n",
        "    \"follow\",\"follows\",\"followed\",\"begin\",\"begins\",\"began\",\"begun\",\"happen\",\"happens\",\"include\",\"includes\",\"continue\",\"continues\",\n",
        "    \"lose\",\"loses\",\"lost\",\"join\",\"joins\",\"joined\",\"hold\",\"holds\",\"show\",\"shows\",\"provide\",\"provides\"\n",
        "}\n",
        "\n",
        "# Try to expand from CMI candidates, but only if expected columns exist\n",
        "cmi_path = BASE_DIR / \"cmi_candidates.csv\"\n",
        "if cmi_path.exists():\n",
        "    cmidf = pd.read_csv(cmi_path)\n",
        "    if not cmidf.empty and {\"verb\",\"noun\"}.issubset(set(cmidf.columns)):\n",
        "        m_artifacts = re.compile(r\"(measure|scale|ruler|square|compass|yardstick|standard|gauge|meter|benchmark|rule)s?$\", re.I)\n",
        "        c_artifacts = re.compile(r\"(tool|machine|gear|axle|wheel|lever|pulley|chisel|hammer|saw|drill|plumb|lathe)s?$\", re.I)\n",
        "        added_m, added_c = 0, 0\n",
        "        for _, r in cmidf.iterrows():\n",
        "            v = str(r[\"verb\"]).lower()\n",
        "            n = str(r[\"noun\"]).lower()\n",
        "            if re.search(m_artifacts, n):\n",
        "                measurement_lexemes.add(v)\n",
        "                added_m += 1\n",
        "            if re.search(c_artifacts, n):\n",
        "                craft_lexemes.add(v)\n",
        "                added_c += 1\n",
        "        print(f\"[INFO] CMI expansion: +{added_m} measurement verbs, +{added_c} craft verbs.\")\n",
        "    else:\n",
        "        print(f\"[WARN] {cmi_path.name} missing expected columns {{'verb','noun'}} or is empty; skipping lexicon expansion.\")\n",
        "else:\n",
        "    print(\"[INFO] No cmi_candidates.csv found; using seed lexicons only.\")\n",
        "\n",
        "# Prune generic verbs\n",
        "measurement_lexemes = {w for w in measurement_lexemes if w not in STOP_VERBS}\n",
        "craft_lexemes = {w for w in craft_lexemes if w not in STOP_VERBS}\n",
        "\n",
        "# ---------------- Family assignment ----------------\n",
        "def derive_family(row):\n",
        "    sd = str(row.get(\"source_domain\",\"\")).lower()\n",
        "    td = str(row.get(\"target_domain\",\"\")).lower()\n",
        "    doms = f\"{sd} {td}\"\n",
        "    if any(k in doms for k in [\"measure\",\"scale\",\"standard\",\"ruler\",\"square\",\"compass\",\"gauge\",\"calibr\"]):\n",
        "        return \"MEASUREMENT\"\n",
        "    if any(k in doms for k in [\"craft\",\"construct\",\"build\",\"tool\",\"machine\",\"mechanic\",\"carpenter\",\"artisan\"]):\n",
        "        return \"CRAFT_CONSTRUCTION\"\n",
        "    lemma = str(row.get(\"focus_lemma\",\"\")).lower()\n",
        "    if lemma in measurement_lexemes:\n",
        "        return \"MEASUREMENT\"\n",
        "    if lemma in craft_lexemes:\n",
        "        return \"CRAFT_CONSTRUCTION\"\n",
        "    return str(row.get(\"family_guess\",\"OTHER\"))\n",
        "\n",
        "use_df[\"family\"] = use_df.apply(derive_family, axis=1)\n",
        "\n",
        "# ---------------- Chapter heatmaps ----------------\n",
        "# Counts per chapter × family (unique row_ids), and normalized per 100 rows in that chapter.\n",
        "# Ensure numeric-ish row_id\n",
        "try:\n",
        "    base_counts = use_df.groupby([\"chapter\",\"family\"])[\"row_id\"].nunique().unstack(fill_value=0)\n",
        "except Exception:\n",
        "    # If row_id is malformed, fallback to counting sentences\n",
        "    base_counts = use_df.groupby([\"chapter\",\"family\"])[\"sentence\"].nunique().unstack(fill_value=0)\n",
        "\n",
        "# Totals per chapter\n",
        "try:\n",
        "    chap_totals = use_df.groupby(\"chapter\")[\"row_id\"].nunique().reindex(base_counts.index)\n",
        "except Exception:\n",
        "    chap_totals = use_df.groupby(\"chapter\")[\"sentence\"].nunique().reindex(base_counts.index)\n",
        "\n",
        "chap_norm = (base_counts.div(chap_totals, axis=0) * 100).fillna(0)\n",
        "\n",
        "# ---------------- Optional: topic heatmaps (BERTopic assignments auto-detect) ----------------\n",
        "def find_topic_csv():\n",
        "    root = Path(\"/content/drive/MyDrive\")\n",
        "    if not root.exists():\n",
        "        return None\n",
        "    for p in root.rglob(\"*.csv\"):\n",
        "        name = p.name.lower()\n",
        "        if \"bertopic\" in name and (\"assign\" in name or \"topics\" in name):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "topic_counts = None\n",
        "topic_norm = None\n",
        "topic_csv = find_topic_csv()\n",
        "if topic_csv:\n",
        "    try:\n",
        "        tdf = pd.read_csv(topic_csv)\n",
        "        key = \"__row_id__\" if \"__row_id__\" in tdf.columns else (\"row_id\" if \"row_id\" in tdf.columns else None)\n",
        "        tcol = \"topic\" if \"topic\" in tdf.columns else (\"Topic\" if \"Topic\" in tdf.columns else None)\n",
        "        if key and tcol:\n",
        "            left = use_df.copy()\n",
        "            left = left.merge(tdf[[key, tcol]], left_on=\"row_id\", right_on=key, how=\"left\")\n",
        "            left[\"topic\"] = left[tcol].fillna(-1).astype(int)\n",
        "            topic_counts = left.groupby([\"topic\",\"family\"])[\"row_id\"].nunique().unstack(fill_value=0)\n",
        "            topic_totals = left.groupby(\"topic\")[\"row_id\"].nunique().reindex(topic_counts.index)\n",
        "            topic_norm = (topic_counts.div(topic_totals, axis=0) * 100).fillna(0)\n",
        "            print(f\"[OK] Joined topic assignments from: {topic_csv.name}\")\n",
        "        else:\n",
        "            print(f\"[WARN] Could not detect join keys in {topic_csv.name}. Skipping topic heatmaps.\")\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Topic heatmap join failed:\", e)\n",
        "\n",
        "# ---------------- Plot helpers ----------------\n",
        "def plot_heatmap(df_counts, title, out_path):\n",
        "    if df_counts is None or df_counts.empty:\n",
        "        print(f\"[WARN] Skipping heatmap: {title} (no data).\")\n",
        "        return\n",
        "    plt.figure(figsize=(max(6, df_counts.shape[1]*1.4), max(4, df_counts.shape[0]*0.6)))\n",
        "    im = plt.imshow(df_counts.values, aspect=\"auto\")\n",
        "    plt.colorbar(im, fraction=0.025, pad=0.02)\n",
        "    plt.yticks(range(df_counts.shape[0]), df_counts.index.tolist())\n",
        "    plt.xticks(range(df_counts.shape[1]), df_counts.columns.tolist(), rotation=45, ha=\"right\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"[OK] Saved heatmap: {out_path.name}\")\n",
        "\n",
        "# ---------------- Save PNGs ----------------\n",
        "plot_heatmap(base_counts, \"Metaphor counts per chapter × family\", BASE_DIR / \"metaphor_heatmap_counts.png\")\n",
        "plot_heatmap(chap_norm.round(2), \"Metaphor normalized (per 100 rows) per chapter × family\", BASE_DIR / \"metaphor_heatmap_normalized.png\")\n",
        "\n",
        "if topic_counts is not None:\n",
        "    plot_heatmap(topic_counts, \"Metaphor counts per topic × family\", BASE_DIR / \"metaphor_heatmap_counts_by_topic.png\")\n",
        "if topic_norm is not None:\n",
        "    plot_heatmap(topic_norm.round(2), \"Metaphor normalized (per 100 rows) per topic × family\", BASE_DIR / \"metaphor_heatmap_normalized_by_topic.png\")\n",
        "\n",
        "# ---------------- Totals printout ----------------\n",
        "print(\"[TOTALS] Chapter × family (head):\")\n",
        "print(base_counts.head())\n",
        "print(\"[OK] Heatmaps done.\")\n"
      ],
      "metadata": {
        "id": "hLzVKA86XPZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 4 — CMI discovery (low-data friendly) → cmi_candidates.csv + cmi_evidence_samples.csv\n",
        "# Looser thresholds, detailed diagnostics, and guaranteed columns to avoid Step 5 warnings.\n",
        "\n",
        "import os, re, json, pathlib, random, math, collections\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# -------- Recover BASE_DIR --------\n",
        "def recover_base_dir() -> Path:\n",
        "    root = Path(\"/content/drive/MyDrive\")\n",
        "    if root.exists():\n",
        "        for p in root.rglob(\"env_manifest.json\"):\n",
        "            try:\n",
        "                meta = json.loads(p.read_text())\n",
        "                bd = meta.get(\"base_dir\")\n",
        "                if bd and Path(bd).exists():\n",
        "                    return Path(bd)\n",
        "            except Exception:\n",
        "                pass\n",
        "    c1 = Path(\"/content/drive/MyDrive/Chinese Philosophy/Mozi\")\n",
        "    c2 = Path(\"/content/Mozi\")\n",
        "    if (c1 / \"mozi_normalized_preview.csv\").exists() or c1.exists():\n",
        "        c1.mkdir(parents=True, exist_ok=True)\n",
        "        return c1\n",
        "    c2.mkdir(parents=True, exist_ok=True)\n",
        "    return c2\n",
        "\n",
        "BASE_DIR = recover_base_dir()\n",
        "print(f\"[INFO] BASE_DIR = {BASE_DIR}\")\n",
        "\n",
        "# -------- Inputs & params --------\n",
        "preview_path = BASE_DIR / \"mozi_normalized_preview.csv\"\n",
        "assert preview_path.exists(), \"[ERR] Run Step 0 first.\"\n",
        "df = pd.read_csv(preview_path)\n",
        "\n",
        "CMI_MIN_PAIR_COUNT = int(os.environ.get(\"CMI_MIN_PAIR_COUNT\", \"1\"))  # was 2; now 1 by default\n",
        "PMI_SMOOTH_K = float(os.environ.get(\"CMI_PMI_SMOOTH_K\", \"1.0\"))\n",
        "\n",
        "# -------- Dependencies --------\n",
        "try:\n",
        "    import spacy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"[ERR] spaCy model 'en_core_web_sm' missing. Run Step 0 & restart.\") from e\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# -------- Source corpus (bootstrap if empty) --------\n",
        "SRC_DIR = BASE_DIR / \"source_corpus\"\n",
        "SRC_DIR.mkdir(exist_ok=True, parents=True)\n",
        "src_txts = list(SRC_DIR.glob(\"*.txt\"))\n",
        "\n",
        "MEASUREMENT_RE = r\"\\b(measure|weigh|scale|ruler|square|compass|yardstick|standard|gauge|calibrate|benchmark|balance|meter|rule|level|calibration|yard-stick|yard stick)\\b\"\n",
        "CRAFT_RE = r\"\\b(build|construct|forge|craft|hammer|chisel|cut|saw|drill|weld|mold|carve|hew|hewn|join|fit|assemble|fabricate|repair|mend|nail|bolt|plane|lathe|engineer|design|machine|tool)\\b\"\n",
        "\n",
        "if not src_txts:\n",
        "    print(\"[INFO] No source_corpus/*.txt found. Bootstrapping from Mozi using measurement/mechanics regex...\")\n",
        "    mozi_texts = df[\"__text__\"].fillna(\"\").tolist()\n",
        "    measure_lines = [t for t in mozi_texts if re.search(MEASUREMENT_RE, t, flags=re.I)]\n",
        "    craft_lines   = [t for t in mozi_texts if re.search(CRAFT_RE, t, flags=re.I)]\n",
        "    (SRC_DIR / \"bootstrap_measurement.txt\").write_text(\"\\n\".join(measure_lines), encoding=\"utf-8\")\n",
        "    (SRC_DIR / \"bootstrap_mechanics.txt\").write_text(\"\\n\".join(craft_lines), encoding=\"utf-8\")\n",
        "    src_txts = list(SRC_DIR.glob(\"*.txt\"))\n",
        "    print(f\"[OK] Bootstrapped: {len(measure_lines)} measurement lines, {len(craft_lines)} mechanics lines.\")\n",
        "\n",
        "# Read source texts\n",
        "source_docs = []\n",
        "total_chars = 0\n",
        "for p in src_txts:\n",
        "    try:\n",
        "        txt = p.read_text(encoding=\"utf-8\")\n",
        "    except Exception:\n",
        "        try:\n",
        "            txt = p.read_text(encoding=\"latin-1\")\n",
        "        except Exception:\n",
        "            print(f\"[WARN] Could not read {p.name}\")\n",
        "            continue\n",
        "    source_docs.append(txt)\n",
        "    total_chars += len(txt)\n",
        "\n",
        "print(f\"[INFO] Source files: {len(source_docs)} | Total chars: {total_chars}\")\n",
        "\n",
        "# -------- Helpers --------\n",
        "STOP_VERBS = {\n",
        "    \"be\",\"am\",\"is\",\"are\",\"was\",\"were\",\"been\",\"being\",\n",
        "    \"have\",\"has\",\"had\",\n",
        "    \"do\",\"does\",\"did\",\"done\",\n",
        "    \"get\",\"gets\",\"got\",\"become\",\n",
        "    \"make\",\"makes\",\"made\",\n",
        "    \"take\",\"takes\",\"took\",\"taken\",\n",
        "    \"give\",\"gives\",\"gave\",\"given\",\n",
        "    \"go\",\"goes\",\"went\",\"gone\",\"come\",\"comes\",\"came\",\n",
        "    \"put\",\"puts\",\"set\",\"sets\",\"keep\",\"keeps\",\"kept\",\"let\",\"lets\",\"seem\",\"seems\",\n",
        "    \"see\",\"sees\",\"saw\",\"seen\",\"look\",\"looks\",\n",
        "    \"want\",\"wants\",\"need\",\"needs\",\n",
        "    \"use\",\"uses\",\"try\",\"tries\",\"tried\",\n",
        "    \"find\",\"finds\",\"found\",\"think\",\"thinks\",\"thought\",\n",
        "    \"tell\",\"tells\",\"told\",\"ask\",\"asks\",\"asked\",\n",
        "    \"work\",\"works\",\"feel\",\"feels\",\"felt\",\n",
        "    \"leave\",\"leaves\",\"call\",\"calls\",\"called\",\n",
        "    \"like\",\"likes\",\"follow\",\"follows\",\"followed\",\n",
        "    \"begin\",\"begins\",\"began\",\"begun\",\"happen\",\"happens\",\"include\",\"includes\",\"continue\",\"continues\",\n",
        "    \"lose\",\"loses\",\"lost\",\"join\",\"joins\",\"joined\",\"hold\",\"holds\",\"show\",\"shows\",\"provide\",\"provides\"\n",
        "}\n",
        "\n",
        "def extract_pairs_from_texts(texts, nlp):\n",
        "    pairs = []\n",
        "    docs = nlp.pipe(texts, disable=[\"ner\"])\n",
        "    for d in docs:\n",
        "        for sent in d.sents:\n",
        "            for v in sent:\n",
        "                if v.pos_ == \"VERB\" and v.lemma_.lower() not in STOP_VERBS:\n",
        "                    for child in v.children:\n",
        "                        if child.dep_ in (\"dobj\",\"obj\",\"pobj\",\"attr\",\"nsubj\",\"nsubjpass\") and child.pos_ in (\"NOUN\",\"PROPN\",\"PRON\"):\n",
        "                            vlem = v.lemma_.lower()\n",
        "                            nlem = (child.lemma_ or child.text).lower()\n",
        "                            pairs.append((vlem, nlem))\n",
        "    return pairs\n",
        "\n",
        "# -------- Extract pairs --------\n",
        "src_pairs = extract_pairs_from_texts(source_docs, nlp)\n",
        "mozi_pairs_per_sent = []  # (row_id, sentence, [(v,n),...])\n",
        "mozi_docs = nlp.pipe(df[\"__text__\"].astype(str).tolist(), disable=[\"ner\"])\n",
        "for rid, d in zip(df[\"__row_id__\"], mozi_docs):\n",
        "    for sent in d.sents:\n",
        "        pairs = []\n",
        "        for v in sent:\n",
        "            if v.pos_ == \"VERB\" and v.lemma_.lower() not in STOP_VERBS:\n",
        "                for child in v.children:\n",
        "                    if child.dep_ in (\"dobj\",\"obj\",\"pobj\",\"attr\",\"nsubj\",\"nsubjpass\") and child.pos_ in (\"NOUN\",\"PROPN\",\"PRON\"):\n",
        "                        pairs.append((v.lemma_.lower(), (child.lemma_ or child.text).lower()))\n",
        "        if pairs:\n",
        "            mozi_pairs_per_sent.append((rid, sent.text.strip(), pairs))\n",
        "\n",
        "print(f\"[INFO] Extracted {len(src_pairs)} verb–noun pairs from source_corpus.\")\n",
        "print(f\"[INFO] Mozi sentences with pairs: {len(mozi_pairs_per_sent)}\")\n",
        "\n",
        "# -------- PMI on source --------\n",
        "cvn = collections.Counter(src_pairs)\n",
        "cv = collections.Counter([v for v,n in src_pairs])\n",
        "cn = collections.Counter([n for v,n in src_pairs])\n",
        "N = max(1, sum(cvn.values()))\n",
        "\n",
        "def pmi(v, n, add_k=PMI_SMOOTH_K):\n",
        "    a = cvn[(v,n)] + add_k\n",
        "    b = cv[v] + add_k\n",
        "    c = cn[n] + add_k\n",
        "    return math.log2((a * N) / (b * c))\n",
        "\n",
        "# -------- Map to synsets (first-sense heuristic) --------\n",
        "def best_synset_verb(lemma):\n",
        "    syns = wn.synsets(lemma, pos=wn.VERB)\n",
        "    return syns[0] if syns else None\n",
        "\n",
        "def best_synset_noun(lemma):\n",
        "    syns = wn.synsets(lemma, pos=wn.NOUN)\n",
        "    return syns[0] if syns else None\n",
        "\n",
        "rows = []\n",
        "for (v,n), cnt in cvn.items():\n",
        "    if cnt < CMI_MIN_PAIR_COUNT:  # tunable threshold\n",
        "        continue\n",
        "    if v in STOP_VERBS:\n",
        "        continue\n",
        "    sv = best_synset_verb(v)\n",
        "    sn = best_synset_noun(n)\n",
        "    rows.append({\n",
        "        \"verb\": v,\n",
        "        \"noun\": n,\n",
        "        \"pair_count\": int(cnt),\n",
        "        \"verb_count\": int(cv[v]),\n",
        "        \"noun_count\": int(cn[n]),\n",
        "        \"PMI\": float(pmi(v,n)),\n",
        "        \"verb_synset\": sv.name() if sv else \"\",\n",
        "        \"verb_lexname\": sv.lexname() if sv else \"\",\n",
        "        \"verb_offset\": sv.offset() if sv else \"\",\n",
        "        \"noun_synset\": sn.name() if sn else \"\",\n",
        "        \"noun_lexname\": sn.lexname() if sn else \"\",\n",
        "        \"noun_offset\": sn.offset() if sn else \"\",\n",
        "    })\n",
        "\n",
        "cand_df = pd.DataFrame(rows, columns=[\n",
        "    \"verb\",\"noun\",\"pair_count\",\"verb_count\",\"noun_count\",\"PMI\",\n",
        "    \"verb_synset\",\"verb_lexname\",\"verb_offset\",\n",
        "    \"noun_synset\",\"noun_lexname\",\"noun_offset\",\n",
        "])\n",
        "\n",
        "# Always write with headers, even if empty\n",
        "cand_out = BASE_DIR / \"cmi_candidates.csv\"\n",
        "cand_df = cand_df.sort_values([\"PMI\",\"pair_count\"], ascending=[False, False]).reset_index(drop=True)\n",
        "cand_df.to_csv(cand_out, index=False)\n",
        "print(f\"[OK] Wrote {cand_out.name} (rows: {len(cand_df)}). Threshold CMI_MIN_PAIR_COUNT={CMI_MIN_PAIR_COUNT}\")\n",
        "\n",
        "# -------- Evidence samples (up to 3 per top pair) --------\n",
        "samples = []\n",
        "if len(cand_df):\n",
        "    top = cand_df.head(200).copy()\n",
        "    # Pre-index Mozi sentences by exact lemma pair\n",
        "    mozi_index = collections.defaultdict(list)\n",
        "    for rid, sent, pairs in mozi_pairs_per_sent:\n",
        "        for (v,n) in pairs:\n",
        "            mozi_index[(v,n)].append((rid, sent))\n",
        "\n",
        "    for _, r in top.iterrows():\n",
        "        key_v = r[\"verb\"]\n",
        "        key_n = r[\"noun\"]\n",
        "        taken = set()\n",
        "        for rid, sent in mozi_index.get((key_v, key_n), [])[:3]:\n",
        "            if (rid, sent) in taken:\n",
        "                continue\n",
        "            samples.append({\"verb_key\": r.get(\"verb_synset\") or key_v, \"noun\": key_n, \"row_id\": rid, \"sentence\": sent})\n",
        "            taken.add((rid, sent))\n",
        "\n",
        "evid_df = pd.DataFrame(samples, columns=[\"verb_key\",\"noun\",\"row_id\",\"sentence\"])\n",
        "evid_out = BASE_DIR / \"cmi_evidence_samples.csv\"\n",
        "evid_df.to_csv(evid_out, index=False)\n",
        "print(f\"[OK] Wrote {evid_out.name} (rows: {len(evid_df)}).\")\n",
        "\n",
        "# -------- Diagnostics & hints --------\n",
        "print(\"\\n[DIAG]\")\n",
        "print(f\"Unique (verb,noun) pairs in source: {len(cvn)} | total tokens N={N}\")\n",
        "print(f\"Kept candidates: {len(cand_df)} (min_pair_count={CMI_MIN_PAIR_COUNT})\")\n",
        "if len(cand_df) == 0:\n",
        "    print(\"[HINT] To get candidates:\")\n",
        "    print(\"  • Add more seed texts into source_corpus/*.txt (technical manuals, measurement/mechanics glossaries).\")\n",
        "    print(\"  • Or lower threshold: set env CMI_MIN_PAIR_COUNT=1 (default) and re-run.\")\n",
        "    print(\"  • Ensure bootstrap regex actually matches your Mozi wording; adjust MEASUREMENT_RE/CRAFT_RE if needed.\")\n"
      ],
      "metadata": {
        "id": "6HWCnLYeZy65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 5 — Heatmaps (counts + normalized) [patched: robust to missing CMI columns]\n",
        "# Builds chapter × family (and topic × family if BERTopic CSV is found) heatmaps.\n",
        "# Defensive against empty/malformed cmi_candidates.csv (e.g., missing 'noun'/'verb').\n",
        "\n",
        "import os, re, json, pathlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------------- Recover BASE_DIR ----------------\n",
        "def recover_base_dir() -> Path:\n",
        "    root = Path(\"/content/drive/MyDrive\")\n",
        "    if root.exists():\n",
        "        for p in root.rglob(\"env_manifest.json\"):\n",
        "            try:\n",
        "                meta = json.loads(p.read_text())\n",
        "                bd = meta.get(\"base_dir\")\n",
        "                if bd and Path(bd).exists():\n",
        "                    return Path(bd)\n",
        "            except Exception:\n",
        "                pass\n",
        "    c1 = Path(\"/content/drive/MyDrive/Chinese Philosophy/Mozi\")\n",
        "    c2 = Path(\"/content/Mozi\")\n",
        "    if (c1 / \"metaphor_enriched.csv\").exists() or c1.exists():\n",
        "        c1.mkdir(parents=True, exist_ok=True)\n",
        "        return c1\n",
        "    c2.mkdir(parents=True, exist_ok=True)\n",
        "    return c2\n",
        "\n",
        "BASE_DIR = recover_base_dir()\n",
        "print(f\"[INFO] BASE_DIR = {BASE_DIR}\")\n",
        "\n",
        "# ---------------- Load labeled (if present) ----------------\n",
        "flag_path = BASE_DIR / \"metaphor_enriched.csv\"\n",
        "lab_path  = BASE_DIR / \"metaphor_enriched_labeled.csv\"\n",
        "assert flag_path.exists(), \"[ERR] Run Step 1 first.\"\n",
        "\n",
        "flag_df = pd.read_csv(flag_path)\n",
        "\n",
        "if lab_path.exists():\n",
        "    use_df = pd.read_csv(lab_path)\n",
        "    print(\"[INFO] Using human-merged labels where available.\")\n",
        "else:\n",
        "    use_df = flag_df.copy()\n",
        "    if \"human_label\" not in use_df.columns:\n",
        "        use_df[\"human_label\"] = 0\n",
        "    if \"source_domain\" not in use_df.columns:\n",
        "        use_df[\"source_domain\"] = \"\"\n",
        "    if \"target_domain\" not in use_df.columns:\n",
        "        use_df[\"target_domain\"] = \"\"\n",
        "    if \"is_metaphor_final\" not in use_df.columns:\n",
        "        use_df[\"is_metaphor_final\"] = use_df.get(\"is_metaphor_heuristic\", 0)\n",
        "\n",
        "# Ensure required columns\n",
        "for need in [\"row_id\",\"chapter\",\"family_guess\",\"focus_lemma\",\"sentence\"]:\n",
        "    if need not in use_df.columns:\n",
        "        use_df[need] = \"\" if need not in (\"row_id\",) else -1\n",
        "\n",
        "# ---------------- Seed lexicons & optional expansion from CMI ----------------\n",
        "MEASUREMENT_SEEDS = {\n",
        "    \"measure\",\"weigh\",\"gauge\",\"calibrate\",\"benchmark\",\"assess\",\"evaluate\",\"estimate\",\"calculate\",\n",
        "    \"count\",\"scale\",\"standardize\",\"balance\",\"level\",\"meter\",\"rate\",\"score\",\"ruler\",\"square\",\"compass\",\"yardstick\",\"standard\",\"gauge\",\"meter\",\"rule\"\n",
        "}\n",
        "CRAFT_SEEDS = {\n",
        "    \"build\",\"construct\",\"forge\",\"craft\",\"hammer\",\"chisel\",\"cut\",\"saw\",\"drill\",\"weld\",\"mold\",\"carve\",\n",
        "    \"join\",\"fit\",\"assemble\",\"fabricate\",\"repair\",\"mend\",\"nail\",\"bolt\",\"plane\",\"lathe\",\"engineer\",\"design\",\"machine\",\"tool\"\n",
        "}\n",
        "\n",
        "measurement_lexemes = set(MEASUREMENT_SEEDS)\n",
        "craft_lexemes = set(CRAFT_SEEDS)\n",
        "\n",
        "STOP_VERBS = {\n",
        "    \"be\",\"am\",\"is\",\"are\",\"was\",\"were\",\"been\",\"being\",\n",
        "    \"have\",\"has\",\"had\",\"do\",\"does\",\"did\",\"done\",\"get\",\"gets\",\"got\",\"become\",\n",
        "    \"make\",\"makes\",\"made\",\"take\",\"takes\",\"took\",\"taken\",\"give\",\"gives\",\"gave\",\"given\",\n",
        "    \"go\",\"goes\",\"went\",\"gone\",\"come\",\"comes\",\"came\",\"put\",\"puts\",\"set\",\"sets\",\"keep\",\"keeps\",\"kept\",\n",
        "    \"let\",\"lets\",\"seem\",\"seems\",\"see\",\"sees\",\"saw\",\"seen\",\"look\",\"looks\",\"want\",\"wants\",\"need\",\"needs\",\n",
        "    \"use\",\"uses\",\"try\",\"tries\",\"tried\",\"find\",\"finds\",\"found\",\"think\",\"thinks\",\"thought\",\"tell\",\"tells\",\"told\",\n",
        "    \"ask\",\"asks\",\"asked\",\"work\",\"works\",\"feel\",\"feels\",\"felt\",\"leave\",\"leaves\",\"call\",\"calls\",\"called\",\"like\",\"likes\",\n",
        "    \"follow\",\"follows\",\"followed\",\"begin\",\"begins\",\"began\",\"begun\",\"happen\",\"happens\",\"include\",\"includes\",\"continue\",\"continues\",\n",
        "    \"lose\",\"loses\",\"lost\",\"join\",\"joins\",\"joined\",\"hold\",\"holds\",\"show\",\"shows\",\"provide\",\"provides\"\n",
        "}\n",
        "\n",
        "# Try to expand from CMI candidates, but only if expected columns exist\n",
        "cmi_path = BASE_DIR / \"cmi_candidates.csv\"\n",
        "if cmi_path.exists():\n",
        "    cmidf = pd.read_csv(cmi_path)\n",
        "    if not cmidf.empty and {\"verb\",\"noun\"}.issubset(set(cmidf.columns)):\n",
        "        m_artifacts = re.compile(r\"(measure|scale|ruler|square|compass|yardstick|standard|gauge|meter|benchmark|rule)s?$\", re.I)\n",
        "        c_artifacts = re.compile(r\"(tool|machine|gear|axle|wheel|lever|pulley|chisel|hammer|saw|drill|plumb|lathe)s?$\", re.I)\n",
        "        added_m, added_c = 0, 0\n",
        "        for _, r in cmidf.iterrows():\n",
        "            v = str(r[\"verb\"]).lower()\n",
        "            n = str(r[\"noun\"]).lower()\n",
        "            if re.search(m_artifacts, n):\n",
        "                measurement_lexemes.add(v)\n",
        "                added_m += 1\n",
        "            if re.search(c_artifacts, n):\n",
        "                craft_lexemes.add(v)\n",
        "                added_c += 1\n",
        "        print(f\"[INFO] CMI expansion: +{added_m} measurement verbs, +{added_c} craft verbs.\")\n",
        "    else:\n",
        "        print(f\"[WARN] {cmi_path.name} missing expected columns {{'verb','noun'}} or is empty; skipping lexicon expansion.\")\n",
        "else:\n",
        "    print(\"[INFO] No cmi_candidates.csv found; using seed lexicons only.\")\n",
        "\n",
        "# Prune generic verbs\n",
        "measurement_lexemes = {w for w in measurement_lexemes if w not in STOP_VERBS}\n",
        "craft_lexemes = {w for w in craft_lexemes if w not in STOP_VERBS}\n",
        "\n",
        "# ---------------- Family assignment ----------------\n",
        "def derive_family(row):\n",
        "    sd = str(row.get(\"source_domain\",\"\")).lower()\n",
        "    td = str(row.get(\"target_domain\",\"\")).lower()\n",
        "    doms = f\"{sd} {td}\"\n",
        "    if any(k in doms for k in [\"measure\",\"scale\",\"standard\",\"ruler\",\"square\",\"compass\",\"gauge\",\"calibr\"]):\n",
        "        return \"MEASUREMENT\"\n",
        "    if any(k in doms for k in [\"craft\",\"construct\",\"build\",\"tool\",\"machine\",\"mechanic\",\"carpenter\",\"artisan\"]):\n",
        "        return \"CRAFT_CONSTRUCTION\"\n",
        "    lemma = str(row.get(\"focus_lemma\",\"\")).lower()\n",
        "    if lemma in measurement_lexemes:\n",
        "        return \"MEASUREMENT\"\n",
        "    if lemma in craft_lexemes:\n",
        "        return \"CRAFT_CONSTRUCTION\"\n",
        "    return str(row.get(\"family_guess\",\"OTHER\"))\n",
        "\n",
        "use_df[\"family\"] = use_df.apply(derive_family, axis=1)\n",
        "\n",
        "# ---------------- Chapter heatmaps ----------------\n",
        "# Counts per chapter × family (unique row_ids), and normalized per 100 rows in that chapter.\n",
        "# Ensure numeric-ish row_id\n",
        "try:\n",
        "    base_counts = use_df.groupby([\"chapter\",\"family\"])[\"row_id\"].nunique().unstack(fill_value=0)\n",
        "except Exception:\n",
        "    # If row_id is malformed, fallback to counting sentences\n",
        "    base_counts = use_df.groupby([\"chapter\",\"family\"])[\"sentence\"].nunique().unstack(fill_value=0)\n",
        "\n",
        "# Totals per chapter\n",
        "try:\n",
        "    chap_totals = use_df.groupby(\"chapter\")[\"row_id\"].nunique().reindex(base_counts.index)\n",
        "except Exception:\n",
        "    chap_totals = use_df.groupby(\"chapter\")[\"sentence\"].nunique().reindex(base_counts.index)\n",
        "\n",
        "chap_norm = (base_counts.div(chap_totals, axis=0) * 100).fillna(0)\n",
        "\n",
        "# ---------------- Optional: topic heatmaps (BERTopic assignments auto-detect) ----------------\n",
        "def find_topic_csv():\n",
        "    root = Path(\"/content/drive/MyDrive\")\n",
        "    if not root.exists():\n",
        "        return None\n",
        "    for p in root.rglob(\"*.csv\"):\n",
        "        name = p.name.lower()\n",
        "        if \"bertopic\" in name and (\"assign\" in name or \"topics\" in name):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "topic_counts = None\n",
        "topic_norm = None\n",
        "topic_csv = find_topic_csv()\n",
        "if topic_csv:\n",
        "    try:\n",
        "        tdf = pd.read_csv(topic_csv)\n",
        "        key = \"__row_id__\" if \"__row_id__\" in tdf.columns else (\"row_id\" if \"row_id\" in tdf.columns else None)\n",
        "        tcol = \"topic\" if \"topic\" in tdf.columns else (\"Topic\" if \"Topic\" in tdf.columns else None)\n",
        "        if key and tcol:\n",
        "            left = use_df.copy()\n",
        "            left = left.merge(tdf[[key, tcol]], left_on=\"row_id\", right_on=key, how=\"left\")\n",
        "            left[\"topic\"] = left[tcol].fillna(-1).astype(int)\n",
        "            topic_counts = left.groupby([\"topic\",\"family\"])[\"row_id\"].nunique().unstack(fill_value=0)\n",
        "            topic_totals = left.groupby(\"topic\")[\"row_id\"].nunique().reindex(topic_counts.index)\n",
        "            topic_norm = (topic_counts.div(topic_totals, axis=0) * 100).fillna(0)\n",
        "            print(f\"[OK] Joined topic assignments from: {topic_csv.name}\")\n",
        "        else:\n",
        "            print(f\"[WARN] Could not detect join keys in {topic_csv.name}. Skipping topic heatmaps.\")\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Topic heatmap join failed:\", e)\n",
        "\n",
        "# ---------------- Plot helpers ----------------\n",
        "def plot_heatmap(df_counts, title, out_path):\n",
        "    if df_counts is None or df_counts.empty:\n",
        "        print(f\"[WARN] Skipping heatmap: {title} (no data).\")\n",
        "        return\n",
        "    plt.figure(figsize=(max(6, df_counts.shape[1]*1.4), max(4, df_counts.shape[0]*0.6)))\n",
        "    im = plt.imshow(df_counts.values, aspect=\"auto\")\n",
        "    plt.colorbar(im, fraction=0.025, pad=0.02)\n",
        "    plt.yticks(range(df_counts.shape[0]), df_counts.index.tolist())\n",
        "    plt.xticks(range(df_counts.shape[1]), df_counts.columns.tolist(), rotation=45, ha=\"right\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"[OK] Saved heatmap: {out_path.name}\")\n",
        "\n",
        "# ---------------- Save PNGs ----------------\n",
        "plot_heatmap(base_counts, \"Metaphor counts per chapter × family\", BASE_DIR / \"metaphor_heatmap_counts.png\")\n",
        "plot_heatmap(chap_norm.round(2), \"Metaphor normalized (per 100 rows) per chapter × family\", BASE_DIR / \"metaphor_heatmap_normalized.png\")\n",
        "\n",
        "if topic_counts is not None:\n",
        "    plot_heatmap(topic_counts, \"Metaphor counts per topic × family\", BASE_DIR / \"metaphor_heatmap_counts_by_topic.png\")\n",
        "if topic_norm is not None:\n",
        "    plot_heatmap(topic_norm.round(2), \"Metaphor normalized (per 100 rows) per topic × family\", BASE_DIR / \"metaphor_heatmap_normalized_by_topic.png\")\n",
        "\n",
        "# ---------------- Totals printout ----------------\n",
        "print(\"[TOTALS] Chapter × family (head):\")\n",
        "print(base_counts.head())\n",
        "print(\"[OK] Heatmaps done.\")\n"
      ],
      "metadata": {
        "id": "OFJnZhaqZ9gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 6 — Paraphrases (LLM) → paraphrases.csv\n",
        "# Uses OPENAI_API_KEY from env; generates literal paraphrases for flagged sentences (batch param); falls back if key missing.\n",
        "import os, time, json, pathlib, math, itertools\n",
        "import pandas as pd\n",
        "from collections import OrderedDict\n",
        "\n",
        "assert (BASE_DIR / \"metaphor_enriched.csv\").exists(), \"[ERR] Run Step 1 first.\"\n",
        "flag_df = pd.read_csv(BASE_DIR / \"metaphor_enriched.csv\")\n",
        "\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\").strip()\n",
        "MODEL_NAME = os.environ.get(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
        "BATCH_SIZE = int(os.environ.get(\"PARAPHRASE_BATCH\", \"200\"))  # configurable via env\n",
        "LENS = os.environ.get(\"COGNITIVE_LENS\", \"ETHICS IS MEASUREMENT; STATE IS A MACHINE; ARGUMENT IS CRAFT/CONSTRUCTION\")\n",
        "\n",
        "# Unique flagged sentences (preserve order)\n",
        "uniq_sentences = []\n",
        "seen = set()\n",
        "for s in flag_df[\"sentence\"].astype(str):\n",
        "    if s not in seen:\n",
        "        seen.add(s)\n",
        "        uniq_sentences.append(s)\n",
        "subset = uniq_sentences[:BATCH_SIZE]\n",
        "\n",
        "out_path = BASE_DIR / \"paraphrases.csv\"\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    # Fallback: write placeholders with hints\n",
        "    pd.DataFrame({\n",
        "        \"sentence\": subset,\n",
        "        \"paraphrase\": [\"(OPENAI_API_KEY missing — add key to env and re-run)\"] * len(subset)\n",
        "    }).to_csv(out_path, index=False)\n",
        "    print(f\"[WARN] OPENAI_API_KEY not set. Wrote placeholders to {out_path.name}.\")\n",
        "else:\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    rows = []\n",
        "    for s in subset:\n",
        "        prompt = (\n",
        "            \"You rewrite sentences into literal, non-metaphorical paraphrases while preserving the intended referent.\\n\"\n",
        "            f\"Cognitive lens: {LENS}\\n\"\n",
        "            \"Rules: Keep a single sentence. Avoid figurative language. Keep proper nouns. No commentary.\\n\"\n",
        "            f\"Sentence: {s}\\n\"\n",
        "            \"Paraphrase:\"\n",
        "        )\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {\"role\":\"system\",\"content\":\"You are a careful, literal paraphraser for philosophical text.\"},\n",
        "                    {\"role\":\"user\",\"content\": prompt}\n",
        "                ],\n",
        "                temperature=0.3,\n",
        "                max_tokens=120\n",
        "                )\n",
        "            text = resp.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            text = f\"(error: {e})\"\n",
        "        rows.append({\"sentence\": s, \"paraphrase\": text})\n",
        "        time.sleep(0.5)  # gentle rate limit\n",
        "\n",
        "    pd.DataFrame(rows).to_csv(out_path, index=False)\n",
        "    print(f\"[OK] Wrote {out_path.name} (rows: {len(rows)}).\")\n"
      ],
      "metadata": {
        "id": "_t3NjXEdaDyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 7 — Summary printout\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "\n",
        "def safe_read(path):\n",
        "    return pd.read_csv(path) if pathlib.Path(path).exists() else None\n",
        "\n",
        "paths = {\n",
        "    \"metaphor_enriched_full.csv\": BASE_DIR / \"metaphor_enriched_full.csv\",\n",
        "    \"metaphor_enriched.csv\": BASE_DIR / \"metaphor_enriched.csv\",\n",
        "    \"metaphor_labels.csv\": BASE_DIR / \"metaphor_labels.csv\",\n",
        "    \"metaphor_enriched_labeled.csv\": BASE_DIR / \"metaphor_enriched_labeled.csv\",\n",
        "    \"cmi_candidates.csv\": BASE_DIR / \"cmi_candidates.csv\",\n",
        "    \"cmi_evidence_samples.csv\": BASE_DIR / \"cmi_evidence_samples.csv\",\n",
        "    \"metaphor_heatmap_counts.png\": BASE_DIR / \"metaphor_heatmap_counts.png\",\n",
        "    \"metaphor_heatmap_normalized.png\": BASE_DIR / \"metaphor_heatmap_normalized.png\",\n",
        "    \"paraphrases.csv\": BASE_DIR / \"paraphrases.csv\",\n",
        "}\n",
        "\n",
        "for name, p in paths.items():\n",
        "    print(f\"{name}: {'OK' if p.exists() else 'MISSING'} — {p.as_posix()}\")\n",
        "\n",
        "flag_df = safe_read(paths[\"metaphor_enriched.csv\"])\n",
        "lab_df = safe_read(paths[\"metaphor_enriched_labeled.csv\"])\n",
        "cmi_df = safe_read(paths[\"cmi_candidates.csv\"])\n",
        "\n",
        "if flag_df is not None:\n",
        "    print(f\"\\n[COUNT] Flagged tokens: {len(flag_df)} across {flag_df['row_id'].nunique()} rows.\")\n",
        "if lab_df is not None:\n",
        "    print(f\"[COUNT] Labeled items: {(lab_df['human_label']>0).sum()} (out of {len(lab_df)}).\")\n",
        "if cmi_df is not None and len(cmi_df):\n",
        "    show = cmi_df.sort_values(['PMI','pair_count'], ascending=[False, False]).head(10)\n",
        "    print(\"\\n[TOP 10 CMI mappings]\")\n",
        "    for _, r in show.iterrows():\n",
        "        print(f\"  {r.get('verb_synset') or r['verb']}  →  {r['noun']}  | PMI={r['PMI']:.2f} (count={int(r['pair_count'])})\")\n",
        "\n",
        "print(\"\\n[OK] Summary complete.\")\n"
      ],
      "metadata": {
        "id": "nLdtiIC2cbcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 7 — Summary printout (patched: robust to missing 'human_label' / PMI columns)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# ---- Recover BASE_DIR defensively ----\n",
        "def recover_base_dir() -> Path:\n",
        "    root = Path(\"/content/drive/MyDrive\")\n",
        "    if root.exists():\n",
        "        for p in root.rglob(\"env_manifest.json\"):\n",
        "            try:\n",
        "                meta = json.loads(p.read_text())\n",
        "                bd = meta.get(\"base_dir\")\n",
        "                if bd and Path(bd).exists():\n",
        "                    return Path(bd)\n",
        "            except Exception:\n",
        "                pass\n",
        "    c1 = Path(\"/content/drive/MyDrive/Chinese Philosophy/Mozi\")\n",
        "    c2 = Path(\"/content/Mozi\")\n",
        "    if (c1 / \"metaphor_enriched.csv\").exists() or c1.exists():\n",
        "        c1.mkdir(parents=True, exist_ok=True)\n",
        "        return c1\n",
        "    c2.mkdir(parents=True, exist_ok=True)\n",
        "    return c2\n",
        "\n",
        "BASE_DIR = recover_base_dir()\n",
        "\n",
        "def safe_read(path: Path):\n",
        "    try:\n",
        "        return pd.read_csv(path) if path.exists() else None\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not read {path.name}: {e}\")\n",
        "        return None\n",
        "\n",
        "paths = {\n",
        "    \"metaphor_enriched_full.csv\": BASE_DIR / \"metaphor_enriched_full.csv\",\n",
        "    \"metaphor_enriched.csv\": BASE_DIR / \"metaphor_enriched.csv\",\n",
        "    \"metaphor_labels.csv\": BASE_DIR / \"metaphor_labels.csv\",\n",
        "    \"metaphor_enriched_labeled.csv\": BASE_DIR / \"metaphor_enriched_labeled.csv\",\n",
        "    \"cmi_candidates.csv\": BASE_DIR / \"cmi_candidates.csv\",\n",
        "    \"cmi_evidence_samples.csv\": BASE_DIR / \"cmi_evidence_samples.csv\",\n",
        "    \"metaphor_heatmap_counts.png\": BASE_DIR / \"metaphor_heatmap_counts.png\",\n",
        "    \"metaphor_heatmap_normalized.png\": BASE_DIR / \"metaphor_heatmap_normalized.png\",\n",
        "    \"paraphrases.csv\": BASE_DIR / \"paraphrases.csv\",\n",
        "}\n",
        "\n",
        "for name, p in paths.items():\n",
        "    print(f\"{name}: {'OK' if p.exists() else 'MISSING'} — {p.as_posix()}\")\n",
        "\n",
        "flag_df = safe_read(paths[\"metaphor_enriched.csv\"])\n",
        "lab_df  = safe_read(paths[\"metaphor_enriched_labeled.csv\"])\n",
        "cmi_df  = safe_read(paths[\"cmi_candidates.csv\"])\n",
        "\n",
        "if flag_df is not None and not flag_df.empty:\n",
        "    # Prefer unique rows by row_id for \"rows\" count; fallback to sentences\n",
        "    try:\n",
        "        rows_unique = flag_df[\"row_id\"].nunique()\n",
        "    except Exception:\n",
        "        rows_unique = flag_df[\"sentence\"].nunique()\n",
        "    print(f\"\\n[COUNT] Flagged tokens: {len(flag_df)} across {rows_unique} rows.\")\n",
        "\n",
        "# Labeled items (robust if 'human_label' missing)\n",
        "if lab_df is not None and not lab_df.empty:\n",
        "    if \"human_label\" in lab_df.columns:\n",
        "        try:\n",
        "            labeled_count = int((pd.to_numeric(lab_df[\"human_label\"], errors=\"coerce\").fillna(0) > 0).sum())\n",
        "        except Exception:\n",
        "            labeled_count = 0\n",
        "    else:\n",
        "        labeled_count = 0\n",
        "        print(\"[NOTE] 'human_label' not found in metaphor_enriched_labeled.csv — showing 0 labeled items.\")\n",
        "    print(f\"[COUNT] Labeled items: {labeled_count} (out of {len(lab_df)}).\")\n",
        "else:\n",
        "    print(\"[COUNT] Labeled items: 0 (no metaphor_enriched_labeled.csv or empty).\")\n",
        "\n",
        "# Top CMI mappings (robust to missing columns)\n",
        "if cmi_df is not None and not cmi_df.empty:\n",
        "    sort_cols = [c for c in [\"PMI\",\"pair_count\"] if c in cmi_df.columns]\n",
        "    if sort_cols:\n",
        "        ascending = [False] * len(sort_cols)\n",
        "        show = cmi_df.sort_values(sort_cols, ascending=ascending).head(10)\n",
        "    else:\n",
        "        show = cmi_df.head(10)\n",
        "    print(\"\\n[TOP 10 CMI mappings]\")\n",
        "    for _, r in show.iterrows():\n",
        "        vdisp = r[\"verb_synset\"] if \"verb_synset\" in r and isinstance(r[\"verb_synset\"], str) and r[\"verb_synset\"] else (r[\"verb\"] if \"verb\" in r else \"(verb)\")\n",
        "        ndisp = r[\"noun\"] if \"noun\" in r else \"(noun)\"\n",
        "        pmi  = r[\"PMI\"] if \"PMI\" in r else np.nan\n",
        "        pc   = r[\"pair_count\"] if \"pair_count\" in r else np.nan\n",
        "        pmi_str = f\"{pmi:.2f}\" if isinstance(pmi, (int,float,np.floating)) else \"n/a\"\n",
        "        pc_str  = str(int(pc)) if isinstance(pc, (int,float,np.floating)) and not np.isnan(pc) else \"n/a\"\n",
        "        print(f\"  {vdisp}  →  {ndisp}  | PMI={pmi_str} (count={pc_str})\")\n",
        "else:\n",
        "    print(\"\\n[TOP 10 CMI mappings] No candidates available.\")\n",
        "\n",
        "print(\"\\n[HEATMAPS]\")\n",
        "print(f\"Counts PNG:      {paths['metaphor_heatmap_counts.png'].as_posix()}  ({'OK' if paths['metaphor_heatmap_counts.png'].exists() else 'MISSING'})\")\n",
        "print(f\"Normalized PNG:  {paths['metaphor_heatmap_normalized.png'].as_posix()}  ({'OK' if paths['metaphor_heatmap_normalized.png'].exists() else 'MISSING'})\")\n",
        "\n",
        "print(\"\\n[OK] Summary complete.\")\n"
      ],
      "metadata": {
        "id": "YYNTdOKsc3eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7f5cc6e"
      },
      "source": [
        "# Task\n",
        "Run the full metaphor analysis pipeline on the file \"/content/drive/MyDrive/Chinese Philosophers/chinese_philosophers_quotes_corrected.csv\", saving all output files to \"/content/drive/MyDrive/Chinese Philosophers/\". The pipeline includes metaphor detection, QA and sampling, CMI analysis, heatmap generation, and optional paraphrasing. Merge the results into the \"metaphor_enriched_full.csv\" file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bf80b5e"
      },
      "source": [
        "## Update setup and data loading\n",
        "\n",
        "### Subtask:\n",
        "Modify the initial setup cell to load data from the new specified file path `/content/drive/MyDrive/Chinese Philosophers/chinese_philosophers_quotes_corrected.csv` and set the output directory to `/content/drive/MyDrive/Chinese Philosophers/`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5443d439"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to modify the initial setup cell to load data from the new path and set the base directory. I will locate the cell that handles setup and data loading, update the `BASE_DIR` variable, and modify the file loading logic to point to the new CSV file. I will also ensure that subdirectory creation uses the updated `BASE_DIR`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c3e4ff2"
      },
      "source": [
        "#@title Step 0 — Setup + data loader (patched: safe version detection)\n",
        "# Installs (pinned), downloads models, mounts Drive, auto-finds Mozi CSV, normalizes columns, and sets BASE_DIR.\n",
        "!pip -q install \"spacy==3.8.2\" \"pandas==2.2.2\" \"matplotlib==3.9.0\" \"nltk==3.9.1\" \"wordfreq==3.1.1\" \"gradio>=4.41,<5\" \"openai>=1.40.0,<2\"\n",
        "\n",
        "import sys, os, pathlib, re, json, random, math, time, textwrap, itertools, collections\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NLTK setup\n",
        "import nltk\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "# Google Drive mount (defensive)\n",
        "# --- MODIFIED: Set BASE_DIR to the new output directory ---\n",
        "BASE_DIR = pathlib.Path(\"/content/drive/MyDrive/Chinese Philosophers\")\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    if not pathlib.Path(\"/content/drive\").exists():\n",
        "        os.makedirs(\"/content/drive\", exist_ok=True)\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Drive mount issue:\", e)\n",
        "        print(\"[HINT] Continuing without Drive; paths will default to /content only.\")\n",
        "        # Fallback BASE_DIR in case of Drive mount failure\n",
        "        BASE_DIR = pathlib.Path(\"/content/Chinese Philosophers\")\n",
        "except Exception as e:\n",
        "    print(\"[WARN] Not running in Colab or Drive not available:\", e)\n",
        "    # Fallback BASE_DIR if not in Colab or Drive unavailable\n",
        "    BASE_DIR = pathlib.Path(\"/content/Chinese Philosophers\")\n",
        "\n",
        "# Ensure BASE_DIR exists\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "# --- Ensure subdirectory creation uses the updated BASE_DIR ---\n",
        "(SRC_DIR := BASE_DIR / \"source_corpus\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# spaCy model check / install\n",
        "import spacy\n",
        "NEEDS_MODEL = False\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except Exception:\n",
        "    NEEDS_MODEL = True\n",
        "    print(\"[INFO] Installing spaCy model en_core_web_sm...\")\n",
        "    import spacy.cli as spacy_cli\n",
        "    spacy_cli.download(\"en_core_web_sm\")\n",
        "    # Will require restart; provided in the next cell.\n",
        "\n",
        "# WordNet\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Auto-discover Mozi CSV (case-insensitive)\n",
        "# --- MODIFIED: Directly specify the new CSV file path ---\n",
        "def find_philosophers_csv():\n",
        "    csv_path = pathlib.Path(\"/content/drive/MyDrive/Chinese Philosophers/chinese_philosophers_quotes_corrected.csv\")\n",
        "    if csv_path.exists():\n",
        "        return csv_path\n",
        "    return None\n",
        "\n",
        "philosophers_csv = find_philosophers_csv()\n",
        "if philosophers_csv is None:\n",
        "    print(f\"[WARN] Could not find the specified CSV: /content/drive/MyDrive/Chinese Philosophers/chinese_philosophers_quotes_corrected.csv\")\n",
        "    print(f\"[HINT] Ensure the file exists at this exact path.\")\n",
        "else:\n",
        "    print(f\"[INFO] Using CSV: {philosophers_csv}\")\n",
        "\n",
        "# Load and normalize\n",
        "def load_normalize_csv(pathlike):\n",
        "    assert pathlike is not None and pathlib.Path(pathlike).exists(), \"CSV file not found.\"\n",
        "    df = pd.read_csv(pathlike)\n",
        "    # Normalize columns\n",
        "    df.columns = [str(c).strip().lower() for c in df.columns]\n",
        "    # Pick a text column\n",
        "    TEXT_CANDIDATES = [\"text\",\"translation\",\"english\",\"content\",\"verse_text\",\"line_text\",\"quote\"] # Added 'quote' as a candidate\n",
        "    text_col = next((c for c in TEXT_CANDIDATES if c in df.columns), None)\n",
        "    if text_col is None:\n",
        "        nonnum = [c for c in df.columns if df[c].dtype == object]\n",
        "        assert nonnum, \"No suitable text column found.\"\n",
        "        text_col = nonnum[0]\n",
        "        print(f\"[WARN] Falling back to column '{text_col}' as text.\")\n",
        "    df = df.copy()\n",
        "    df[\"__text__\"] = df[text_col].astype(str)\n",
        "    # Chapter-like column, optional - Using 'chapter' as a placeholder if needed, but might not be relevant for quotes\n",
        "    CHAPTER_CAND = [\"chapter\",\"chap\",\"juan\",\"book\",\"section\",\"source\",\"philosopher\"] # Added 'source' and 'philosopher'\n",
        "    chap_col = next((c for c in CHAPTER_CAND if c in df.columns), None)\n",
        "    if chap_col is None:\n",
        "        df[\"__chapter__\"] = \"Unknown\"\n",
        "    else:\n",
        "        df[\"__chapter__\"] = df[chap_col].astype(str).replace({np.nan:\"Unknown\"})\n",
        "    # Add row_id\n",
        "    if \"__row_id__\" not in df.columns:\n",
        "        df[\"__row_id__\"] = np.arange(len(df))\n",
        "    # Minimal clean\n",
        "    df[\"__text__\"] = df[\"__text__\"].fillna(\"\").map(lambda s: re.sub(r\"\\s+\", \" \", s).strip())\n",
        "    return df, text_col, chap_col\n",
        "\n",
        "df, TEXT_COL, CHAP_COL = (None, None, None)\n",
        "try:\n",
        "    if philosophers_csv:\n",
        "        df, TEXT_COL, CHAP_COL = load_normalize_csv(philosophers_csv)\n",
        "        print(f\"[INFO] Loaded {len(df)} rows; text='{TEXT_COL}', chapter='{CHAP_COL or '__chapter__'}'.\")\n",
        "        print(df[[ \"__row_id__\", \"__chapter__\", \"__text__\" ]].head(2))\n",
        "        # Cache normalized copy\n",
        "        df.to_csv(BASE_DIR / \"philosophers_normalized_preview.csv\", index=False)\n",
        "        print(f\"[OK] Preview written: { (BASE_DIR / 'philosophers_normalized_preview.csv').as_posix() }\")\n",
        "    else:\n",
        "        print(\"[ERR] No CSV loaded.\")\n",
        "except AssertionError as e:\n",
        "    print(\"[ERR]\", e)\n",
        "    print(\"[HINT] Ensure your CSV exists and has a recognizable text column.\")\n",
        "\n",
        "# ---- Safe package version reporting (fix for wordfreq lacking __version__) ----\n",
        "try:\n",
        "    from importlib.metadata import version as _pkg_version\n",
        "except Exception:\n",
        "    _pkg_version = None\n",
        "\n",
        "def get_version(pkg: str, module_name: str | None = None) -> str:\n",
        "    try:\n",
        "        mod = __import__(module_name or pkg)\n",
        "        v = getattr(mod, \"__version__\", None)\n",
        "        if v:\n",
        "            return str(v)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if _pkg_version is not None:\n",
        "        try:\n",
        "            return _pkg_version(pkg)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return \"unknown\"\n",
        "\n",
        "# Save a small env manifest for reproducibility\n",
        "manifest = {\n",
        "    \"python\": sys.version,\n",
        "    \"packages\": {\n",
        "        \"spacy\": get_version(\"spacy\"),\n",
        "        \"pandas\": get_version(\"pandas\"),\n",
        "        \"matplotlib\": get_version(\"matplotlib\", \"matplotlib\"),\n",
        "        \"nltk\": get_version(\"nltk\"),\n",
        "        \"wordfreq\": get_version(\"wordfreq\"),\n",
        "        \"gradio\": get_version(\"gradio\"),\n",
        "        \"openai\": get_version(\"openai\"),\n",
        "    },\n",
        "    # --- MODIFIED: Reference the new CSV path in the manifest ---\n",
        "    \"data_csv\": str(philosophers_csv) if philosophers_csv else None,\n",
        "    \"base_dir\": BASE_DIR.as_posix()\n",
        "}\n",
        "with open(BASE_DIR / \"env_manifest.json\",\"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "print(\"[OK] Wrote env_manifest.json\")\n",
        "print(\"[NOTE] If the spaCy model was just installed, run the next cell once to restart the runtime.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "485f0734"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully modified the setup cell and loaded the data from the new path. The next step is to run the optional restart cell if the spaCy model was just installed. This ensures that spaCy loads correctly with the updated dependencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e28d521a"
      },
      "source": [
        "#@title (Optional) Restart runtime (run once if en_core_web_sm was just installed)\n",
        "import os, sys\n",
        "print(\"[INFO] Restarting runtime now...\")\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01755a4e"
      },
      "source": [
        "**Reasoning**:\n",
        "The runtime has been restarted. The next step is to run the modified Step 1 cell to process the data from the new file and generate the initial metaphor candidates and enriched CSV files, using the updated `BASE_DIR`. This cell is self-contained and will recover the necessary variables and data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4615bc31"
      },
      "source": [
        "#@title Step 1 — Heuristic metaphor candidates → CSV (patched: self-contained, rebuilds BASE_DIR if missing)\n",
        "# spaCy parse with nlp.pipe; SPV/CCO rules; WordNet-based abstractness; writes:\n",
        "# - metaphor_enriched_full.csv (all tokens)\n",
        "# - metaphor_enriched.csv (flagged tokens only)\n",
        "\n",
        "import os, json, re, math, pathlib, collections\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ---------- Recover BASE_DIR (works even after runtime restart) ----------\n",
        "from pathlib import Path\n",
        "\n",
        "def recover_base_dir() -> Path:\n",
        "    # Try env manifest anywhere under MyDrive first\n",
        "    root = Path(\"/content/drive/MyDrive\")\n",
        "    if root.exists():\n",
        "        for p in root.rglob(\"env_manifest.json\"):\n",
        "            try:\n",
        "                meta = json.loads(p.read_text())\n",
        "                bd = meta.get(\"base_dir\")\n",
        "                if bd and Path(bd).exists():\n",
        "                    return Path(bd)\n",
        "            except Exception:\n",
        "                pass\n",
        "    # Fallbacks\n",
        "    c1 = Path(\"/content/drive/MyDrive/Chinese Philosophers\") # --- MODIFIED: Use the new path ---\n",
        "    c2 = Path(\"/content/Chinese Philosophers\")              # --- MODIFIED: Use the new path ---\n",
        "    if (c1 / \"philosophers_normalized_preview.csv\").exists() or c1.exists(): # --- MODIFIED: Use the new preview file name ---\n",
        "        c1.mkdir(parents=True, exist_ok=True)\n",
        "        return c1\n",
        "    c2.mkdir(parents=True, exist_ok=True)\n",
        "    return c2\n",
        "\n",
        "BASE_DIR = recover_base_dir()\n",
        "print(f\"[INFO] BASE_DIR = {BASE_DIR}\")\n",
        "\n",
        "# ---------- Ensure normalized preview exists; rebuild if missing ----------\n",
        "def find_philosophers_csv(): # --- MODIFIED: Function name reflects new file ---\n",
        "    # --- MODIFIED: Directly specify the new CSV file path ---\n",
        "    csv_path = Path(\"/content/drive/MyDrive/Chinese Philosophers/chinese_philosophers_quotes_corrected.csv\")\n",
        "    if csv_path.exists():\n",
        "        return csv_path\n",
        "    return None\n",
        "\n",
        "\n",
        "def load_normalize_csv(pathlike: Path):\n",
        "    df = pd.read_csv(pathlike)\n",
        "    df.columns = [str(c).strip().lower() for c in df.columns]\n",
        "    TEXT_CANDIDATES = [\"text\",\"translation\",\"english\",\"content\",\"verse_text\",\"line_text\",\"quote\"] # --- MODIFIED: Added 'quote' ---\n",
        "    text_col = next((c for c in TEXT_CANDIDATES if c in df.columns), None)\n",
        "    if text_col is None:\n",
        "        nonnum = [c for c in df.columns if df[c].dtype == object]\n",
        "        assert nonnum, \"No suitable text column found.\"\n",
        "        text_col = nonnum[0]\n",
        "        print(f\"[WARN] Falling back to column '{text_col}' as text.\")\n",
        "    CHAPTER_CAND = [\"chapter\",\"chap\",\"juan\",\"book\",\"section\",\"source\",\"philosopher\"] # --- MODIFIED: Added 'source' and 'philosopher' ---\n",
        "    chap_col = next((c for c in CHAPTER_CAND if c in df.columns), None)\n",
        "\n",
        "    df = df.copy()\n",
        "    df[\"__text__\"] = df[text_col].astype(str)\n",
        "    df[\"__chapter__\"] = (df[chap_col].astype(str) if chap_col else \"Unknown\")\n",
        "    if \"__row_id__\" not in df.columns:\n",
        "        df[\"__row_id__\"] = np.arange(len(df))\n",
        "    df[\"__text__\"] = df[\"__text__\"].fillna(\"\").map(lambda s: re.sub(r\"\\s+\", \" \", s).strip())\n",
        "    return df\n",
        "\n",
        "# --- MODIFIED: Use new preview file name ---\n",
        "preview_path = BASE_DIR / \"philosophers_normalized_preview.csv\"\n",
        "if not preview_path.exists():\n",
        "    print(\"[INFO] No philosophers_normalized_preview.csv found — rebuilding from source CSV...\")\n",
        "    # --- MODIFIED: Use the function for the new CSV file ---\n",
        "    src_csv = find_philosophers_csv()\n",
        "    assert src_csv is not None and src_csv.exists(), (\n",
        "        \"[ERR] Could not find the specified CSV. Place a file like 'chinese_philosophers_quotes_corrected.csv' under \"\n",
        "        f\"/content/drive/MyDrive/Chinese Philosophers, then re-run.\" # --- MODIFIED: Updated hint ---\n",
        "    )\n",
        "    print(f\"[INFO] Using CSV: {src_csv}\")\n",
        "    df_norm = load_normalize_csv(src_csv)\n",
        "    df_norm.to_csv(preview_path, index=False)\n",
        "    print(f\"[OK] Preview written: {preview_path}\")\n",
        "\n",
        "assert preview_path.exists(), \"[ERR] No normalized preview. Re-run Step 0 if this persists.\"\n",
        "df = pd.read_csv(preview_path)\n",
        "print(f\"[INFO] Loaded normalized preview: {len(df)} rows\")\n",
        "\n",
        "# ---------- Dependencies ----------\n",
        "# No need to import spacy, nltk, wordfreq here as they are imported in the function body\n",
        "\n",
        "# ---------- Heuristic resources ----------\n",
        "MEASUREMENT_VERBS = {\n",
        "    \"measure\",\"weigh\",\"gauge\",\"calibrate\",\"benchmark\",\"assess\",\"evaluate\",\"estimate\",\"calculate\",\"count\",\"scale\",\"standardize\",\"balance\",\"level\",\"meter\",\"rate\",\"score\"\n",
        "}\n",
        "MEASUREMENT_NOUNS = {\n",
        "    \"measure\",\"measurement\",\"weight\",\"scale\",\"ruler\",\"square\",\"compass\",\"yardstick\",\"standard\",\"gauge\",\"balance\",\"meter\",\"rule\",\"benchmark\",\"calibration\"\n",
        "}\n",
        "CRAFT_VERBS = {\n",
        "    \"build\",\"construct\",\"forge\",\"craft\",\"hammer\",\"chisel\",\"cut\",\"saw\",\"drill\",\"weld\",\"mold\",\"carve\",\"join\",\"fit\",\"assemble\",\"fabricate\",\"repair\",\"mend\",\"nail\",\"bolt\",\"plane\",\"lathe\",\"engineer\",\"design\",\"hew\",\"hewn\"\n",
        "}\n",
        "CRAFT_NOUNS = {\n",
        "    \"tool\",\"machine\",\"gear\",\"axle\",\"wheel\",\"lever\",\"pulley\",\"square\",\"ruler\",\"compass\",\"chisel\",\"hammer\",\"saw\",\"drill\",\"plumb\",\"line\",\"string\",\"inkline\",\"compass\"\n",
        "}\n",
        "ABSTRACT_LEXNAMES = {\n",
        "    \"noun.cognition\",\"noun.attribute\",\"noun.communication\",\"noun.act\",\"noun.event\",\"noun.feeling\",\"noun.motive\",\"noun.state\",\"noun.time\",\"noun.location\",\"noun.group\"\n",
        "}\n",
        "CONCRETE_LEXNAMES = {\n",
        "    \"noun.artifact\",\"noun.object\",\"noun.body\",\"noun.animal\",\"noun.plant\",\"noun.food\",\"noun.substance\",\"noun.shape\",\"noun.quantity\"\n",
        "}\n",
        "INTANGIBLE_OVERRIDES = {\n",
        "    \"virtue\",\"kindness\",\"righteousness\",\"faithfulness\",\"justice\",\"policy\",\"plan\",\"argument\",\"doctrine\",\"speech\",\"language\",\"order\",\"disorder\",\"governance\",\"law\",\"heaven\",\"heaven's\",\"heavens\",\"profit\",\"benefit\",\"harm\",\"intention\"\n",
        "}\n",
        "STOP_VERBS = {\n",
        "    \"be\",\"am\",\"is\",\"are\",\"was\",\"were\",\"been\",\"being\",\n",
        "    \"have\",\"has\",\"had\",\n",
        "    \"do\",\"does\",\"did\",\"done\",\n",
        "    \"get\",\"gets\",\"got\",\"become\",\n",
        "    \"make\",\"makes\",\"made\",\n",
        "    \"take\",\"takes\",\"took\",\"taken\",\n",
        "    \"give\",\"gives\",\"gave\",\"given\",\n",
        "    \"go\",\"goes\",\"went\",\"gone\",\"come\",\"comes\",\"came\",\n",
        "    \"put\",\"puts\",\"set\",\"sets\",\"keep\",\"keeps\",\"kept\",\"let\",\"lets\",\"seem\",\"seems\",\n",
        "    \"see\",\"sees\",\"saw\",\"seen\",\"look\",\"looks\",\n",
        "    \"want\",\"wants\",\"need\",\"needs\",\n",
        "    \"use\",\"uses\",\"try\",\"tries\",\"tried\",\n",
        "    \"find\",\"finds\",\"found\",\"think\",\"thinks\",\"thought\",\n",
        "    \"tell\",\"tells\",\"told\",\"ask\",\"asks\",\"asked\",\n",
        "    \"work\",\"works\",\"feel\",\"feels\",\"felt\",\n",
        "    \"leave\",\"leaves\",\"call\",\"calls\",\"called\",\n",
        "    \"like\",\"likes\",\"follow\",\"follows\",\"followed\",\n",
        "    \"begin\",\"begins\",\"began\",\"begun\",\"happen\",\"happens\",\"include\",\"includes\",\"continue\",\"continues\",\n",
        "    \"lose\",\"loses\",\"lost\",\"join\",\"joins\",\"joined\",\"hold\",\"holds\",\"show\",\"shows\",\"provide\",\"provides\"\n",
        "}\n",
        "\n",
        "# ---------- Load spaCy and other libs inside the function scope if needed ----------\n",
        "def load_dependencies():\n",
        "    try:\n",
        "        import spacy\n",
        "        nlp_model = spacy.load(\"en_core_web_sm\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"[ERR] spaCy model 'en_core_web_sm' missing. Run Step 0 and the optional restart cell.\") from e\n",
        "\n",
        "    try:\n",
        "        from nltk.corpus import wordnet as wn_corpus\n",
        "    except Exception:\n",
        "         raise RuntimeError(\"[ERR] NLTK WordNet missing. Run Step 0.\") from None\n",
        "\n",
        "    try:\n",
        "        from wordfreq import zipf_frequency_func\n",
        "    except Exception:\n",
        "        # Safe fallback if wordfreq not available (treat as frequent enough)\n",
        "        def zipf_frequency_func(token_text: str, lang: str) -> float:\n",
        "            return 5.0\n",
        "        print(\"[WARN] wordfreq not available. Using fallback for zipf_frequency.\")\n",
        "\n",
        "\n",
        "    return nlp_model, wn_corpus, zipf_frequency_func\n",
        "\n",
        "nlp, wn, zipf_frequency = load_dependencies()\n",
        "\n",
        "\n",
        "def wordnet_lexnames(word, pos=None):\n",
        "    try:\n",
        "        syns = wn.synsets(word, pos=pos)\n",
        "        return {s.lexname() for s in syns}\n",
        "    except Exception:\n",
        "        return set()\n",
        "\n",
        "def is_abstract_noun(noun_lemma: str) -> bool:\n",
        "    noun_lemma = noun_lemma.lower()\n",
        "    if noun_lemma in INTANGIBLE_OVERRIDES:\n",
        "        return True\n",
        "    lex = wordnet_lexnames(noun_lemma, pos=wn.NOUN)\n",
        "    if not lex:\n",
        "        return False\n",
        "    return any(l in ABSTRACT_LEXNAMES for l in lex) and not any(l in CONCRETE_LEXNAMES for l in lex)\n",
        "\n",
        "def family_guess_for(verb_lemma: str, obj_lemma: str) -> str:\n",
        "    if verb_lemma in MEASUREMENT_VERBS or obj_lemma in MEASUREMENT_NOUNS:\n",
        "        return \"MEASUREMENT\"\n",
        "    if verb_lemma in CRAFT_VERBS or obj_lemma in CRAFT_NOUNS:\n",
        "        return \"CRAFT_CONSTRUCTION\"\n",
        "    return \"OTHER\"\n",
        "\n",
        "def zipf_ok(token_text: str, min_zipf=1.5):\n",
        "    try:\n",
        "        return zipf_frequency(token_text, \"en\") >= min_zipf\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "# ---------- Parse & flag ----------\n",
        "texts = df[\"__text__\"].fillna(\"\").tolist()\n",
        "chapters = df[\"__chapter__\"].fillna(\"Unknown\").tolist()\n",
        "row_ids = df[\"__row_id__\"].tolist()\n",
        "\n",
        "docs = list(nlp.pipe(texts, disable=[\"ner\"]))\n",
        "print(f\"[INFO] Parsed {len(docs)} documents with spaCy.\")\n",
        "\n",
        "full_rows = []\n",
        "flag_rows = []\n",
        "\n",
        "for rid, chap, doc, raw in zip(row_ids, chapters, docs, texts):\n",
        "    for sent_i, sent in enumerate(doc.sents):\n",
        "        for tok in sent:\n",
        "            if tok.pos_ == \"VERB\" and tok.lemma_.lower() not in STOP_VERBS:\n",
        "                reasons = []\n",
        "                fam = \"OTHER\"\n",
        "                obj_nouns = [c for c in tok.children if c.dep_ in (\"dobj\",\"obj\",\"attr\",\"pobj\") and c.pos_ in (\"NOUN\",\"PROPN\",\"PRON\")]\n",
        "                subj_nouns = [c for c in tok.children if c.dep_ in (\"nsubj\",\"nsubjpass\") and c.pos_ in (\"NOUN\",\"PROPN\",\"PRON\")]\n",
        "\n",
        "                def noun_lemma(n):\n",
        "                    return (n.lemma_ or n.text).lower()\n",
        "\n",
        "                for n in obj_nouns + subj_nouns:\n",
        "                    v = tok.lemma_.lower()\n",
        "                    nlem = noun_lemma(n)\n",
        "                    fam = family_guess_for(v, nlem)\n",
        "                    if (v in MEASUREMENT_VERBS or v in CRAFT_VERBS) and is_abstract_noun(nlem) and zipf_ok(n.text):\n",
        "                        if v in MEASUREMENT_VERBS:\n",
        "                            reasons.append(f\"MEASUREMENT_on_abstract_{n.dep_}:{nlem}\")\n",
        "                        else:\n",
        "                            reasons.append(f\"CRAFT_on_abstract_{n.dep_}:{nlem}\")\n",
        "                    if nlem in (MEASUREMENT_NOUNS | CRAFT_NOUNS) and any(is_abstract_noun(h.lemma_.lower()) for h in [tok.head] if h is not None):\n",
        "                        reasons.append(f\"ARTIFACT_modifier_to_abstract:{nlem}\")\n",
        "\n",
        "                row = {\n",
        "                    \"row_id\": rid,\n",
        "                    \"chapter\": chap,\n",
        "                    \"sent_id\": f\"{rid}:{sent_i}\",\n",
        "                    \"sentence\": sent.text.strip(),\n",
        "                    \"focus_token\": tok.text,\n",
        "                    \"focus_lemma\": tok.lemma_.lower(),\n",
        "                    \"focus_pos\": tok.pos_,\n",
        "                    \"focus_dep\": tok.dep_,\n",
        "                    \"is_metaphor_heuristic\": 1 if reasons else 0,\n",
        "                    \"family_guess\": fam if reasons else \"OTHER\",\n",
        "                    \"reasons\": \";\".join(sorted(set(reasons))) if reasons else \"\",\n",
        "                }\n",
        "                full_rows.append(row)\n",
        "                if reasons:\n",
        "                    flag_rows.append(row)\n",
        "\n",
        "full_df = pd.DataFrame(full_rows)\n",
        "flag_df = pd.DataFrame(flag_rows)\n",
        "\n",
        "out_full = BASE_DIR / \"metaphor_enriched_full.csv\"\n",
        "out_flag = BASE_DIR / \"metaphor_enriched.csv\"\n",
        "full_df.to_csv(out_full, index=False)\n",
        "flag_df.to_csv(out_flag, index=False)\n",
        "\n",
        "print(f\"[OK] Wrote {out_full.name} (rows: {len(full_df)}) and {out_flag.name} (flagged rows: {len(flag_df)}).\")\n",
        "if len(flag_df) == 0:\n",
        "    print(\"[HINT] No flagged items found. You can broaden heuristics or proceed to CMI discovery.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0351831b"
      },
      "source": [
        "**Reasoning**:\n",
        "Step 1 has successfully generated the heuristic metaphor candidates. The next logical step in the pipeline is Step 2, which performs QA, deduplication, and optional sampling of these candidates for manual labeling. This step is necessary before proceeding to CMI analysis or heatmap generation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a76ce4ec"
      },
      "source": [
        "#@title Step 2 — Candidate QA + optional sampling for labeling\n",
        "# Builds quick QA summaries, deduplicates candidates, and (optionally) creates a stratified sample\n",
        "# so you can label a subset. Outputs:\n",
        "#   - metaphor_QA_summary.csv\n",
        "#   - metaphor_candidates_dedup.csv\n",
        "#   - metaphor_label_queue.csv  (stratified sample; size via LABEL_SAMPLE env, default=300)\n",
        "# Also ensures 'idx' exists in metaphor_enriched.csv for stable labeling.\n",
        "\n",
        "import os, pathlib, math, json, random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Recover BASE_DIR defensively\n",
        "from pathlib import Path\n",
        "def recover_base_dir() -> Path:\n",
        "    root = Path(\"/content/drive/MyDrive\")\n",
        "    if root.exists():\n",
        "        for p in root.rglob(\"env_manifest.json\"):\n",
        "            try:\n",
        "                meta = json.loads(p.read_text())\n",
        "                bd = meta.get(\"base_dir\")\n",
        "                if bd and Path(bd).exists():\n",
        "                    return Path(bd)\n",
        "            except Exception:\n",
        "                pass\n",
        "    c1 = Path(\"/content/drive/MyDrive/Chinese Philosophers\") # --- MODIFIED: Use the new path ---\n",
        "    c2 = Path(\"/content/Chinese Philosophers\")              # --- MODIFIED: Use the new path ---\n",
        "    if (c1 / \"metaphor_enriched.csv\").exists() or c1.exists():\n",
        "        c1.mkdir(parents=True, exist_ok=True)\n",
        "        return c1\n",
        "    c2.mkdir(parents=True, exist_ok=True)\n",
        "    return c2\n",
        "\n",
        "BASE_DIR = recover_base_dir()\n",
        "FLAG_PATH = BASE_DIR / \"metaphor_enriched.csv\"\n",
        "FULL_PATH = BASE_DIR / \"metaphor_enriched_full.csv\" # Although not used in this specific step, keep for consistency\n",
        "\n",
        "assert FLAG_PATH.exists(), \"[ERR] Run Step 1 first to produce metaphor_enriched.csv\"\n",
        "\n",
        "df = pd.read_csv(FLAG_PATH)\n",
        "if df.empty:\n",
        "    print(\"[WARN] No flagged candidates to summarize. You can still proceed to CMI (Step 4).\")\n",
        "\n",
        "# Ensure stable 'idx' for labeler\n",
        "if \"idx\" not in df.columns:\n",
        "    df[\"idx\"] = np.arange(len(df))\n",
        "    df.to_csv(FLAG_PATH, index=False)\n",
        "    print(\"[OK] Added 'idx' column to metaphor_enriched.csv\")\n",
        "\n",
        "# Deduplicate by (sentence, focus_lemma) to reduce near-duplicates\n",
        "dedup_cols = [\"sentence\",\"focus_lemma\"]\n",
        "dedup_df = (df\n",
        "            .sort_values([\"sentence\",\"focus_lemma\",\"idx\"])\n",
        "            .drop_duplicates(subset=dedup_cols, keep=\"first\")\n",
        "            .reset_index(drop=True))\n",
        "dedup_out = BASE_DIR / \"metaphor_candidates_dedup.csv\"\n",
        "dedup_df.to_csv(dedup_out, index=False)\n",
        "\n",
        "# QA summary: counts by family and by (chapter,family)\n",
        "fam_counts = df.groupby(\"family_guess\")[\"row_id\"].nunique().sort_values(ascending=False)\n",
        "chap_fam = df.groupby([\"chapter\",\"family_guess\"])[\"row_id\"].nunique().unstack(fill_value=0)\n",
        "summary_rows = []\n",
        "for fam, cnt in fam_counts.items():\n",
        "    summary_rows.append({\"level\":\"family\",\"key\":str(fam),\"rows\":int(cnt)})\n",
        "\n",
        "# Add chapter × family counts if chap_fam is not empty\n",
        "if not chap_fam.empty:\n",
        "    for chap, row in chap_fam.iterrows():\n",
        "        for fam in row.index:\n",
        "             # Only add if count is greater than 0\n",
        "            if row[fam] > 0:\n",
        "                summary_rows.append({\"level\":\"chapter×family\",\"key\":f\"{chap}×{fam}\",\"rows\":int(row[fam])})\n",
        "\n",
        "\n",
        "qa_df = pd.DataFrame(summary_rows).sort_values([\"level\",\"rows\"], ascending=[True, False])\n",
        "qa_out = BASE_DIR / \"metaphor_QA_summary.csv\"\n",
        "qa_df.to_csv(qa_out, index=False)\n",
        "\n",
        "print(f\"[OK] Wrote {qa_out.name} (rows: {len(qa_df)})\")\n",
        "print(f\"[OK] Wrote {dedup_out.name} (rows: {len(dedup_df)})\")\n",
        "\n",
        "# Optional: create a stratified sample for labeling\n",
        "LABEL_SAMPLE = int(os.environ.get(\"LABEL_SAMPLE\", \"300\"))  # set env to change size\n",
        "if len(dedup_df) == 0:\n",
        "    print(\"[HINT] No items to sample from.\")\n",
        "else:\n",
        "    random.seed(42)\n",
        "    # Use 'family_guess' for simple stratification; ensure presence\n",
        "    if \"family_guess\" not in dedup_df.columns:\n",
        "        dedup_df[\"family_guess\"] = \"OTHER\"\n",
        "    fam_sizes = dedup_df[\"family_guess\"].value_counts().to_dict()\n",
        "    total = len(dedup_df)\n",
        "    # Compute target per family proportional to frequency, at least 1 each if possible\n",
        "    targets = {fam: max(1, round(LABEL_SAMPLE * cnt / total)) for fam, cnt in fam_sizes.items()}\n",
        "    # Adjust totals if rounding drift\n",
        "    diff = LABEL_SAMPLE - sum(targets.values())\n",
        "    if diff != 0:\n",
        "        # Distribute remainder to largest families (positive) or subtract from largest (negative)\n",
        "        order = sorted(fam_sizes.items(), key=lambda kv: kv[1], reverse=True)\n",
        "        i = 0\n",
        "        while diff != 0 and order:\n",
        "            fam = order[i % len(order)][0]\n",
        "            if diff > 0:\n",
        "                targets[fam] += 1\n",
        "                diff -= 1\n",
        "            else:\n",
        "                if targets[fam] > 1:\n",
        "                    targets[fam] -= 1\n",
        "                    diff += 1\n",
        "            i += 1\n",
        "\n",
        "    parts = []\n",
        "    for fam, k in targets.items():\n",
        "        sub = dedup_df[dedup_df[\"family_guess\"]==fam]\n",
        "        if len(sub) <= k:\n",
        "            parts.append(sub)\n",
        "        else:\n",
        "            parts.append(sub.sample(n=k, random_state=42))\n",
        "    sample_df = pd.concat(parts, ignore_index=True).sort_values(\"idx\").reset_index(drop=True)\n",
        "\n",
        "    # Ensure columns that labeler expects are present\n",
        "    keep_cols = [\"idx\",\"row_id\",\"sent_id\",\"sentence\",\"focus_token\",\"focus_lemma\",\"chapter\",\"is_metaphor_heuristic\",\"family_guess\",\"reasons\"]\n",
        "    for c in keep_cols:\n",
        "        if c not in sample_df.columns:\n",
        "            sample_df[c] = \"\" if c not in (\"is_metaphor_heuristic\",) else 0\n",
        "\n",
        "    sample_out = BASE_DIR / \"metaphor_label_queue.csv\"\n",
        "    sample_df.to_csv(sample_out, index=False)\n",
        "    print(f\"[OK] Wrote {sample_out.name} (rows: {len(sample_df)}).\")\n",
        "\n",
        "print(\"[NOTE] To label only the sample, you can edit Step 3 to set FLAG_PATH = BASE_DIR/'metaphor_label_queue.csv'. Otherwise Step 3 will load all flagged items.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9363649b"
      },
      "source": [
        "**Reasoning**:\n",
        "Step 2 has completed QA and sampling. The next step is to run the CMI analysis (Step 4) to identify verb-noun pairs indicative of conceptual metaphors. This is a crucial step in the metaphor analysis pipeline. Step 3 (Labeler) is optional and can be run separately.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "471b5d52"
      },
      "source": [
        "#@title Step 4 — CMI discovery (low-data friendly) → cmi_candidates.csv + cmi_evidence_samples.csv\n",
        "# Looser thresholds, detailed diagnostics, and guaranteed columns to avoid Step 5 warnings.\n",
        "\n",
        "import os, re, json, pathlib, random, math, collections\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# -------- Recover BASE_DIR --------\n",
        "def recover_base_dir() -> Path:\n",
        "    root = Path(\"/content/drive/MyDrive\")\n",
        "    if root.exists():\n",
        "        for p in root.rglob(\"env_manifest.json\"):\n",
        "            try:\n",
        "                meta = json.loads(p.read_text())\n",
        "                bd = meta.get(\"base_dir\")\n",
        "                if bd and Path(bd).exists():\n",
        "                    return Path(bd)\n",
        "            except Exception:\n",
        "                pass\n",
        "    # Fallbacks\n",
        "    c1 = Path(\"/content/drive/MyDrive/Chinese Philosophers\") # --- MODIFIED: Use the new path ---\n",
        "    c2 = Path(\"/content/Chinese Philosophers\")              # --- MODIFIED: Use the new path ---\n",
        "    if (c1 / \"philosophers_normalized_preview.csv\").exists() or c1.exists(): # --- MODIFIED: Use the new preview file name ---\n",
        "        c1.mkdir(parents=True, exist_ok=True)\n",
        "        return c1\n",
        "    c2.mkdir(parents=True, exist_ok=True)\n",
        "    return c2\n",
        "\n",
        "BASE_DIR = recover_base_dir()\n",
        "print(f\"[INFO] BASE_DIR = {BASE_DIR}\")\n",
        "\n",
        "# -------- Inputs & params --------\n",
        "# --- MODIFIED: Use the new preview file name ---\n",
        "preview_path = BASE_DIR / \"philosophers_normalized_preview.csv\"\n",
        "assert preview_path.exists(), \"[ERR] Run Step 0 first.\"\n",
        "df = pd.read_csv(preview_path)\n",
        "\n",
        "CMI_MIN_PAIR_COUNT = int(os.environ.get(\"CMI_MIN_PAIR_COUNT\", \"1\"))  # was 2; now 1 by default\n",
        "PMI_SMOOTH_K = float(os.environ.get(\"CMI_PMI_SMOOTH_K\", \"1.0\"))\n",
        "\n",
        "# -------- Dependencies --------\n",
        "try:\n",
        "    import spacy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"[ERR] spaCy model 'en_core_web_sm' missing. Run Step 0 & restart.\") from e\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# -------- Source corpus (bootstrap if empty) --------\n",
        "SRC_DIR = BASE_DIR / \"source_corpus\"\n",
        "SRC_DIR.mkdir(exist_ok=True, parents=True)\n",
        "src_txts = list(SRC_DIR.glob(\"*.txt\"))\n",
        "\n",
        "MEASUREMENT_RE = r\"\\b(measure|weigh|scale|ruler|square|compass|yardstick|standard|gauge|calibrate|benchmark|balance|meter|rule|level|calibration|yard-stick|yard stick)\\b\"\n",
        "CRAFT_RE = r\"\\b(build|construct|forge|craft|hammer|chisel|cut|saw|drill|weld|mold|carve|hew|hewn|join|fit|assemble|fabricate|repair|mend|nail|bolt|plane|lathe|engineer|design|machine|tool)\\b\"\n",
        "\n",
        "if not src_txts:\n",
        "    print(\"[INFO] No source_corpus/*.txt found. Bootstrapping from the main text using measurement/mechanics regex...\") # --- MODIFIED: Clarified source ---\n",
        "    # --- MODIFIED: Use the correct text column from the dataframe ---\n",
        "    main_texts = df[\"__text__\"].fillna(\"\").tolist()\n",
        "    measure_lines = [t for t in main_texts if re.search(MEASUREMENT_RE, t, flags=re.I)]\n",
        "    craft_lines   = [t for t in main_texts if re.search(CRAFT_RE, t, flags=re.I)]\n",
        "    (SRC_DIR / \"bootstrap_measurement.txt\").write_text(\"\\n\".join(measure_lines), encoding=\"utf-8\")\n",
        "    (SRC_DIR / \"bootstrap_mechanics.txt\").write_text(\"\\n\".join(craft_lines), encoding=\"utf-8\")\n",
        "    src_txts = list(SRC_DIR.glob(\"*.txt\"))\n",
        "    print(f\"[OK] Bootstrapped: {len(measure_lines)} measurement lines, {len(craft_lines)} mechanics lines.\")\n",
        "\n",
        "# Read source texts\n",
        "source_docs = []\n",
        "total_chars = 0\n",
        "for p in src_txts:\n",
        "    try:\n",
        "        txt = p.read_text(encoding=\"utf-8\")\n",
        "    except Exception:\n",
        "        try:\n",
        "            txt = p.read_text(encoding=\"latin-1\")\n",
        "        except Exception:\n",
        "            print(f\"[WARN] Could not read {p.name}\")\n",
        "            continue\n",
        "    source_docs.append(txt)\n",
        "    total_chars += len(txt)\n",
        "\n",
        "print(f\"[INFO] Source files: {len(source_docs)} | Total chars: {total_chars}\")\n",
        "\n",
        "# -------- Helpers --------\n",
        "STOP_VERBS = {\n",
        "    \"be\",\"am\",\"is\",\"are\",\"was\",\"were\",\"been\",\"being\",\n",
        "    \"have\",\"has\",\"had\",\n",
        "    \"do\",\"does\",\"did\",\"done\",\n",
        "    \"get\",\"gets\",\"got\",\"become\",\n",
        "    \"make\",\"makes\",\"made\",\n",
        "    \"take\",\"takes\",\"took\",\"taken\",\n",
        "    \"give\",\"gives\",\"gave\",\"given\",\n",
        "    \"go\",\"goes\",\"went\",\"gone\",\"come\",\"comes\",\"came\",\n",
        "    \"put\",\"puts\",\"set\",\"sets\",\"keep\",\"keeps\",\"kept\",\"let\",\"lets\",\"seem\",\"seems\",\n",
        "    \"see\",\"sees\",\"saw\",\"seen\",\"look\",\"looks\",\n",
        "    \"want\",\"wants\",\"need\",\"needs\",\n",
        "    \"use\",\"uses\",\"try\",\"tries\",\"tried\",\n",
        "    \"find\",\"finds\",\"found\",\"think\",\"thinks\",\"thought\",\n",
        "    \"tell\",\"tells\",\"told\",\"ask\",\"asks\",\"asked\",\n",
        "    \"work\",\"works\",\"feel\",\"feels\",\"felt\",\n",
        "    \"leave\",\"leaves\",\"call\",\"calls\",\"called\",\n",
        "    \"like\",\"likes\",\"follow\",\"follows\",\"followed\",\n",
        "    \"begin\",\"begins\",\"began\",\"begun\",\"happen\",\"happens\",\"include\",\"includes\",\"continue\",\"continues\",\n",
        "    \"lose\",\"loses\",\"lost\",\"join\",\"joins\",\"joined\",\"hold\",\"holds\",\"show\",\"shows\",\"provide\",\"provides\"\n",
        "}\n",
        "\n",
        "def extract_pairs_from_texts(texts, nlp):\n",
        "    pairs = []\n",
        "    docs = nlp.pipe(texts, disable=[\"ner\"])\n",
        "    for d in docs:\n",
        "        for sent in d.sents:\n",
        "            for v in sent:\n",
        "                if v.pos_ == \"VERB\" and v.lemma_.lower() not in STOP_VERBS:\n",
        "                    for child in v.children:\n",
        "                        if child.dep_ in (\"dobj\",\"obj\",\"pobj\",\"attr\",\"nsubj\",\"nsubjpass\") and child.pos_ in (\"NOUN\",\"PROPN\",\"PRON\"):\n",
        "                            vlem = v.lemma_.lower()\n",
        "                            nlem = (child.lemma_ or child.text).lower()\n",
        "                            pairs.append((vlem, nlem))\n",
        "    return pairs\n",
        "\n",
        "# -------- Extract pairs --------\n",
        "src_pairs = extract_pairs_from_texts(source_docs, nlp)\n",
        "mozi_pairs_per_sent = []  # (row_id, sentence, [(v,n),...])\n",
        "# --- MODIFIED: Use the correct text and row_id columns ---\n",
        "mozi_docs = nlp.pipe(df[\"__text__\"].astype(str).tolist(), disable=[\"ner\"])\n",
        "for rid, d in zip(df[\"__row_id__\"], mozi_docs):\n",
        "    for sent in d.sents:\n",
        "        pairs = []\n",
        "        for v in sent:\n",
        "            if v.pos_ == \"VERB\" and v.lemma_.lower() not in STOP_VERBS:\n",
        "                for child in v.children:\n",
        "                    if child.dep_ in (\"dobj\",\"obj\",\"pobj\",\"attr\",\"nsubj\",\"nsubjpass\") and child.pos_ in (\"NOUN\",\"PROPN\",\"PRON\"):\n",
        "                        pairs.append((v.lemma_.lower(), (child.lemma_ or child.text).lower()))\n",
        "        if pairs:\n",
        "            mozi_pairs_per_sent.append((rid, sent.text.strip(), pairs))\n",
        "\n",
        "print(f\"[INFO] Extracted {len(src_pairs)} verb–noun pairs from source_corpus.\")\n",
        "print(f\"[INFO] Mozi sentences with pairs: {len(mozi_pairs_per_sent)}\")\n",
        "\n",
        "# -------- PMI on source --------\n",
        "cvn = collections.Counter(src_pairs)\n",
        "cv = collections.Counter([v for v,n in src_pairs])\n",
        "cn = collections.Counter([n for v,n in src_pairs])\n",
        "N = max(1, sum(cvn.values()))\n",
        "\n",
        "def pmi(v, n, add_k=PMI_SMOOTH_K):\n",
        "    a = cvn[(v,n)] + add_k\n",
        "    b = cv[v] + add_k\n",
        "    c = cn[n] + add_k\n",
        "    # Ensure non-zero denominator before log\n",
        "    denominator = b * c\n",
        "    if denominator <= 0:\n",
        "        # Handle case where counts are zero even after smoothing (unlikely with add_k=1.0)\n",
        "        return -float('inf') # Or some large negative value\n",
        "\n",
        "    return math.log2((a * N) / denominator)\n",
        "\n",
        "\n",
        "# -------- Map to synsets (first-sense heuristic) --------\n",
        "def best_synset_verb(lemma):\n",
        "    syns = wn.synsets(lemma, pos=wn.VERB)\n",
        "    return syns[0] if syns else None\n",
        "\n",
        "def best_synset_noun(lemma):\n",
        "    syns = wn.synsets(lemma, pos=wn.NOUN)\n",
        "    return syns[0] if syns else None\n",
        "\n",
        "rows = []\n",
        "for (v,n), cnt in cvn.items():\n",
        "    if cnt < CMI_MIN_PAIR_COUNT:  # tunable threshold\n",
        "        continue\n",
        "    if v in STOP_VERBS:\n",
        "        continue\n",
        "    sv = best_synset_verb(v)\n",
        "    sn = best_synset_noun(n)\n",
        "    rows.append({\n",
        "        \"verb\": v,\n",
        "        \"noun\": n,\n",
        "        \"pair_count\": int(cnt),\n",
        "        \"verb_count\": int(cv[v]),\n",
        "        \"noun_count\": int(cn[n]),\n",
        "        \"PMI\": float(pmi(v,n)),\n",
        "        \"verb_synset\": sv.name() if sv else \"\",\n",
        "        \"verb_lexname\": sv.lexname() if sv else \"\",\n",
        "        \"verb_offset\": sv.offset() if sv else \"\",\n",
        "        \"noun_synset\": sn.name() if sn else \"\",\n",
        "        \"noun_lexname\": sn.lexname() if sn else \"\",\n",
        "        \"noun_offset\": sn.offset() if sn else \"\",\n",
        "    })\n",
        "\n",
        "cand_df = pd.DataFrame(rows, columns=[\n",
        "    \"verb\",\"noun\",\"pair_count\",\"verb_count\",\"noun_count\",\"PMI\",\n",
        "    \"verb_synset\",\"verb_lexname\",\"verb_offset\",\n",
        "    \"noun_synset\",\"noun_lexname\",\"noun_offset\",\n",
        "])\n",
        "\n",
        "# Always write with headers, even if empty\n",
        "cand_out = BASE_DIR / \"cmi_candidates.csv\"\n",
        "cand_df = cand_df.sort_values([\"PMI\",\"pair_count\"], ascending=[False, False]).reset_index(drop=True)\n",
        "cand_df.to_csv(cand_out, index=False)\n",
        "print(f\"[OK] Wrote {cand_out.name} (rows: {len(cand_df)}). Threshold CMI_MIN_PAIR_COUNT={CMI_MIN_PAIR_COUNT}\")\n",
        "\n",
        "# -------- Evidence samples (up to 3 per top pair) --------\n",
        "samples = []\n",
        "if len(cand_df):\n",
        "    top = cand_df.head(200).copy()\n",
        "    # Pre-index Mozi sentences by exact lemma pair\n",
        "    mozi_index = collections.defaultdict(list)\n",
        "    for rid, sent, pairs in mozi_pairs_per_sent:\n",
        "        for (v,n) in pairs:\n",
        "            mozi_index[(v,n)].append((rid, sent))\n",
        "\n",
        "    for _, r in top.iterrows():\n",
        "        key_v = r[\"verb\"]\n",
        "        key_n = r[\"noun\"]\n",
        "        taken = set()\n",
        "        for rid, sent in mozi_index.get((key_v, key_n), [])[:3]:\n",
        "            if (rid, sent) in taken:\n",
        "                continue\n",
        "            samples.append({\"verb_key\": r.get(\"verb_synset\") or key_v, \"noun\": key_n, \"row_id\": rid, \"sentence\": sent})\n",
        "            taken.add((rid, sent))\n",
        "\n",
        "evid_df = pd.DataFrame(samples, columns=[\"verb_key\",\"noun\",\"row_id\",\"sentence\"])\n",
        "evid_out = BASE_DIR / \"cmi_evidence_samples.csv\"\n",
        "evid_df.to_csv(evid_out, index=False)\n",
        "print(f\"[OK] Wrote {evid_out.name} (rows: {len(evid_df)}).\")\n",
        "\n",
        "# -------- Diagnostics & hints --------\n",
        "print(\"\\n[DIAG]\")\n",
        "print(f\"Unique (verb,noun) pairs in source: {len(cvn)} | total tokens N={N}\")\n",
        "print(f\"Kept candidates: {len(cand_df)} (min_pair_count={CMI_MIN_PAIR_COUNT})\")\n",
        "if len(cand_df) == 0:\n",
        "    print(\"[HINT] To get candidates:\")\n",
        "    print(\"  • Add more seed texts into source_corpus/*.txt (technical manuals, measurement/mechanics glossaries).\")\n",
        "    print(\"  • Or lower threshold: set env CMI_MIN_PAIR_COUNT=1 (default) and re-run.\")\n",
        "    print(\"  • Ensure bootstrap regex actually matches your Mozi wording; adjust MEASUREMENT_RE/CRAFT_RE if needed.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}